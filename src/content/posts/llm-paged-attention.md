---
title: "LLM Inference ìµœì í™” #1: Paged Attentionì´ ë­ê¸¸ë˜?"
description: "vLLMì˜ í•µì‹¬ ê¸°ìˆ ì¸ Paged Attentionì„ ì´í•´í•˜ê³ , ì–´ë–»ê²Œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„ 10ë°° ë†’ì´ëŠ”ì§€ ì•Œì•„ë´…ë‹ˆë‹¤."
pubDate: 2026-02-06
author: "Yh Na"
tags: ["llm", "optimization", "vllm", "attention", "memory"]
draft: false
---

# LLM Inference ìµœì í™” #1: Paged Attention

LLMì„ ì„œë¹™í•˜ë‹¤ ë³´ë©´ **ë©”ëª¨ë¦¬ê°€ ê¸ˆë°© ì°¹ë‹ˆë‹¤**. GPT-3 ê·œëª¨ì˜ ëª¨ë¸ì„ ëŒë¦¬ë©´ GPU ë©”ëª¨ë¦¬ ëŒ€ë¶€ë¶„ì´ **KV Cache**ì— ì¡ì•„ë¨¹íˆì£ .

vLLMì€ ì´ ë¬¸ì œë¥¼ **Paged Attention**ìœ¼ë¡œ í•´ê²°í–ˆìŠµë‹ˆë‹¤. ë©”ëª¨ë¦¬ íš¨ìœ¨ì„ **10ë°°** ë†’ì´ê³ , ì²˜ë¦¬ëŸ‰(throughput)ì„ **24ë°°** ì˜¬ë ¸ì–´ìš”.

ì–´ë–»ê²Œ ê°€ëŠ¥í–ˆì„ê¹Œìš”?

---

## ë¬¸ì œ: KV Cacheê°€ ë©”ëª¨ë¦¬ë¥¼ ì¡ì•„ë¨¹ëŠ”ë‹¤

### Transformer Attention ë³µìŠµ

Transformerì˜ attention ê³„ì‚°:

```
Attention(Q, K, V) = softmax(QK^T / âˆšd) V
```

- **Q** (Query): í˜„ì¬ í† í°
- **K** (Key): ëª¨ë“  ì´ì „ í† í°ë“¤
- **V** (Value): ëª¨ë“  ì´ì „ í† í°ë“¤

**ë¬¸ì œëŠ” Kì™€ Vì…ë‹ˆë‹¤.**

### ì˜ˆì‹œ: "Hello, how are you?" ìƒì„±

```
Step 1: "Hello"
  K = [K_Hello]
  V = [V_Hello]

Step 2: "Hello," â†’ ","
  K = [K_Hello, K_,]
  V = [V_Hello, V_,]

Step 3: "Hello, how" â†’ "how"
  K = [K_Hello, K_,, K_how]
  V = [V_Hello, V_,, V_how]

...
```

í† í°ì„ ìƒì„±í•  ë•Œë§ˆë‹¤ K, Vê°€ ëŠ˜ì–´ë‚©ë‹ˆë‹¤. ì´ê±¸ **KV Cache**ë¼ê³  ë¶€ë¦…ë‹ˆë‹¤.

### ë©”ëª¨ë¦¬ ê³„ì‚°

**ëª¨ë¸**: LLaMA-13B (40 layers, hidden 5120)
**ë°°ì¹˜ í¬ê¸°**: 1
**ì‹œí€€ìŠ¤ ê¸¸ì´**: 2048

```
KV Cache í¬ê¸° = 2 (K+V) Ã— 40 (layers) Ã— 5120 (hidden) Ã— 2048 (seq) Ã— 2 (fp16)
             = 1.6 GB
```

**ë‹¨ í•œ ê°œì˜ ìš”ì²­**ì´ 1.6GBë¥¼ ë¨¹ìŠµë‹ˆë‹¤!

ë°°ì¹˜ í¬ê¸° 16ì´ë©´? **25.6GB**. A100 80GBë„ 3ê°œ ë°°ì¹˜ë©´ ëì´ì—ìš”.

---

## ê¸°ì¡´ ë°©ì‹ì˜ ë¬¸ì œì 

### 1. ë©”ëª¨ë¦¬ ë‹¨í¸í™” (Fragmentation)

ê¸°ì¡´ ë°©ì‹ì€ ê° ìš”ì²­ë§ˆë‹¤ **ì—°ì†ëœ ë©”ëª¨ë¦¬**ë¥¼ í• ë‹¹í•©ë‹ˆë‹¤.

```
Request 1: [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 2048 tokens (full)
Request 2: [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 512 tokens (not full)
Request 3: [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘] 1024 tokens (not full)
```

Request 2, 3ì€ ìµœëŒ€ ê¸¸ì´ë§Œí¼ ë©”ëª¨ë¦¬ë¥¼ ì˜ˆì•½í•˜ì§€ë§Œ, ì‹¤ì œë¡  ì¼ë¶€ë§Œ ì”ë‹ˆë‹¤.

**ë‚¨ì€ ê³µê°„ì€ ë‚­ë¹„ë©ë‹ˆë‹¤.**

### 2. ë™ì  ê¸¸ì´ë¥¼ ì²˜ë¦¬ ëª»í•¨

ì‚¬ìš©ìë§ˆë‹¤ ìƒì„± ê¸¸ì´ê°€ ë‹¤ë¦…ë‹ˆë‹¤:
- "Hello" â†’ ì§§ìŒ (10 tokens)
- "Explain quantum physics" â†’ ê¹€ (500 tokens)

í•˜ì§€ë§Œ **ë¯¸ë¦¬ ìµœëŒ€ ê¸¸ì´ë¥¼ í• ë‹¹**í•´ì•¼ í•˜ë‹ˆ ë‚­ë¹„ê°€ ì‹¬í•©ë‹ˆë‹¤.

### 3. Batching íš¨ìœ¨ ë–¨ì–´ì§

```
Batch 1: [2048, 512, 1024, 128] tokens
```

ê°€ì¥ ê¸´ 2048ì— ë§ì¶°ì„œ ëª¨ë‘ 2048ë§Œí¼ í• ë‹¹ â†’ **76% ë‚­ë¹„**

---

## í•´ê²°ì±…: Paged Attention

ì•„ì´ë””ì–´ëŠ” ê°„ë‹¨í•©ë‹ˆë‹¤:

> **ìš´ì˜ì²´ì œì˜ ê°€ìƒ ë©”ëª¨ë¦¬ì²˜ëŸ¼ ë©”ëª¨ë¦¬ë¥¼ í˜ì´ì§€ ë‹¨ìœ„ë¡œ ê´€ë¦¬í•˜ì!**

### í•µì‹¬ ê°œë…

**1. í˜ì´ì§€ ë‹¨ìœ„ í• ë‹¹**

ì—°ì†ëœ í° ë©”ëª¨ë¦¬ ëŒ€ì‹ , ì‘ì€ **í˜ì´ì§€(block)**ë¡œ ë‚˜ëˆ•ë‹ˆë‹¤.

```
Page size = 16 tokens

ê¸°ì¡´:
Request 1: [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 2048 tokens

Paged Attention:
Request 1: [â–ˆâ–ˆâ–ˆâ–ˆ] [â–ˆâ–ˆâ–ˆâ–ˆ] [â–ˆâ–ˆâ–ˆâ–ˆ] ... [â–ˆâ–ˆâ–ˆâ–ˆ]
           page1  page2  page3      page128
```

### 2. ë¹„ì—°ì† ë©”ëª¨ë¦¬ ì‚¬ìš©

í˜ì´ì§€ë“¤ì€ ë¬¼ë¦¬ì ìœ¼ë¡œ ë–¨ì–´ì ¸ ìˆì–´ë„ ë©ë‹ˆë‹¤.

```
Physical Memory:
[Page A] [Free] [Page C] [Free] [Page B] [Free]

Logical View (Request 1):
[Page A] â†’ [Page B] â†’ [Page C]
```

OSì˜ í˜ì´ì§€ í…Œì´ë¸”ì²˜ëŸ¼, **ë§¤í•‘ í…Œì´ë¸”**ë¡œ ê´€ë¦¬í•©ë‹ˆë‹¤.

### 3. ë™ì  í• ë‹¹

í•„ìš”í•  ë•Œë§Œ í˜ì´ì§€ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.

```
Step 1: "Hello" (5 tokens)
  [â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘]  1 page (16 tokens)

Step 10: 50 tokens ìƒì„±
  [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ]
   page 1 (full)      page 2 (full)      page 3 (partial)
```

---

## vLLM êµ¬í˜„

### Block Table (í˜ì´ì§€ í…Œì´ë¸”)

ê° ìš”ì²­ë§ˆë‹¤ **Block Table**ì„ ìœ ì§€í•©ë‹ˆë‹¤.

```python
class Sequence:
    def __init__(self):
        self.tokens = []
        self.block_table = []  # ë¬¼ë¦¬ ë¸”ë¡ IDë“¤
    
    def append_token(self, token):
        self.tokens.append(token)
        
        # í˜„ì¬ ë¸”ë¡ì´ ê½‰ ì°¼ìœ¼ë©´ ìƒˆ ë¸”ë¡ í• ë‹¹
        if len(self.tokens) % BLOCK_SIZE == 1:
            new_block = allocate_block()
            self.block_table.append(new_block)
```

### Block Manager

ë©”ëª¨ë¦¬ í’€ì„ ê´€ë¦¬í•©ë‹ˆë‹¤.

```python
class BlockSpaceManager:
    def __init__(self, num_blocks, block_size):
        self.block_size = block_size
        self.free_blocks = list(range(num_blocks))
        self.allocated_blocks = {}
    
    def allocate(self, seq_id):
        """ìƒˆ ë¸”ë¡ í• ë‹¹"""
        if not self.free_blocks:
            raise OutOfMemoryError()
        
        block_id = self.free_blocks.pop()
        self.allocated_blocks[seq_id] = block_id
        return block_id
    
    def free(self, seq_id):
        """ë¸”ë¡ í•´ì œ"""
        block_id = self.allocated_blocks.pop(seq_id)
        self.free_blocks.append(block_id)
```

### Attention ê³„ì‚°

ê¸°ì¡´ attentionê³¼ ë™ì¼í•˜ì§€ë§Œ, **ë¶ˆì—°ì† ë©”ëª¨ë¦¬**ë¥¼ ì½ìŠµë‹ˆë‹¤.

```python
def paged_attention(query, key_cache, value_cache, block_table):
    """
    query: [batch, num_heads, head_dim]
    key_cache: [num_blocks, block_size, num_heads, head_dim]
    value_cache: [num_blocks, block_size, num_heads, head_dim]
    block_table: [batch, max_num_blocks] - ê° ì‹œí€€ìŠ¤ì˜ ë¸”ë¡ IDë“¤
    """
    batch_size = query.shape[0]
    outputs = []
    
    for i in range(batch_size):
        # ì´ ì‹œí€€ìŠ¤ì˜ ë¸”ë¡ë“¤ ê°€ì ¸ì˜¤ê¸°
        blocks = block_table[i]
        
        # ê° ë¸”ë¡ì—ì„œ K, V ìˆ˜ì§‘
        keys = []
        values = []
        for block_id in blocks:
            keys.append(key_cache[block_id])
            values.append(value_cache[block_id])
        
        # Attention ê³„ì‚° (í‘œì¤€ ë°©ì‹)
        K = torch.cat(keys, dim=0)  # [seq_len, num_heads, head_dim]
        V = torch.cat(values, dim=0)
        
        scores = query[i] @ K.transpose(-2, -1) / sqrt(d)
        attn = softmax(scores, dim=-1)
        output = attn @ V
        
        outputs.append(output)
    
    return torch.stack(outputs)
```

ì‹¤ì œë¡œëŠ” CUDA ì»¤ë„ë¡œ ìµœì í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤!

---

## CUDA ì»¤ë„ ìµœì í™”

### ë¬¸ì œ: ë¹„ì—°ì† ë©”ëª¨ë¦¬ ì½ê¸°ëŠ” ëŠë¦¬ë‹¤

```cuda
// ë‚˜ì´ë¸Œ êµ¬í˜„: ë¸”ë¡ë§ˆë‹¤ ë©”ëª¨ë¦¬ ì ‘ê·¼
for (int block_idx = 0; block_idx < num_blocks; block_idx++) {
    int block_id = block_table[block_idx];
    // key_cache[block_id]ì—ì„œ ì½ê¸° â†’ ìºì‹œ ë¯¸ìŠ¤ ë§ìŒ
}
```

### í•´ê²°ì±…: Fused Kernel

ëª¨ë“  ë¸”ë¡ì„ **í•œ ë²ˆì—** ì²˜ë¦¬í•˜ëŠ” CUDA ì»¤ë„:

```cuda
__global__ void paged_attention_kernel(
    const float* Q,           // [batch, heads, head_dim]
    const float* K_cache,     // [num_blocks, block_size, heads, head_dim]
    const float* V_cache,     // [num_blocks, block_size, heads, head_dim]
    const int* block_table,   // [batch, max_blocks]
    float* output,            // [batch, heads, head_dim]
    int block_size
) {
    int batch_idx = blockIdx.x;
    int head_idx = blockIdx.y;
    int tid = threadIdx.x;
    
    // Shared memoryì— Q ë¡œë“œ
    __shared__ float Q_shared[HEAD_DIM];
    if (tid < HEAD_DIM) {
        Q_shared[tid] = Q[batch_idx * num_heads * HEAD_DIM + 
                          head_idx * HEAD_DIM + tid];
    }
    __syncthreads();
    
    // ê° ë¸”ë¡ ìˆœíšŒ
    float attn_sum = 0.0f;
    for (int block_idx = 0; block_idx < max_blocks; block_idx++) {
        int physical_block = block_table[batch_idx * max_blocks + block_idx];
        if (physical_block < 0) break;  // ìœ íš¨í•œ ë¸”ë¡ ë
        
        // ì´ ë¸”ë¡ì˜ ëª¨ë“  í† í°ì— ëŒ€í•´ attention
        for (int token_idx = 0; token_idx < block_size; token_idx++) {
            // Kì™€ ë‚´ì 
            float score = 0.0f;
            for (int d = tid; d < HEAD_DIM; d += blockDim.x) {
                int k_idx = physical_block * block_size * num_heads * HEAD_DIM +
                           token_idx * num_heads * HEAD_DIM +
                           head_idx * HEAD_DIM + d;
                score += Q_shared[d] * K_cache[k_idx];
            }
            
            // Reduce across threads
            score = warp_reduce_sum(score);
            
            // Softmax ë¶„ì ê³„ì‚°
            float exp_score = expf(score / sqrtf(HEAD_DIM));
            attn_sum += exp_score;
            
            // Vì™€ ê³±í•˜ê¸° (ëˆ„ì )
            // ...
        }
    }
    
    // Softmax ì •ê·œí™” & ì¶œë ¥
    // ...
}
```

### í•µì‹¬ ìµœì í™”

1. **Shared Memory**: Që¥¼ ê³µìœ  ë©”ëª¨ë¦¬ì— ìºì‹±
2. **Coalesced Access**: K, Vë¥¼ ì—°ì†ìœ¼ë¡œ ì½ê¸°
3. **Warp Reduction**: ìŠ¤ë ˆë“œ ê°„ í•©ì‚° ë³‘ë ¬í™”
4. **Fused Operation**: Attention ì „ì²´ë¥¼ í•œ ì»¤ë„ì—ì„œ

---

## ì„±ëŠ¥ ë¹„êµ

### ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰

**Scenario**: LLaMA-13B, ë°°ì¹˜ í¬ê¸° 64, í‰ê·  ì‹œí€€ìŠ¤ ê¸¸ì´ 512

| ë°©ì‹ | ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ |
|------|--------------|
| Naive (ê³ ì • í• ë‹¹) | 102 GB |
| Paged Attention | 12 GB |

**8.5ë°° ì ˆì•½!**

### Throughput

| ë°©ì‹ | ì²˜ë¦¬ëŸ‰ (requests/sec) |
|------|----------------------|
| HuggingFace Transformers | 0.8 |
| FasterTransformer | 4.5 |
| vLLM (Paged Attention) | 24.0 |

**30ë°° í–¥ìƒ!**

---

## ì¶”ê°€ ìµœì í™”: Copy-on-Write

### ë¬¸ì œ: Prefix ê³µìœ 

ê°™ì€ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ ì—¬ëŸ¬ ìš”ì²­ì´ ê³µìœ í•©ë‹ˆë‹¤:

```
Request 1: "You are a helpful assistant. What is AI?"
Request 2: "You are a helpful assistant. Explain quantum physics."
Request 3: "You are a helpful assistant. Write a poem."
```

"You are a helpful assistant"ëŠ” **ëª¨ë‘ ê°™ì€ë°** ê°ì ë©”ëª¨ë¦¬ë¥¼ ì”ë‹ˆë‹¤.

### í•´ê²°ì±…: Block ê³µìœ 

```python
class Sequence:
    def __init__(self, prefix_blocks=None):
        self.block_table = prefix_blocks.copy() if prefix_blocks else []
        self.num_shared_blocks = len(self.block_table)
    
    def append_token(self, token):
        # ê³µìœ  ë¸”ë¡ì— ì“°ë ¤ê³  í•˜ë©´ ë³µì‚¬ (Copy-on-Write)
        if self.num_shared_blocks > 0:
            last_shared = self.block_table[-1]
            new_block = copy_block(last_shared)
            self.block_table[-1] = new_block
            self.num_shared_blocks -= 1
```

### íš¨ê³¼

ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ê°€ 100 í† í°ì´ê³ , 1000ê°œ ìš”ì²­ì´ë©´:

- **Before**: 100 Ã— 1000 = 100,000 í† í° ì €ì¥
- **After**: 100 í† í° ì €ì¥ (ê³µìœ )

**1000ë°° ì ˆì•½!**

---

## êµ¬í˜„ ì˜ˆì œ (ê°„ë‹¨ ë²„ì „)

ì „ì²´ vLLMì€ ë³µì¡í•˜ë‹ˆ, í•µì‹¬ë§Œ êµ¬í˜„í•´ë´…ì‹œë‹¤.

```python
import torch
import torch.nn.functional as F

class PagedKVCache:
    def __init__(self, num_blocks, block_size, num_layers, num_heads, head_dim):
        self.num_blocks = num_blocks
        self.block_size = block_size
        
        # Physical memory pool
        self.key_cache = torch.zeros(
            num_layers, num_blocks, block_size, num_heads, head_dim,
            dtype=torch.float16, device='cuda'
        )
        self.value_cache = torch.zeros_like(self.key_cache)
        
        # Free block list
        self.free_blocks = list(range(num_blocks))
    
    def allocate_block(self):
        if not self.free_blocks:
            raise RuntimeError("Out of memory")
        return self.free_blocks.pop()
    
    def free_block(self, block_id):
        self.free_blocks.append(block_id)
    
    def write(self, layer_id, block_id, slot_id, key, value):
        """ë¸”ë¡ì˜ íŠ¹ì • ìŠ¬ë¡¯ì— K, V ì“°ê¸°"""
        self.key_cache[layer_id, block_id, slot_id] = key
        self.value_cache[layer_id, block_id, slot_id] = value
    
    def read(self, layer_id, block_table):
        """ë¸”ë¡ í…Œì´ë¸”ì—ì„œ K, V ì½ê¸°"""
        keys = []
        values = []
        for block_id in block_table:
            keys.append(self.key_cache[layer_id, block_id])
            values.append(self.value_cache[layer_id, block_id])
        
        # [num_blocks * block_size, num_heads, head_dim]
        K = torch.cat(keys, dim=0)
        V = torch.cat(values, dim=0)
        return K, V


class PagedAttention:
    def __init__(self, kv_cache, num_heads, head_dim):
        self.kv_cache = kv_cache
        self.num_heads = num_heads
        self.head_dim = head_dim
    
    def forward(self, query, layer_id, block_table, seq_len):
        """
        query: [num_heads, head_dim]
        block_table: [num_blocks]
        seq_len: ì‹¤ì œ í† í° ìˆ˜
        """
        # KV Cacheì—ì„œ ì½ê¸°
        K, V = self.kv_cache.read(layer_id, block_table)
        
        # ì‹¤ì œ ê¸¸ì´ë§Œí¼ë§Œ ì‚¬ìš©
        K = K[:seq_len]  # [seq_len, num_heads, head_dim]
        V = V[:seq_len]
        
        # Attention ê³„ì‚°
        query = query.unsqueeze(0)  # [1, num_heads, head_dim]
        
        scores = torch.matmul(query, K.transpose(-2, -1))  # [1, num_heads, seq_len]
        scores = scores / (self.head_dim ** 0.5)
        
        attn = F.softmax(scores, dim=-1)
        output = torch.matmul(attn, V)  # [1, num_heads, head_dim]
        
        return output.squeeze(0)


# ì‚¬ìš© ì˜ˆì œ
def generate_with_paged_attention():
    # ì´ˆê¸°í™”
    kv_cache = PagedKVCache(
        num_blocks=1024,
        block_size=16,
        num_layers=32,
        num_heads=32,
        head_dim=128
    )
    
    attention = PagedAttention(kv_cache, num_heads=32, head_dim=128)
    
    # ì‹œí€€ìŠ¤ ìƒíƒœ
    block_table = []
    seq_len = 0
    
    # í† í° ìƒì„± ë£¨í”„
    for step in range(100):
        # ìƒˆ ë¸”ë¡ í•„ìš”?
        if seq_len % kv_cache.block_size == 0:
            new_block = kv_cache.allocate_block()
            block_table.append(new_block)
        
        # Forward pass (ê° ë ˆì´ì–´ë§ˆë‹¤)
        for layer_id in range(32):
            # Query ê³„ì‚° (ëª¨ë¸ì—ì„œ)
            query = get_query(layer_id)  # [num_heads, head_dim]
            
            # Paged Attention
            output = attention.forward(query, layer_id, block_table, seq_len)
            
            # KV Cacheì— ì €ì¥
            key, value = compute_kv(output)
            block_id = block_table[-1]
            slot_id = seq_len % kv_cache.block_size
            kv_cache.write(layer_id, block_id, slot_id, key, value)
        
        # ë‹¤ìŒ í† í° ìƒì„±
        next_token = sample_token(output)
        seq_len += 1
        
        if next_token == EOS:
            break
    
    # ë©”ëª¨ë¦¬ í•´ì œ
    for block_id in block_table:
        kv_cache.free_block(block_id)
```

---

## ì‹¤ì „ vLLM ì‚¬ìš©

### ì„¤ì¹˜

```bash
pip install vllm
```

### ê¸°ë³¸ ì‚¬ìš©

```python
from vllm import LLM, SamplingParams

# ëª¨ë¸ ë¡œë“œ
llm = LLM(model="meta-llama/Llama-2-7b-hf")

# í”„ë¡¬í”„íŠ¸ë“¤
prompts = [
    "Write a poem about AI:",
    "Explain quantum physics:",
    "Tell me a joke:",
]

# ìƒ˜í”Œë§ íŒŒë¼ë¯¸í„°
sampling_params = SamplingParams(
    temperature=0.8,
    top_p=0.95,
    max_tokens=512
)

# ë°°ì¹˜ ìƒì„±
outputs = llm.generate(prompts, sampling_params)

for output in outputs:
    print(output.outputs[0].text)
```

### ê³ ê¸‰ ì„¤ì •

```python
llm = LLM(
    model="meta-llama/Llama-2-13b-hf",
    tensor_parallel_size=2,     # 2 GPU ì‚¬ìš©
    dtype="float16",
    gpu_memory_utilization=0.9, # GPU ë©”ëª¨ë¦¬ 90% ì‚¬ìš©
    block_size=16,              # í˜ì´ì§€ í¬ê¸°
    max_num_seqs=256,           # ìµœëŒ€ ë°°ì¹˜ í¬ê¸°
)
```

---

## í•œê³„ì™€ íŠ¸ë ˆì´ë“œì˜¤í”„

### 1. ì¶”ê°€ ì˜¤ë²„í—¤ë“œ

ë¸”ë¡ í…Œì´ë¸” ê´€ë¦¬ì™€ ë¶ˆì—°ì† ë©”ëª¨ë¦¬ ì½ê¸°ëŠ” ì•½ê°„ì˜ ì˜¤ë²„í—¤ë“œê°€ ìˆìŠµë‹ˆë‹¤.

**But**: ë©”ëª¨ë¦¬ ì ˆì•½ìœ¼ë¡œ ë” í° ë°°ì¹˜ë¥¼ ì“¸ ìˆ˜ ìˆì–´ì„œ **ì „ì²´ ì„±ëŠ¥ì€ í–¥ìƒ**

### 2. ë¸”ë¡ í¬ê¸° ì„ íƒ

- **ì‘ì€ ë¸”ë¡** (8): ë©”ëª¨ë¦¬ íš¨ìœ¨ â†‘, ì˜¤ë²„í—¤ë“œ â†‘
- **í° ë¸”ë¡** (32): ë©”ëª¨ë¦¬ íš¨ìœ¨ â†“, ì˜¤ë²„í—¤ë“œ â†“

**vLLM ê¸°ë³¸ê°’: 16** (ì¢‹ì€ ë°¸ëŸ°ìŠ¤)

### 3. CUDA ì»¤ë„ ë³µì¡ë„

í‘œì¤€ attentionë³´ë‹¤ êµ¬í˜„ì´ ë³µì¡í•©ë‹ˆë‹¤. í•˜ì§€ë§Œ vLLMì´ ë‹¤ í•´ì¤˜ì„œ ì‚¬ìš©ìëŠ” ì‹ ê²½ ì•ˆ ì¨ë„ ë¨!

---

## ìš”ì•½

**Paged Attention**ì€:

1. **ë©”ëª¨ë¦¬ë¥¼ í˜ì´ì§€(ë¸”ë¡) ë‹¨ìœ„ë¡œ ê´€ë¦¬**
2. **ë¹„ì—°ì† ë©”ëª¨ë¦¬ ì‚¬ìš© ê°€ëŠ¥**
3. **ë™ì  í• ë‹¹ìœ¼ë¡œ ë‚­ë¹„ ìµœì†Œí™”**
4. **Copy-on-Writeë¡œ prefix ê³µìœ **

ê²°ê³¼:
- **ë©”ëª¨ë¦¬: 8-10ë°° ì ˆì•½**
- **ì²˜ë¦¬ëŸ‰: 20-30ë°° í–¥ìƒ**
- **ë” í° ë°°ì¹˜, ë” ê¸´ ì‹œí€€ìŠ¤ ê°€ëŠ¥**

---

## ë‹¤ìŒ ê¸€

**5í¸: KV Caching ì™„ì „ ì •ë³µ**
- KV Cacheê°€ ì •í™•íˆ ë­ê¸¸ë˜?
- ë©”ëª¨ë¦¬ ë ˆì´ì•„ì›ƒ
- Multi-head attention ìµœì í™”

ê¸°ëŒ€í•´ì£¼ì„¸ìš”! ğŸš€

---

*ì§ˆë¬¸ì´ë‚˜ í”¼ë“œë°±ì€ [GitHub](https://github.com/yhna941)ì—ì„œ í™˜ì˜í•©ë‹ˆë‹¤!*
