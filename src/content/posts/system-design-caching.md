---
title: "System Design #3: Caching ì „ëµ - Redis ì‹¬í™”ì™€ Cache Invalidation"
description: "ëŒ€ê·œëª¨ ì‹œìŠ¤í…œì˜ ì„±ëŠ¥ì„ ê·¹ëŒ€í™”í•˜ëŠ” ìºì‹± ì „ëµê³¼ ì‹¤ì „ íŒ¨í„´ì„ ì•Œì•„ë´…ë‹ˆë‹¤."
pubDate: 2026-02-06
author: "Yh Na"
tags: ["system-design", "caching", "redis", "performance", "scalability"]
draft: false
---

# System Design #3: Caching ì „ëµ

**"Cacheê°€ ì—†ìœ¼ë©´ ì£½ëŠ”ë‹¤"**

ì„±ëŠ¥ ë¹„êµ:
```
Without Cache:
- Response: 500ms
- DB load: 100%
- Cost: $$$

With Cache:
- Response: 5ms (100ë°° ë¹ ë¦„!)
- DB load: 10%
- Cost: $
```

---

## Cache ê³„ì¸µ

### ì „ì²´ ì•„í‚¤í…ì²˜

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Client  â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
     â”‚
â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ CDN (Static) â”‚ ğŸŒ Edge locations
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚
â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ API Server   â”‚
â””â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”˜
  â”‚        â”‚
â”Œâ”€â–¼â”€â”€â”€â”€â” â”Œâ”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Redis â”‚ â”‚Applicationâ”‚ ğŸ’¾ In-memory
â”‚      â”‚ â”‚Cache      â”‚
â””â”€â”¬â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”‚
â”Œâ”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Database â”‚ ğŸ’¿ Disk
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ê³„ì¸µë³„ íŠ¹ì„±:**

```
L1 - Application Cache:
- Latency: 0.1ms
- Size: 100MB
- Scope: Single process

L2 - Redis:
- Latency: 1ms
- Size: 100GB
- Scope: All servers

L3 - Database:
- Latency: 10-100ms
- Size: 10TB
- Scope: Persistent
```

---

## Redis ê¸°ì´ˆ

### ë°ì´í„° êµ¬ì¡°

```python
import redis

r = redis.Redis(host='localhost', port=6379, decode_responses=True)

# 1. String (ê°€ì¥ ê¸°ë³¸)
r.set('user:1000:name', 'John')
r.get('user:1000:name')  # 'John'

# TTL (Time To Live)
r.setex('session:abc', 3600, 'user_data')  # 1ì‹œê°„ í›„ ìë™ ì‚­ì œ

# 2. Hash (ê°ì²´ ì €ì¥)
r.hset('user:1000', mapping={
    'name': 'John',
    'email': 'john@example.com',
    'age': 30
})
r.hgetall('user:1000')  # {'name': 'John', 'email': '...', 'age': '30'}

# 3. List (í, ìŠ¤íƒ)
r.lpush('queue:tasks', 'task1', 'task2', 'task3')
r.rpop('queue:tasks')  # 'task1' (FIFO)

# 4. Set (ì¤‘ë³µ ì œê±°)
r.sadd('user:1000:followers', '1001', '1002', '1003')
r.sismember('user:1000:followers', '1001')  # True

# 5. Sorted Set (ë¦¬ë”ë³´ë“œ)
r.zadd('leaderboard', {'user1': 100, 'user2': 250, 'user3': 150})
r.zrevrange('leaderboard', 0, 9)  # Top 10
```

### ì‹¤ì „ íŒ¨í„´

**User Session:**

```python
class SessionManager:
    def __init__(self, redis_client):
        self.redis = redis_client
        self.ttl = 86400  # 24ì‹œê°„
    
    def create_session(self, session_id, user_id):
        """Create user session"""
        session_key = f"session:{session_id}"
        self.redis.hset(session_key, mapping={
            'user_id': user_id,
            'created_at': time.time()
        })
        self.redis.expire(session_key, self.ttl)
    
    def get_user(self, session_id):
        """Get user from session"""
        session_key = f"session:{session_id}"
        return self.redis.hget(session_key, 'user_id')
    
    def extend_session(self, session_id):
        """Extend session TTL"""
        session_key = f"session:{session_id}"
        self.redis.expire(session_key, self.ttl)
```

**Rate Limiting:**

```python
class RateLimiter:
    def __init__(self, redis_client, max_requests=100, window=60):
        self.redis = redis_client
        self.max_requests = max_requests
        self.window = window  # seconds
    
    def is_allowed(self, user_id):
        """Check if request is allowed"""
        key = f"rate_limit:{user_id}"
        
        # Increment counter
        count = self.redis.incr(key)
        
        # Set expiry on first request
        if count == 1:
            self.redis.expire(key, self.window)
        
        return count <= self.max_requests

# ì‚¬ìš©
limiter = RateLimiter(redis_client, max_requests=100, window=60)

if limiter.is_allowed(user_id):
    # Process request
    pass
else:
    # 429 Too Many Requests
    return {"error": "Rate limit exceeded"}
```

**Leaderboard:**

```python
class Leaderboard:
    def __init__(self, redis_client):
        self.redis = redis_client
        self.key = "leaderboard:global"
    
    def update_score(self, user_id, score):
        """Update user score"""
        self.redis.zadd(self.key, {user_id: score})
    
    def increment_score(self, user_id, delta):
        """Increment user score"""
        self.redis.zincrby(self.key, delta, user_id)
    
    def get_top(self, n=10):
        """Get top N users"""
        # ZREVRANGE: highest to lowest
        top_users = self.redis.zrevrange(
            self.key, 0, n-1, withscores=True
        )
        return [(user, int(score)) for user, score in top_users]
    
    def get_rank(self, user_id):
        """Get user rank (0-indexed)"""
        rank = self.redis.zrevrank(self.key, user_id)
        return rank + 1 if rank is not None else None
    
    def get_score(self, user_id):
        """Get user score"""
        score = self.redis.zscore(self.key, user_id)
        return int(score) if score else 0

# ì‚¬ìš©
lb = Leaderboard(redis_client)
lb.update_score('user1', 1000)
lb.increment_score('user1', 50)

print(lb.get_top(10))  # [(user, score), ...]
print(lb.get_rank('user1'))  # 1
```

---

## Caching ì „ëµ

### 1. Cache-Aside (Lazy Loading)

**ê°€ì¥ ì¼ë°˜ì :**

```python
def get_user(user_id):
    cache_key = f"user:{user_id}"
    
    # 1. Try cache
    user = cache.get(cache_key)
    if user:
        return user  # Cache hit
    
    # 2. Cache miss - fetch from DB
    user = db.query("SELECT * FROM users WHERE id = ?", user_id)
    
    # 3. Store in cache
    cache.set(cache_key, user, ttl=3600)
    
    return user
```

**ì¥ì :**
- ê°„ë‹¨
- í•„ìš”í•œ ê²ƒë§Œ ìºì‹±
- Fault tolerant (cache ì£½ì–´ë„ ë™ì‘)

**ë‹¨ì :**
- Cold start (ì²˜ìŒì—” ëŠë¦¼)
- Cache miss penalty

### 2. Write-Through

**ì“¸ ë•Œë§ˆë‹¤ ìºì‹œ ì—…ë°ì´íŠ¸:**

```python
def update_user(user_id, data):
    cache_key = f"user:{user_id}"
    
    # 1. Update DB
    db.update("UPDATE users SET ... WHERE id = ?", user_id, data)
    
    # 2. Update cache
    cache.set(cache_key, data, ttl=3600)
    
    return data
```

**ì¥ì :**
- í•­ìƒ ìµœì‹  ë°ì´í„°
- Cache miss ì ìŒ

**ë‹¨ì :**
- ì“°ê¸° ëŠë¦¼ (2ë²ˆ write)
- ì•ˆ ì½ëŠ” ë°ì´í„°ë„ ìºì‹±

### 3. Write-Behind (Write-Back)

**ë¹„ë™ê¸° ì“°ê¸°:**

```python
import asyncio
from collections import deque

class WriteBehindCache:
    def __init__(self, cache, db, batch_size=100, flush_interval=5):
        self.cache = cache
        self.db = db
        self.write_queue = deque()
        self.batch_size = batch_size
        self.flush_interval = flush_interval
        
        # Background flusher
        asyncio.create_task(self.flush_worker())
    
    def set(self, key, value):
        # 1. Update cache immediately
        self.cache.set(key, value)
        
        # 2. Queue DB write
        self.write_queue.append((key, value))
        
        # 3. Flush if batch full
        if len(self.write_queue) >= self.batch_size:
            self.flush()
    
    def flush(self):
        """Flush queue to DB"""
        while self.write_queue:
            key, value = self.write_queue.popleft()
            self.db.update(key, value)
    
    async def flush_worker(self):
        """Periodic flush"""
        while True:
            await asyncio.sleep(self.flush_interval)
            self.flush()
```

**ì¥ì :**
- ì“°ê¸° ë¹ ë¦„ (ë¹„ë™ê¸°)
- Batch write (íš¨ìœ¨ì )

**ë‹¨ì :**
- ë°ì´í„° ì†ì‹¤ ìœ„í—˜
- ë³µì¡

### 4. Refresh-Ahead

**ë§Œë£Œ ì „ ê°±ì‹ :**

```python
class RefreshAheadCache:
    def __init__(self, cache, db, ttl=3600, refresh_threshold=0.8):
        self.cache = cache
        self.db = db
        self.ttl = ttl
        self.refresh_threshold = refresh_threshold
    
    def get(self, key):
        # Get from cache
        value, remaining_ttl = self.cache.get_with_ttl(key)
        
        if value is None:
            # Cache miss
            value = self.db.get(key)
            self.cache.set(key, value, ttl=self.ttl)
        elif remaining_ttl < self.ttl * self.refresh_threshold:
            # Refresh in background
            asyncio.create_task(self.refresh(key))
        
        return value
    
    async def refresh(self, key):
        """Background refresh"""
        value = self.db.get(key)
        self.cache.set(key, value, ttl=self.ttl)
```

---

## Cache Invalidation

**"ì»´í“¨í„° ê³¼í•™ì˜ ê°€ì¥ ì–´ë ¤ìš´ ë¬¸ì œ"**

### ì „ëµ

#### 1. TTL (Time To Live)

```python
# ê°„ë‹¨í•˜ì§€ë§Œ stale data ê°€ëŠ¥
cache.set('user:1000', user_data, ttl=3600)  # 1ì‹œê°„
```

**ì–¸ì œ ì‚¬ìš©:**
- ì•½ê°„ì˜ staleness í—ˆìš©
- ë°ì´í„° ìì£¼ ë³€ê²½ ì•ˆ ë¨

#### 2. Explicit Invalidation

```python
def update_user(user_id, data):
    # 1. Update DB
    db.update(user_id, data)
    
    # 2. Invalidate cache
    cache.delete(f"user:{user_id}")
    
    # 3. Invalidate related caches
    cache.delete(f"user:{user_id}:posts")
    cache.delete(f"feed:{user_id}")
```

**ì£¼ì˜:** Cascading invalidation!

#### 3. Cache Stampede ë°©ì§€

**ë¬¸ì œ:**

```
Cache expires
â†’ 1000 requests hit DB simultaneously
â†’ DB overload!
```

**í•´ê²°: Lock**

```python
import threading

class CacheWithLock:
    def __init__(self, cache, db):
        self.cache = cache
        self.db = db
        self.locks = {}
    
    def get(self, key):
        # Try cache
        value = self.cache.get(key)
        if value:
            return value
        
        # Acquire lock for this key
        if key not in self.locks:
            self.locks[key] = threading.Lock()
        
        lock = self.locks[key]
        
        with lock:
            # Double-check after acquiring lock
            value = self.cache.get(key)
            if value:
                return value
            
            # Fetch from DB (only one thread does this)
            value = self.db.get(key)
            self.cache.set(key, value, ttl=3600)
            
            return value
```

**Redis Lock:**

```python
def get_with_redis_lock(key, redis_client, db):
    # Try cache
    value = redis_client.get(key)
    if value:
        return value
    
    # Try to acquire lock
    lock_key = f"lock:{key}"
    lock_acquired = redis_client.set(lock_key, '1', nx=True, ex=10)
    
    if lock_acquired:
        # I got the lock - fetch from DB
        value = db.get(key)
        redis_client.set(key, value, ex=3600)
        redis_client.delete(lock_key)
        return value
    else:
        # Someone else is fetching - wait and retry
        time.sleep(0.1)
        return get_with_redis_lock(key, redis_client, db)
```

#### 4. Probabilistic Early Expiration

**ëœë¤í•˜ê²Œ ë¯¸ë¦¬ ë§Œë£Œ:**

```python
import random

def get_with_early_expiration(key, cache, db, base_ttl=3600):
    value, remaining_ttl = cache.get_with_ttl(key)
    
    if value is None:
        # Cache miss
        value = db.get(key)
        cache.set(key, value, ttl=base_ttl)
        return value
    
    # Probabilistic refresh
    # More likely as expiration approaches
    delta = base_ttl - remaining_ttl
    probability = delta * random.random() / base_ttl
    
    if random.random() < probability:
        # Refresh
        value = db.get(key)
        cache.set(key, value, ttl=base_ttl)
    
    return value
```

---

## Multi-Layer Caching

```python
class MultiLevelCache:
    def __init__(self, l1_cache, l2_redis, db):
        """
        l1_cache: Local in-memory cache (LRU)
        l2_redis: Redis (shared)
        db: Database
        """
        self.l1 = l1_cache
        self.l2 = l2_redis
        self.db = db
    
    def get(self, key):
        # L1 (local)
        value = self.l1.get(key)
        if value:
            return value
        
        # L2 (Redis)
        value = self.l2.get(key)
        if value:
            self.l1.set(key, value)  # Populate L1
            return value
        
        # L3 (Database)
        value = self.db.get(key)
        self.l2.set(key, value, ttl=3600)  # Populate L2
        self.l1.set(key, value)  # Populate L1
        
        return value
    
    def set(self, key, value):
        # Invalidate all levels
        self.l1.delete(key)
        self.l2.delete(key)
        
        # Update DB
        self.db.update(key, value)
```

---

## Redis ê³ ê¸‰ ê¸°ëŠ¥

### 1. Pub/Sub (Cache Invalidation)

```python
# Publisher (when data changes)
def update_user(user_id, data):
    db.update(user_id, data)
    
    # Notify all servers
    redis_client.publish('cache_invalidation', f'user:{user_id}')

# Subscriber (on each server)
def cache_invalidation_listener():
    pubsub = redis_client.pubsub()
    pubsub.subscribe('cache_invalidation')
    
    for message in pubsub.listen():
        if message['type'] == 'message':
            key = message['data']
            local_cache.delete(key)
            print(f"Invalidated: {key}")

# Run subscriber in background
threading.Thread(target=cache_invalidation_listener, daemon=True).start()
```

### 2. Lua Scripts (Atomic Operations)

```python
# Atomic compare-and-set
lua_script = """
local current = redis.call('GET', KEYS[1])
if current == ARGV[1] then
    redis.call('SET', KEYS[1], ARGV[2])
    return 1
else
    return 0
end
"""

script = redis_client.register_script(lua_script)

# Usage
success = script(keys=['user:1000:version'], args=[old_version, new_version])
if success:
    print("Updated!")
else:
    print("Conflict!")
```

### 3. Redis Streams (Event Sourcing)

```python
# Producer
redis_client.xadd('events', {
    'type': 'user_updated',
    'user_id': '1000',
    'timestamp': time.time()
})

# Consumer
last_id = '0'
while True:
    events = redis_client.xread({'events': last_id}, block=1000)
    
    for stream, messages in events:
        for message_id, data in messages:
            process_event(data)
            last_id = message_id
```

---

## ì‹¤ì „ ì˜ˆì œ: Twitter Timeline

```python
class TwitterTimeline:
    def __init__(self, redis_client, db):
        self.redis = redis_client
        self.db = db
    
    def post_tweet(self, user_id, tweet_id):
        """User posts a tweet"""
        # 1. Save to DB
        self.db.insert_tweet(user_id, tweet_id)
        
        # 2. Fan-out to followers' timelines (Redis)
        followers = self.db.get_followers(user_id)
        
        for follower_id in followers:
            # Add to follower's timeline (Sorted Set)
            self.redis.zadd(
                f"timeline:{follower_id}",
                {tweet_id: time.time()}
            )
            
            # Keep only recent 1000 tweets
            self.redis.zremrangebyrank(
                f"timeline:{follower_id}",
                0, -1001
            )
    
    def get_timeline(self, user_id, limit=20):
        """Get user's timeline"""
        timeline_key = f"timeline:{user_id}"
        
        # Get from Redis (sorted by timestamp)
        tweet_ids = self.redis.zrevrange(timeline_key, 0, limit-1)
        
        if not tweet_ids:
            # Cold start - load from DB
            tweet_ids = self.db.get_timeline(user_id, limit)
            
            # Populate cache
            for tweet_id in tweet_ids:
                self.redis.zadd(
                    timeline_key,
                    {tweet_id: self.db.get_tweet_time(tweet_id)}
                )
        
        # Fetch tweet details (batch)
        tweets = self.db.get_tweets(tweet_ids)
        
        return tweets
```

---

## ìš”ì•½

**Caching ì „ëµ:**

1. **Cache-Aside**: ê°€ì¥ ì¼ë°˜ì 
2. **Write-Through**: í•­ìƒ ìµœì‹ 
3. **Write-Behind**: ì“°ê¸° ë¹ ë¦„
4. **Refresh-Ahead**: Proactive

**Cache Invalidation:**

1. **TTL**: ê°„ë‹¨
2. **Explicit**: ì •í™•
3. **Lock**: Stampede ë°©ì§€
4. **Probabilistic**: ë¶„ì‚°

**Redis í™œìš©:**
- Session
- Rate Limiting
- Leaderboard
- Pub/Sub
- Streams

**í•µì‹¬:**

> "CacheëŠ” ì•½ì´ì§€ë§Œ, ì˜ëª» ì“°ë©´ ë…!"

**ë‹¤ìŒ ê¸€:**
- **Message Queue**: Kafka, RabbitMQ
- **Microservices**: ì„œë¹„ìŠ¤ ë¶„ë¦¬
- **API Gateway**: ë¼ìš°íŒ…, ì¸ì¦

ê¸°ëŒ€í•´ì£¼ì„¸ìš”! ğŸš€

---

*ì§ˆë¬¸ì´ë‚˜ í”¼ë“œë°±ì€ [GitHub](https://github.com/yhna941)ì—ì„œ í™˜ì˜í•©ë‹ˆë‹¤!*
