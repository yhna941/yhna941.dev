---
title: "LLM Post-training #2: DPO - Reward Model ì—†ì´ ì§ì ‘ í•™ìŠµí•˜ê¸°"
description: "Direct Preference Optimization (DPO)ë¡œ RLHFì˜ ë³µì¡ì„± ì—†ì´ ê°„ë‹¨í•˜ê³  ì•ˆì •ì ìœ¼ë¡œ ëª¨ë¸ì„ ì •ë ¬í•˜ëŠ” ë°©ë²•ì„ ì•Œì•„ë´…ë‹ˆë‹¤."
pubDate: 2026-02-06
author: "Yh Na"
tags: ["llm", "dpo", "alignment", "post-training", "preference-learning"]
draft: false
---

# LLM Post-training #2: DPO

RLHFì˜ ë¬¸ì œ:
- **3ë‹¨ê³„** íŒŒì´í”„ë¼ì¸ (SFT â†’ Reward â†’ PPO)
- Reward model í•™ìŠµ í•„ìš”
- PPO ë¶ˆì•ˆì • (hyperparameter ë¯¼ê°)
- ëŠë¦¼

**DPO (Direct Preference Optimization)**ëŠ” í˜ì‹ ì ì…ë‹ˆë‹¤:
- **1ë‹¨ê³„**ë§Œ! (SFT â†’ DPO)
- Reward model ë¶ˆí•„ìš”
- ì•ˆì •ì  (supervised learningì²˜ëŸ¼)
- ë¹ ë¦„ (2-3ë°°)

ê²°ê³¼:
- Zephyr-7B: DPOë¡œ GPT-3.5 ëŠ¥ê°€
- Starling-7B: DPOë¡œ GPT-4 ê·¼ì ‘
- ê°„ë‹¨í•˜ë©´ì„œ ê°•ë ¥!

---

## í•µì‹¬ ì•„ì´ë””ì–´

### RLHFì˜ ë³µì¡ì„±

```
1. Reward Model í•™ìŠµ
   r(x, y) = RewardModel(x, y)

2. PPOë¡œ Policy ìµœì í™”
   max E[r(x, y) - Î²Â·KL(Ï€||Ï€_ref)]
   
ë¬¸ì œ: ë‘ ë‹¨ê³„, ë¶ˆì•ˆì •, ëŠë¦¼
```

### DPOì˜ ê°„ê²°í•¨

```
Preference dataë§Œ ìˆìœ¼ë©´:
  (x, y_w, y_l)  where y_w > y_l

ì§ì ‘ í•™ìŠµ:
  max P(y_w > y_l | x)
  
ì¥ì : í•œ ë‹¨ê³„, ì•ˆì •, ë¹ ë¦„
```

---

## ìˆ˜í•™ì  ìœ ë„

### 1. RLHF Objective

Rewardë¥¼ ìµœëŒ€í™”í•˜ë˜, reference modelê³¼ ë„ˆë¬´ ë©€ì–´ì§€ì§€ ì•Šê¸°:

```
Ï€* = argmax E_{x~D, y~Ï€(y|x)} [r(x,y) - Î²Â·log(Ï€(y|x)/Ï€_ref(y|x))]
```

### 2. Optimal Policy

ì´ objectiveì˜ ìµœì í•´:

```
Ï€*(y|x) = Ï€_ref(y|x) Â· exp(r(x,y)/Î²) / Z(x)

where Z(x) = Î£_y Ï€_ref(y|x) Â· exp(r(x,y)/Î²)
```

### 3. Reward ì—­ì‚°

ìœ„ ì‹ì„ ì •ë¦¬í•˜ë©´:

```
r(x,y) = Î²Â·log(Ï€*(y|x)/Ï€_ref(y|x)) + Î²Â·log Z(x)
```

### 4. Bradley-Terry Model

Preference í™•ë¥ :

```
P(y_w > y_l | x) = Ïƒ(r(x,y_w) - r(x,y_l))
```

Rewardë¥¼ ëŒ€ì…:

```
P(y_w > y_l | x) = Ïƒ(Î²Â·log(Ï€*(y_w|x)/Ï€_ref(y_w|x)) - Î²Â·log(Ï€*(y_l|x)/Ï€_ref(y_l|x)))
```

**í•µì‹¬:** Z(x) í•­ì´ ì†Œê±°ë¨!

### 5. DPO Loss

```
L_DPO = -E[(x,y_w,y_l)~D] [log Ïƒ(Î²Â·log(Ï€_Î¸(y_w|x)/Ï€_ref(y_w|x)) - Î²Â·log(Ï€_Î¸(y_l|x)/Ï€_ref(y_l|x)))]
```

ê°„ë‹¨íˆ:

```
L_DPO = -log Ïƒ(Î²Â·(log Ï€_Î¸(y_w|x) - log Ï€_ref(y_w|x) - log Ï€_Î¸(y_l|x) + log Ï€_ref(y_l|x)))
```

**Reward model ì—†ì´ ì§ì ‘ í•™ìŠµ!**

---

## êµ¬í˜„

### Naive ë²„ì „

```python
import torch
import torch.nn.functional as F

def dpo_loss(
    policy_model,
    reference_model,
    prompt,
    chosen_response,
    rejected_response,
    beta=0.1
):
    """
    DPO loss ê³„ì‚°
    
    Args:
        policy_model: í•™ìŠµí•  ëª¨ë¸ (Î¸)
        reference_model: ì°¸ì¡° ëª¨ë¸ (frozen)
        prompt: ì…ë ¥
        chosen_response: ì„ í˜¸ ë‹µë³€ (y_w)
        rejected_response: ë¹„ì„ í˜¸ ë‹µë³€ (y_l)
        beta: KL penalty ê³„ìˆ˜
    """
    # Tokenize
    chosen_tokens = tokenizer(prompt + chosen_response, return_tensors="pt")
    rejected_tokens = tokenizer(prompt + rejected_response, return_tensors="pt")
    
    # Log probabilities
    with torch.no_grad():
        ref_chosen_logprobs = reference_model(**chosen_tokens).logits.log_softmax(-1)
        ref_rejected_logprobs = reference_model(**rejected_tokens).logits.log_softmax(-1)
    
    policy_chosen_logprobs = policy_model(**chosen_tokens).logits.log_softmax(-1)
    policy_rejected_logprobs = policy_model(**rejected_tokens).logits.log_softmax(-1)
    
    # Gather log probs for actual tokens
    chosen_logprobs = policy_chosen_logprobs.gather(-1, chosen_tokens.input_ids.unsqueeze(-1)).squeeze(-1).sum()
    rejected_logprobs = policy_rejected_logprobs.gather(-1, rejected_tokens.input_ids.unsqueeze(-1)).squeeze(-1).sum()
    
    ref_chosen_logprobs = ref_chosen_logprobs.gather(-1, chosen_tokens.input_ids.unsqueeze(-1)).squeeze(-1).sum()
    ref_rejected_logprobs = ref_rejected_logprobs.gather(-1, rejected_tokens.input_ids.unsqueeze(-1)).squeeze(-1).sum()
    
    # Log ratios
    chosen_ratio = chosen_logprobs - ref_chosen_logprobs
    rejected_ratio = rejected_logprobs - ref_rejected_logprobs
    
    # DPO loss
    loss = -F.logsigmoid(beta * (chosen_ratio - rejected_ratio))
    
    return loss
```

### ì‹¤ì „ êµ¬í˜„ (TRL)

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from trl import DPOTrainer, DPOConfig
from datasets import load_dataset

# ëª¨ë¸ ë¡œë“œ
model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b-sft")
ref_model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b-sft")
tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-sft")

# ë°ì´í„° ë¡œë“œ
dataset = load_dataset("Anthropic/hh-rlhf")

# ë°ì´í„° í¬ë§·
def format_dataset(example):
    return {
        "prompt": example["prompt"],
        "chosen": example["chosen"],
        "rejected": example["rejected"]
    }

dataset = dataset.map(format_dataset)

# DPO ì„¤ì •
training_args = DPOConfig(
    output_dir="./llama2-7b-dpo",
    num_train_epochs=1,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    learning_rate=5e-7,
    beta=0.1,  # KL penalty
    logging_steps=10,
    save_strategy="epoch",
    remove_unused_columns=False
)

# Trainer
dpo_trainer = DPOTrainer(
    model=model,
    ref_model=ref_model,
    args=training_args,
    train_dataset=dataset["train"],
    tokenizer=tokenizer
)

# í•™ìŠµ
dpo_trainer.train()
```

---

## Beta íŒŒë¼ë¯¸í„°

### ì—­í• 

```
Î² = 0.01: ê±°ì˜ ë³€í™” ì—†ìŒ (ì•ˆì „)
Î² = 0.1:  ì ë‹¹í•œ ë³€í™” (ê¶Œì¥)
Î² = 1.0:  í° ë³€í™” (ìœ„í—˜)
```

### ì„ íƒ ê°€ì´ë“œ

```python
# ì‘ì€ ëª¨ë¸ (7B)
beta = 0.1

# ì¤‘ê°„ ëª¨ë¸ (13B-30B)
beta = 0.05

# í° ëª¨ë¸ (70B+)
beta = 0.01

# ë„ë©”ì¸ shift í° ê²½ìš°
beta = 0.2
```

---

## DPO vs RLHF

### í•™ìŠµ ì‹œê°„

**RLHF:**
```
SFT:          10 hours
Reward Model: 5 hours
PPO:          20 hours
Total:        35 hours
```

**DPO:**
```
SFT: 10 hours
DPO: 8 hours
Total: 18 hours  (2ë°° ë¹ ë¦„!)
```

### ë©”ëª¨ë¦¬

**RLHF:**
```
Policy model:    7B params
Value model:     7B params
Reference model: 7B params
Reward model:    7B params
Total:           28B (4 models!)
```

**DPO:**
```
Policy model:    7B params
Reference model: 7B params
Total:           14B (2 models)
```

### ì•ˆì •ì„±

**RLHF PPO:**
```python
# Hyperparameters
ppo_epochs = 4
clip_range = 0.2
vf_coef = 0.5
entropy_coef = 0.01
gae_lambda = 0.95
target_kl = 0.1
# ... ë§ìŒ!

# ë¶ˆì•ˆì •í•˜ë©´ ë°œì‚°
```

**DPO:**
```python
# Hyperparameters
learning_rate = 5e-7
beta = 0.1
# ë!

# Supervised learningì²˜ëŸ¼ ì•ˆì •
```

---

## ê°œì„  ë²„ì „ë“¤

### 1. IPO (Identity Preference Optimization)

**ë¬¸ì œ:** DPOëŠ” logit ì°¨ì´ì— ë¯¼ê°

**í•´ê²°:**
```
L_IPO = E[(r(x,y_w) - r(x,y_l) - 1)^2]

ê°„ë‹¨í•œ MSE loss!
```

```python
def ipo_loss(policy_logprobs, ref_logprobs, beta=0.1):
    log_ratio = policy_logprobs - ref_logprobs
    loss = (log_ratio - 1) ** 2
    return loss.mean()
```

### 2. KTO (Kahneman-Tversky Optimization)

**ë¬¸ì œ:** Pairwise comparison ë°ì´í„° ìˆ˜ì§‘ ì–´ë ¤ì›€

**í•´ê²°:** Binary feedbackë§Œ ì‚¬ìš©
```
Data: (x, y, label)
  label âˆˆ {ì¢‹ìŒ, ë‚˜ì¨}

L_KTO = E[loss(y, label)]
```

```python
def kto_loss(
    policy_logprobs,
    ref_logprobs,
    label,  # 0 or 1
    beta=0.1
):
    log_ratio = policy_logprobs - ref_logprobs
    
    if label == 1:  # ì¢‹ì€ ë‹µë³€
        loss = -F.logsigmoid(beta * log_ratio)
    else:  # ë‚˜ìœ ë‹µë³€
        loss = -F.logsigmoid(-beta * log_ratio)
    
    return loss
```

### 3. ORPO (Odds Ratio Preference Optimization)

**ë¬¸ì œ:** Reference model í•„ìš” (ë©”ëª¨ë¦¬)

**í•´ê²°:** Reference ì—†ì´ í•™ìŠµ
```
L_ORPO = L_SFT + Î»Â·L_OR

where:
  L_OR = log(odds(y_w)/odds(y_l))
  odds(y) = p(y)/(1-p(y))
```

```python
def orpo_loss(
    logits,
    chosen_tokens,
    rejected_tokens,
    lambda_coef=0.1
):
    # SFT loss
    sft_loss = F.cross_entropy(logits, chosen_tokens)
    
    # Odds ratio loss
    chosen_probs = F.softmax(logits, dim=-1).gather(-1, chosen_tokens)
    rejected_probs = F.softmax(logits, dim=-1).gather(-1, rejected_tokens)
    
    chosen_odds = chosen_probs / (1 - chosen_probs + 1e-8)
    rejected_odds = rejected_probs / (1 - rejected_probs + 1e-8)
    
    or_loss = -torch.log(chosen_odds / rejected_odds).mean()
    
    return sft_loss + lambda_coef * or_loss
```

---

## ì‹¤ì „ íŒ

### 1. ë°ì´í„° í’ˆì§ˆ

```python
# ì¢‹ì€ preference data
{
  "prompt": "Explain quantum entanglement",
  "chosen": "Quantum entanglement is a phenomenon where...",  # ìƒì„¸, ì •í™•
  "rejected": "It's when particles are connected"  # ì§§ê³  ë¶ˆì¶©ë¶„
}

# ë‚˜ìœ preference data
{
  "prompt": "What's 2+2?",
  "chosen": "4",
  "rejected": "5"  # ë„ˆë¬´ ëª…í™•, í•™ìŠµ ê°€ì¹˜ ë‚®ìŒ
}
```

**ê·œì¹™:**
- Marginì´ ì ë‹¹íˆ ìˆì–´ì•¼ í•¨
- ëª…ë°±í•œ ì°¨ì´ë³´ë‹¤ ë¯¸ë¬˜í•œ ì°¨ì´
- ë‹¤ì–‘í•œ ì¸¡ë©´ (ì •í™•ë„, ìœ ìš©ì„±, ì•ˆì „ì„±)

### 2. Learning Rate

```python
# DPOëŠ” ë§¤ìš° ì‘ì€ LR í•„ìš”
learning_rate = 5e-7  # RLHFë³´ë‹¤ 10ë°° ì‘ìŒ

# í° ëª¨ë¸ì€ ë” ì‘ê²Œ
if model_size >= 70B:
    learning_rate = 1e-7
```

### 3. í‰ê°€

```python
# í•™ìŠµ ì¤‘ ëª¨ë‹ˆí„°ë§
metrics = {
    "chosen_reward": chosen_ratio.mean(),
    "rejected_reward": rejected_ratio.mean(),
    "reward_margin": (chosen_ratio - rejected_ratio).mean(),
    "reward_accuracy": (chosen_ratio > rejected_ratio).float().mean()
}

# Reward margin > 0 ìœ ì§€
# Reward accuracy > 60% ëª©í‘œ
```

---

## ë²¤ì¹˜ë§ˆí¬

### Zephyr-7B (DPO)

| ëª¨ë¸ | Method | MT-Bench | AlpacaEval |
|------|--------|----------|------------|
| Llama-2-7B-chat | RLHF | 6.27 | - |
| Mistral-7B-Instruct | - | 6.84 | - |
| **Zephyr-7B-beta** | **DPO** | **7.34** | **90.6%** |

DPOê°€ RLHFë³´ë‹¤ ì¢‹ìŒ!

### Starling-7B (DPO)

| ëª¨ë¸ | MT-Bench | AlpacaEval 2.0 |
|------|----------|----------------|
| GPT-4-Turbo | 9.32 | 50.0% |
| Claude-3-Opus | 9.00 | 40.5% |
| **Starling-LM-7B** | **8.09** | **36.6%** |
| Llama-2-70B-chat | 6.86 | 13.9% |

7B ëª¨ë¸ì´ 70B ëŠ¥ê°€!

---

## DPOì˜ í•œê³„

### 1. Length Bias

DPOëŠ” ê¸´ ë‹µë³€ ì„ í˜¸:

```python
# ë¬¸ì œ
chosen = "Short answer."
rejected = "Very very very long but wrong answer..."

# DPOëŠ” rejectedì— ë†’ì€ í™•ë¥  (ê¸¸ì´ ë•Œë¬¸)
```

**í•´ê²°:**
- Length-normalized rewards
- Explicit length penalty

```python
def length_normalized_dpo_loss(
    chosen_logprobs,
    rejected_logprobs,
    chosen_length,
    rejected_length,
    beta=0.1
):
    # Normalize by length
    chosen_logprobs = chosen_logprobs / chosen_length
    rejected_logprobs = rejected_logprobs / rejected_length
    
    log_ratio = beta * (chosen_logprobs - rejected_logprobs)
    loss = -F.logsigmoid(log_ratio)
    
    return loss
```

### 2. Reward Hacking

DPOë„ reward hacking ê°€ëŠ¥:

```python
# ëª¨ë¸ì´ íŠ¹ì • íŒ¨í„´ í•™ìŠµ
"I'm happy to help! ..." â†’ ë†’ì€ í™•ë¥ 
(ì‹¤ì œ ë‚´ìš© ìƒê´€ì—†ì´)
```

**í•´ê²°:**
- ë‹¤ì–‘í•œ ë°ì´í„°
- Iterative DPO

### 3. Out-of-distribution

Training dataì™€ ë‹¤ë¥¸ ì…ë ¥ì— ì•½í•¨:

```python
# Training: ì˜ì–´ ëŒ€í™”
# Test: ì½”ë“œ ìƒì„± â†’ ì„±ëŠ¥ í•˜ë½
```

**í•´ê²°:**
- ë‹¤ì–‘í•œ ë„ë©”ì¸ ë°ì´í„°
- Domain-specific DPO

---

## ê³ ê¸‰ ê¸°ë²•

### 1. Iterative DPO

```python
# Round 1
model_v1 = dpo_train(sft_model, preference_data_v1)

# Generate new data with v1
new_data = generate_preference_data(model_v1)

# Round 2
model_v2 = dpo_train(model_v1, new_data)

# Repeat...
```

### 2. Multi-objective DPO

ì—¬ëŸ¬ ëª©í‘œ ë™ì‹œ ìµœì í™”:

```python
# Helpfulness + Harmlessness + Honesty
loss = (
    w1 * dpo_loss(helpful_data) +
    w2 * dpo_loss(harmless_data) +
    w3 * dpo_loss(honest_data)
)
```

### 3. Conditional DPO

ì¡°ê±´ë¶€ í•™ìŠµ:

```python
# Persona-specific
loss = dpo_loss(
    prompt="[Friendly] " + user_input,
    chosen=friendly_response,
    rejected=formal_response
)
```

---

## ì‹¤ì „ ì˜ˆì œ: ì „ì²´ íŒŒì´í”„ë¼ì¸

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from trl import SFTTrainer, DPOTrainer
from datasets import load_dataset

# 1. SFT
print("Stage 1: Supervised Fine-Tuning")
base_model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b-hf")
sft_dataset = load_dataset("yahma/alpaca-cleaned")

sft_trainer = SFTTrainer(
    model=base_model,
    train_dataset=sft_dataset["train"],
    max_seq_length=512,
    packing=True
)
sft_trainer.train()
sft_trainer.save_model("./llama2-7b-sft")

# 2. DPO
print("Stage 2: Direct Preference Optimization")
sft_model = AutoModelForCausalLM.from_pretrained("./llama2-7b-sft")
ref_model = AutoModelForCausalLM.from_pretrained("./llama2-7b-sft")
dpo_dataset = load_dataset("Anthropic/hh-rlhf")

dpo_trainer = DPOTrainer(
    model=sft_model,
    ref_model=ref_model,
    train_dataset=dpo_dataset["train"],
    beta=0.1,
    max_length=512,
    max_prompt_length=256
)
dpo_trainer.train()
dpo_trainer.save_model("./llama2-7b-dpo")

# 3. Evaluation
print("Stage 3: Evaluation")
model = AutoModelForCausalLM.from_pretrained("./llama2-7b-dpo")
tokenizer = AutoTokenizer.from_pretrained("./llama2-7b-dpo")

test_prompts = [
    "Explain quantum mechanics simply",
    "Write a poem about AI",
    "How to make a website?"
]

for prompt in test_prompts:
    inputs = tokenizer(prompt, return_tensors="pt")
    outputs = model.generate(**inputs, max_new_tokens=200)
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    print(f"\nPrompt: {prompt}")
    print(f"Response: {response}")
```

---

## ìš”ì•½

**DPO**ëŠ”:

1. **ê°„ë‹¨**: Reward model ë¶ˆí•„ìš”
2. **ë¹ ë¦„**: RLHFë³´ë‹¤ 2ë°° ë¹ ë¦„
3. **ì•ˆì •**: Supervised learningì²˜ëŸ¼
4. **íš¨ê³¼ì **: Zephyr, Starling ë“± SOTA

**Loss:**
```
L_DPO = -log Ïƒ(Î²Â·log(Ï€_Î¸(y_w)/Ï€_ref(y_w)) - Î²Â·log(Ï€_Î¸(y_l)/Ï€_ref(y_l)))
```

**í•µì‹¬:**
- Preference dataë§Œ ìˆìœ¼ë©´ ë¨
- Reference modelê³¼ ë¹„êµ
- Betaë¡œ ë³€í™”ëŸ‰ ì¡°ì ˆ

**ê°œì„ :**
- IPO: MSE loss
- KTO: Binary feedback
- ORPO: Reference ì—†ì´

**ë‹¤ìŒ ê¸€:**
- **GRPO**: Group-based reward
- **Online RL**: ì‹¤ì‹œê°„ í”¼ë“œë°±

ê¸°ëŒ€í•´ì£¼ì„¸ìš”! ğŸš€

---

*ì§ˆë¬¸ì´ë‚˜ í”¼ë“œë°±ì€ [GitHub](https://github.com/yhna941)ì—ì„œ í™˜ì˜í•©ë‹ˆë‹¤!*
