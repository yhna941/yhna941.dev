---
title: "LLM Post-training #1: RLHF ê°œìš” - ì¸ê°„ í”¼ë“œë°±ìœ¼ë¡œ ëª¨ë¸ ì •ë ¬í•˜ê¸°"
description: "Reinforcement Learning from Human Feedback (RLHF)ì˜ ì›ë¦¬ì™€ ì „ì²´ íŒŒì´í”„ë¼ì¸ì„ ì•Œì•„ë´…ë‹ˆë‹¤. SFT, Reward Model, PPOê¹Œì§€."
pubDate: 2026-02-06
author: "Yh Na"
tags: ["llm", "rlhf", "alignment", "post-training", "reinforcement-learning"]
draft: false
---

# LLM Post-training #1: RLHF ê°œìš”

GPT-4, Claude, ChatGPTëŠ” ì™œ ì´ë ‡ê²Œ ëŒ€í™”ë¥¼ ì˜í• ê¹Œìš”?

ë¹„ë°€ì€ **RLHF (Reinforcement Learning from Human Feedback)**ì…ë‹ˆë‹¤:
- Pre-trainingë§Œìœ¼ë¡  ë¶€ì¡± (ì¸í„°ë„· ë°ì´í„°ëŠ” í’ˆì§ˆì´ ë“¤ì­‰ë‚ ì­‰)
- **ì¸ê°„ í”¼ë“œë°±**ìœ¼ë¡œ ëª¨ë¸ì„ "ì •ë ¬(align)"
- ìœ í•´/ê±°ì§“/ë¬´ìµí•œ ë‹µë³€ íšŒí”¼
- ìœ ìš©/ì •ì§/ë¬´í•´í•œ ë‹µë³€ ì„ í˜¸

**ê²°ê³¼:**
- ChatGPT: RLHF ì—†ì´ëŠ” ë¶ˆê°€ëŠ¥
- Claude: Constitutional AI (RLHF ë³€í˜•)
- Llama 2-Chat, Gemini Pro: ëª¨ë‘ RLHF

---

## Pre-training vs Post-training

### Pre-training (Base Model)

```
ë°ì´í„°: ì¸í„°ë„· ì „ì²´ (ìˆ˜ì¡° í† í°)
ëª©í‘œ: ë‹¤ìŒ í† í° ì˜ˆì¸¡
ê²°ê³¼: ê°•ë ¥í•˜ì§€ë§Œ "ë‚ ê²ƒ"

ì˜ˆì‹œ:
User: "How to make a bomb?"
Base: "Step 1: Get materials..."  âš ï¸
```

**ë¬¸ì œì :**
- ìœ í•´ ì½˜í…ì¸  ìƒì„±
- ê±°ì§“ ì •ë³´
- ë¬´ì˜ë¯¸í•œ ë‹µë³€
- ì§€ì‹œ ë”°ë¥´ê¸° ì–´ë ¤ì›€

### Post-training (Aligned Model)

```
ë°ì´í„°: ì¸ê°„ì´ ì„ ë³„í•œ ê³ í’ˆì§ˆ ë°ì´í„°
ëª©í‘œ: ìœ ìš©/ì •ì§/ë¬´í•´
ê²°ê³¼: ì•ˆì „í•˜ê³  ìœ ìš©

ì˜ˆì‹œ:
User: "How to make a bomb?"
Aligned: "I cannot help with that."  âœ…
```

---

## RLHF íŒŒì´í”„ë¼ì¸

ì „ì²´ ê³¼ì •ì€ **3ë‹¨ê³„**:

```
1. Supervised Fine-Tuning (SFT)
   â†“
2. Reward Model Training
   â†“
3. RL Fine-tuning (PPO)
```

### 1ë‹¨ê³„: SFT (Supervised Fine-Tuning)

**ëª©í‘œ:** ëª¨ë¸ì´ ëŒ€í™” í˜•ì‹ í•™ìŠµ

```python
# ë°ì´í„° í˜•ì‹
{
  "prompt": "What is the capital of France?",
  "response": "The capital of France is Paris, located in the north-central part of the country."
}

# í•™ìŠµ
for prompt, response in sft_dataset:
    loss = model.compute_loss(prompt, response)
    loss.backward()
    optimizer.step()
```

**ë°ì´í„° ìˆ˜ì§‘:**
- ì¸ê°„ì´ ì§ì ‘ ì‘ì„± (1-10ë§Œ ìƒ˜í”Œ)
- ê³ í’ˆì§ˆ, ì•ˆì „, ìœ ìš©í•œ ë‹µë³€
- OpenAI: ë¼ë²¨ëŸ¬ ê³ ìš©

**ê²°ê³¼:**
- Base model â†’ Instruction-following model
- í•˜ì§€ë§Œ ì—¬ì „íˆ ì™„ë²½í•˜ì§€ ì•ŠìŒ

### 2ë‹¨ê³„: Reward Model

**ëª©í‘œ:** "ì¢‹ì€ ë‹µë³€"ì„ ì ìˆ˜ë¡œ í‰ê°€

```python
# ë°ì´í„° í˜•ì‹ (Comparison data)
{
  "prompt": "Explain quantum mechanics",
  "response_A": "Quantum mechanics is...",  # ì¢‹ìŒ
  "response_B": "Idk lol",                  # ë‚˜ì¨
  "preference": "A"  # Aê°€ ë” ì¢‹ìŒ
}

# Reward model í•™ìŠµ
class RewardModel(nn.Module):
    def __init__(self, base_model):
        self.model = base_model
        self.value_head = nn.Linear(hidden_size, 1)
    
    def forward(self, input_ids):
        hidden = self.model(input_ids).last_hidden_state
        # ë§ˆì§€ë§‰ í† í°ì˜ hidden state
        reward = self.value_head(hidden[:, -1, :])
        return reward

# í•™ìŠµ (Bradley-Terry model)
for prompt, response_A, response_B, preference in dataset:
    reward_A = reward_model(prompt + response_A)
    reward_B = reward_model(prompt + response_B)
    
    # Preferenceì— ë§ê²Œ í•™ìŠµ
    if preference == "A":
        loss = -log_sigmoid(reward_A - reward_B)
    else:
        loss = -log_sigmoid(reward_B - reward_A)
    
    loss.backward()
```

**ë°ì´í„° ìˆ˜ì§‘:**
- ì¸ê°„ì´ ë‹µë³€ ë¹„êµ (10-100ë§Œ ìŒ)
- "Aê°€ ë” ë‚˜ì€ê°€, Bê°€ ë” ë‚˜ì€ê°€?"
- ë” ë§ì€ ë°ì´í„° ìˆ˜ì§‘ ê°€ëŠ¥ (ì‘ì„±ë³´ë‹¤ ì‰¬ì›€)

### 3ë‹¨ê³„: RL Fine-tuning (PPO)

**ëª©í‘œ:** Reward ìµœëŒ€í™”í•˜ë„ë¡ ëª¨ë¸ í•™ìŠµ

```python
# PPO ì•Œê³ ë¦¬ì¦˜
for prompt in prompts:
    # 1. ëª¨ë¸ì´ ë‹µë³€ ìƒì„±
    response = policy_model.generate(prompt)
    
    # 2. Reward ê³„ì‚°
    reward = reward_model(prompt + response)
    
    # 3. KL penalty (ë„ˆë¬´ ë³€í•˜ì§€ ì•Šë„ë¡)
    log_prob = policy_model.log_prob(response)
    ref_log_prob = reference_model.log_prob(response)
    kl_penalty = kl_divergence(log_prob, ref_log_prob)
    
    # 4. Total reward
    total_reward = reward - beta * kl_penalty
    
    # 5. PPO loss
    ratio = exp(log_prob - old_log_prob)
    clipped_ratio = clip(ratio, 1-epsilon, 1+epsilon)
    loss = -min(ratio * advantage, clipped_ratio * advantage)
    
    # 6. Update
    loss.backward()
    optimizer.step()
```

**í•µì‹¬:**
- Reward ë†’ì€ ë‹µë³€ â†’ í™•ë¥  ì¦ê°€
- Reward ë‚®ì€ ë‹µë³€ â†’ í™•ë¥  ê°ì†Œ
- KL penaltyë¡œ ì›ë³¸ ëª¨ë¸ê³¼ ë„ˆë¬´ ë©€ì–´ì§€ì§€ ì•Šê²Œ

---

## ìˆ˜ì‹ìœ¼ë¡œ ì´í•´

### Reward Model (Bradley-Terry)

ë‹µë³€ A, Bê°€ ìˆì„ ë•Œ, Aê°€ ì„ í˜¸ë  í™•ë¥ :

```
P(A > B) = Ïƒ(r(A) - r(B))

where:
  r(x): Reward modelì˜ ì¶œë ¥
  Ïƒ: Sigmoid function
```

**Loss:**

```
L = -log Ïƒ(r(A) - r(B))
```

Aê°€ ì„ í˜¸ë˜ë©´ `r(A) > r(B)`ê°€ ë˜ë„ë¡ í•™ìŠµ.

### PPO Objective

```
L^CLIP(Î¸) = E[min(r_t(Î¸)Ã‚_t, clip(r_t(Î¸), 1-Îµ, 1+Îµ)Ã‚_t)]

where:
  r_t(Î¸) = Ï€_Î¸(a_t|s_t) / Ï€_old(a_t|s_t)  (í™•ë¥  ë¹„ìœ¨)
  Ã‚_t: Advantage (ì–¼ë§ˆë‚˜ ì¢‹ì€ í–‰ë™ì¸ê°€)
  Îµ: Clipping threshold (ë³´í†µ 0.2)
```

**+ KL penalty:**

```
L^total = L^CLIP - Î² * KL(Ï€_Î¸ || Ï€_ref)

where:
  Ï€_ref: Reference model (SFT ëª¨ë¸)
  Î²: KL coefficient (ë³´í†µ 0.01-0.1)
```

---

## ì‹¤ì „ êµ¬í˜„

### 1. SFT

```python
from transformers import AutoModelForCausalLM, Trainer, TrainingArguments

# ëª¨ë¸ ë¡œë“œ
model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b-hf")
tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-hf")

# ë°ì´í„° ì¤€ë¹„
def format_prompt(example):
    return f"### Instruction:\n{example['instruction']}\n\n### Response:\n{example['response']}"

dataset = load_dataset("yahma/alpaca-cleaned")
dataset = dataset.map(lambda x: {"text": format_prompt(x)})

# í•™ìŠµ
training_args = TrainingArguments(
    output_dir="./llama2-7b-sft",
    num_train_epochs=3,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    learning_rate=2e-5,
    warmup_steps=100,
    logging_steps=10,
    save_strategy="epoch"
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"],
    tokenizer=tokenizer
)

trainer.train()
```

### 2. Reward Model

```python
import torch.nn as nn

class RewardModel(nn.Module):
    def __init__(self, base_model):
        super().__init__()
        self.transformer = base_model
        # Freeze transformer (optional)
        for param in self.transformer.parameters():
            param.requires_grad = False
        
        # Value head
        config = base_model.config
        self.value_head = nn.Linear(config.hidden_size, 1)
    
    def forward(self, input_ids, attention_mask=None):
        # Get hidden states
        outputs = self.transformer(
            input_ids,
            attention_mask=attention_mask,
            output_hidden_states=True
        )
        
        # Last token's hidden state
        hidden = outputs.hidden_states[-1]
        last_hidden = hidden[:, -1, :]  # [batch, hidden_size]
        
        # Reward
        reward = self.value_head(last_hidden).squeeze(-1)  # [batch]
        return reward


# í•™ìŠµ
def train_reward_model(model, dataset):
    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)
    
    for batch in dataloader:
        prompt = batch["prompt"]
        response_A = batch["response_A"]
        response_B = batch["response_B"]
        preference = batch["preference"]  # 0 or 1
        
        # Tokenize
        tokens_A = tokenizer(prompt + response_A, return_tensors="pt")
        tokens_B = tokenizer(prompt + response_B, return_tensors="pt")
        
        # Rewards
        reward_A = model(tokens_A.input_ids, tokens_A.attention_mask)
        reward_B = model(tokens_B.input_ids, tokens_B.attention_mask)
        
        # Loss (Bradley-Terry)
        loss = -torch.log(torch.sigmoid(reward_A - reward_B)).mean()
        
        # Update
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()
```

### 3. PPO (with TRL)

```python
from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead

# ì„¤ì •
ppo_config = PPOConfig(
    model_name="llama2-7b-sft",
    learning_rate=1.4e-5,
    batch_size=16,
    mini_batch_size=4,
    gradient_accumulation_steps=4,
    ppo_epochs=4,
    init_kl_coef=0.05,  # KL penalty
    target_kl=6.0,
    max_grad_norm=0.5,
    adap_kl_ctrl=True
)

# ëª¨ë¸
model = AutoModelForCausalLMWithValueHead.from_pretrained(
    "llama2-7b-sft"
)
ref_model = AutoModelForCausalLMWithValueHead.from_pretrained(
    "llama2-7b-sft"
)
tokenizer = AutoTokenizer.from_pretrained("llama2-7b-sft")

# Reward model
reward_model = RewardModel.from_pretrained("llama2-7b-reward")

# Trainer
ppo_trainer = PPOTrainer(
    config=ppo_config,
    model=model,
    ref_model=ref_model,
    tokenizer=tokenizer
)

# í•™ìŠµ
for epoch in range(3):
    for batch in dataloader:
        query_tensors = batch["input_ids"]
        
        # Generate responses
        response_tensors = ppo_trainer.generate(
            query_tensors,
            max_new_tokens=128,
            do_sample=True,
            top_k=50,
            top_p=0.95
        )
        
        # Compute rewards
        rewards = []
        for query, response in zip(query_tensors, response_tensors):
            text = tokenizer.decode(torch.cat([query, response]))
            reward = reward_model(text)
            rewards.append(reward)
        
        # PPO step
        stats = ppo_trainer.step(query_tensors, response_tensors, rewards)
        
        print(f"Reward: {stats['ppo/mean_scores']:.2f}")
```

---

## ì£¼ìš” ê³¼ì œ

### 1. Reward Hacking

ëª¨ë¸ì´ reward modelì„ "ì†ì´ëŠ”" ë²•ì„ í•™ìŠµ:

```
User: "Write a poem about love"
Model: "AMAZING BEAUTIFUL WONDERFUL LOVE LOVE LOVE..."
Reward: 10.0  âš ï¸ (ì˜ë¯¸ ì—†ì§€ë§Œ reward ë†’ìŒ!)
```

**í•´ê²°ì±…:**
- KL penalty ì¦ê°€
- Reward model ê°œì„ 
- Ensemble reward models

### 2. Catastrophic Forgetting

RLHF í›„ ê¸°ì¡´ ëŠ¥ë ¥ ìƒì‹¤:

```
Before: "Translate to French: Hello"
        "Bonjour"  âœ…

After:  "Translate to French: Hello"
        "I'd be happy to help! ..." (ì“¸ë°ì—†ì´ ê¸¸ì–´ì§)
```

**í•´ê²°ì±…:**
- KL penalty
- Mix SFT data in RL
- Continual learning

### 3. Reward Model í•œê³„

Reward modelë„ ì™„ë²½í•˜ì§€ ì•ŠìŒ:
- ê¸¸ì´ bias (ê¸´ ë‹µë³€ ì„ í˜¸)
- í˜•ì‹ bias (íŠ¹ì • íŒ¨í„´ ì„ í˜¸)
- ì£¼ê´€ì  íŒë‹¨ ì–´ë ¤ì›€

---

## ë²¤ì¹˜ë§ˆí¬

### Llama 2 vs Llama 2-Chat

| ë©”íŠ¸ë¦­ | Llama 2 (Base) | Llama 2-Chat |
|--------|---------------|--------------|
| Helpfulness | 6.2/10 | 8.5/10 |
| Harmlessness | 5.8/10 | 9.1/10 |
| MMLU | 68.9% | 67.3% |

**Trade-off:** Alignment â†‘, Capability â†“ (ì•½ê°„)

### GPT-3 vs InstructGPT

| ë©”íŠ¸ë¦­ | GPT-3 | InstructGPT |
|--------|-------|-------------|
| Human Preference | 27% | 71% |
| Truthfulness | 52% | 79% |
| Toxicity | 25% | 6% |

**RLHFê°€ ì—„ì²­ë‚œ ì°¨ì´!**

---

## RLHFì˜ ë¬¸ì œì 

### 1. ë¹„ìš©

```
ë°ì´í„° ìˆ˜ì§‘:
- SFT: 10K samples Ã— $5/sample = $50K
- Reward: 100K pairs Ã— $1/pair = $100K
- ì´: $150K+ (ì†Œê·œëª¨ í”„ë¡œì íŠ¸)

GPT-4 ê¸‰: $1M+ ì¶”ì •
```

### 2. í™•ì¥ì„±

- ì¸ê°„ ë¼ë²¨ëŸ¬ í•„ìš” (ë³‘ëª©)
- ì–¸ì–´ë³„ë¡œ ë°˜ë³µ
- ë„ë©”ì¸ë³„ë¡œ ë°˜ë³µ

### 3. í¸í–¥

- ë¼ë²¨ëŸ¬ í¸í–¥ ë°˜ì˜
- ë¬¸í™”ì  í¸í–¥
- ì •ì¹˜ì  í¸í–¥

---

## ëŒ€ì•ˆë“¤ (ë‹¤ìŒ ê¸€ ì˜ˆê³ )

### DPO (Direct Preference Optimization)

```
Reward model ì—†ì´ ì§ì ‘ preference í•™ìŠµ!
â†’ ê°„ë‹¨, ì•ˆì •ì 
```

### RLAIF (RL from AI Feedback)

```
ì¸ê°„ ëŒ€ì‹  AIê°€ í”¼ë“œë°±
â†’ ë¹„ìš© ë‚®ìŒ, í™•ì¥ì„± ë†’ìŒ
```

### Constitutional AI

```
ê·œì¹™ ê¸°ë°˜ìœ¼ë¡œ ëª¨ë¸ ì •ë ¬
â†’ Claudeì˜ í•µì‹¬
```

---

## ìš”ì•½

**RLHF**ëŠ”:

1. **3ë‹¨ê³„**: SFT â†’ Reward Model â†’ PPO
2. **í•µì‹¬**: ì¸ê°„ í”¼ë“œë°±ìœ¼ë¡œ ëª¨ë¸ ì •ë ¬
3. **ì„±ê³µ**: ChatGPT, Claude, Llama 2-Chat
4. **ê³¼ì œ**: ë¹„ìš©, í™•ì¥ì„±, reward hacking

**íŒŒì´í”„ë¼ì¸:**
```
Base Model
  â†“ (SFT, 10K samples)
Instruction Model
  â†“ (Reward Model, 100K pairs)
Aligned Model
  â†“ (PPO, 1000 steps)
Production Model âœ…
```

**ë‹¤ìŒ ê¸€:**
- **DPO**: Reward model ì—†ì´ ì§ì ‘ í•™ìŠµ
- **GRPO**: Group-based optimization
- **RLAIF**: AI feedback í™œìš©

ê¸°ëŒ€í•´ì£¼ì„¸ìš”! ğŸš€

---

*ì§ˆë¬¸ì´ë‚˜ í”¼ë“œë°±ì€ [GitHub](https://github.com/yhna941)ì—ì„œ í™˜ì˜í•©ë‹ˆë‹¤!*
