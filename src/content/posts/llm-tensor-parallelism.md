---
title: "LLM Inference ìµœì í™” #8: Tensor Parallelism - ê±°ëŒ€ ëª¨ë¸ì„ ì—¬ëŸ¬ GPUì— ë‚˜ëˆ„ê¸°"
description: "í•˜ë‚˜ì˜ ë ˆì´ì–´ë¥¼ ì—¬ëŸ¬ GPUì— ë¶„ì‚°ì‹œí‚¤ëŠ” Tensor Parallelismìœ¼ë¡œ ë©”ëª¨ë¦¬ í•œê³„ë¥¼ ê·¹ë³µí•˜ëŠ” ë°©ë²•ì„ ì•Œì•„ë´…ë‹ˆë‹¤."
pubDate: 2026-02-06
author: "Yh Na"
tags: ["llm", "optimization", "parallelism", "distributed", "multi-gpu"]
draft: false
---

# LLM Inference ìµœì í™” #8: Tensor Parallelism

GPT-3 175BëŠ” **350GB** ë©”ëª¨ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤ (FP16). ë‹¨ì¼ GPUë¡œëŠ” ë¶ˆê°€ëŠ¥í•˜ì£ .

**Tensor Parallelism**ì€ ì´ë¥¼ í•´ê²°í•©ë‹ˆë‹¤:
- **í•˜ë‚˜ì˜ ë ˆì´ì–´ë¥¼ ì—¬ëŸ¬ GPUì— ë¶„ì‚°**
- ê° GPUê°€ **ì¼ë¶€ë§Œ ê³„ì‚°**
- í†µì‹ ìœ¼ë¡œ ê²°ê³¼ ê²°í•©
- **ë©”ëª¨ë¦¬: Nê°œ GPUë¡œ 1/N**

ê²°ê³¼:
- 175B ëª¨ë¸ â†’ 8ê°œ A100ìœ¼ë¡œ ì‹¤í–‰
- í†µì‹  ì˜¤ë²„í—¤ë“œ ìµœì†Œí™”
- Linear scaling (ì´ë¡ ì )

---

## ë¬¸ì œ: ëª¨ë¸ì´ GPUì— ì•ˆ ë“¤ì–´ê°

### ë©”ëª¨ë¦¬ ê³„ì‚°

**GPT-3 175B (FP16):**
```
Parameters: 175B Ã— 2 bytes = 350 GB
+ Activations: ~50 GB
+ KV Cache (batch 32): ~100 GB
= 500 GB ì´ í•„ìš”
```

**A100 80GBë¡œëŠ” ë¶ˆê°€ëŠ¥!**

### ê¸°ì¡´ í•´ê²°ì±…ë“¤

**1. Data Parallelism (ì•ˆ ë¨)**
```
ê° GPUê°€ ì „ì²´ ëª¨ë¸ ë³µì‚¬ â†’ ë©”ëª¨ë¦¬ ë™ì¼
```

**2. Model Parallelism (ìˆœì°¨)**
```
GPU 1: Layer 1-10
GPU 2: Layer 11-20
GPU 3: Layer 21-30
...

ë¬¸ì œ: GPU í™œìš©ë¥  ë‚®ìŒ (ìˆœì°¨ ì‹¤í–‰)
```

---

## Tensor Parallelism

### í•µì‹¬ ì•„ì´ë””ì–´

> **í•˜ë‚˜ì˜ í–‰ë ¬ ì—°ì‚°ì„ ì—¬ëŸ¬ GPUì— ë‚˜ëˆˆë‹¤**

```
Standard:
Y = XW    # W: [4096, 4096]

Tensor Parallel (2 GPUs):
W = [W1 | W2]  # Split column-wise

GPU 0: Y1 = X @ W1  # [batch, 2048]
GPU 1: Y2 = X @ W2  # [batch, 2048]

Y = [Y1 | Y2]  # Concat
```

### ìˆ˜ì‹

**Linear layer:**
```
Y = XW + b
  where W: [d_in, d_out]
```

**Split W column-wise (n GPUs):**
```
W = [W_1, W_2, ..., W_n]
  where W_i: [d_in, d_out/n]

GPU i: Y_i = XW_i
All: Y = [Y_1, Y_2, ..., Y_n]
```

**ë©”ëª¨ë¦¬:** ê° GPUëŠ” `d_out/n`ë§Œ ì €ì¥!

---

## Transformerì— ì ìš©

### 1. Self-Attention

**MLP (Feed-Forward):**
```
h = activation(X @ W1)
Y = h @ W2

Standard:
  W1: [d_model, 4*d_model]
  W2: [4*d_model, d_model]

Tensor Parallel (column split):
  W1 = [W1_1, W1_2, ..., W1_n]
  W2 = [W2_1; W2_2; ...; W2_n]  (row split!)
```

**êµ¬í˜„:**
```python
class ParallelMLP(nn.Module):
    def __init__(self, d_model, d_ff, world_size):
        super().__init__()
        self.world_size = world_size
        self.rank = dist.get_rank()
        
        # W1: Column parallel
        self.fc1 = ColumnParallelLinear(
            d_model,
            d_ff // world_size,
            gather_output=False  # Keep split
        )
        
        # W2: Row parallel
        self.fc2 = RowParallelLinear(
            d_ff // world_size,
            d_model,
            input_is_parallel=True  # Already split
        )
    
    def forward(self, x):
        # x: [batch, seq, d_model]
        
        # W1 (column parallel)
        h = self.fc1(x)  # [batch, seq, d_ff/world_size]
        h = F.gelu(h)
        
        # W2 (row parallel)
        y = self.fc2(h)  # [batch, seq, d_model]
        
        return y
```

### 2. Multi-Head Attention

**Q, K, Vë¥¼ head ë‹¨ìœ„ë¡œ ë¶„ì‚°:**

```python
# Standard
num_heads = 32
head_dim = 128

# Tensor Parallel (4 GPUs)
num_heads_per_gpu = 32 // 4 = 8

class ParallelAttention(nn.Module):
    def __init__(self, d_model, num_heads, world_size):
        self.num_heads = num_heads // world_size
        self.head_dim = d_model // num_heads
        
        # Q, K, V: Column parallel
        self.qkv = ColumnParallelLinear(
            d_model,
            3 * d_model // world_size,
            gather_output=False
        )
        
        # Output: Row parallel
        self.out = RowParallelLinear(
            d_model // world_size,
            d_model,
            input_is_parallel=True
        )
    
    def forward(self, x):
        # x: [batch, seq, d_model]
        
        # QKV projection (ê° GPUëŠ” 8 headsë§Œ)
        qkv = self.qkv(x)  # [batch, seq, 3*d_model/world_size]
        q, k, v = qkv.chunk(3, dim=-1)
        
        # Reshape
        q = q.view(batch, seq, self.num_heads, self.head_dim)
        k = k.view(batch, seq, self.num_heads, self.head_dim)
        v = v.view(batch, seq, self.num_heads, self.head_dim)
        
        # Attention (local to each GPU)
        attn_out = self.attention(q, k, v)
        
        # Output projection (all-reduce)
        out = self.out(attn_out)  # [batch, seq, d_model]
        
        return out
```

---

## í†µì‹  íŒ¨í„´

### Column Parallel

```python
class ColumnParallelLinear(nn.Module):
    """
    Y = XW
    Wë¥¼ column-wiseë¡œ ë¶„í• 
    
    Input: X (replicated)
    Output: Y_i (split)
    Communication: None
    """
    def forward(self, x):
        # x: [batch, in_features] on all GPUs
        
        # Local matmul
        output = F.linear(x, self.weight, self.bias)
        
        if self.gather_output:
            # All-gather across GPUs
            output = gather_from_model_parallel_region(output)
        
        return output
```

### Row Parallel

```python
class RowParallelLinear(nn.Module):
    """
    Y = XW
    Wë¥¼ row-wiseë¡œ ë¶„í• 
    
    Input: X_i (split)
    Output: Y (replicated)
    Communication: All-reduce
    """
    def forward(self, x):
        # x: [batch, in_features/world_size] on each GPU
        
        # Local matmul
        output_parallel = F.linear(x, self.weight, self.bias)
        
        # All-reduce across GPUs
        output = reduce_from_model_parallel_region(output_parallel)
        
        return output
```

### í†µì‹  ì—°ì‚°

```python
def all_gather(tensor, dim=0):
    """ëª¨ë“  GPUì—ì„œ ìˆ˜ì§‘"""
    world_size = dist.get_world_size()
    tensor_list = [torch.zeros_like(tensor) for _ in range(world_size)]
    dist.all_gather(tensor_list, tensor)
    return torch.cat(tensor_list, dim=dim)

def all_reduce(tensor):
    """ëª¨ë“  GPUì˜ ê²°ê³¼ í•©ì‚°"""
    dist.all_reduce(tensor, op=dist.ReduceOp.SUM)
    return tensor

def scatter(tensor, dim=0):
    """ê° GPUì— ì¼ë¶€ ë¶„ë°°"""
    world_size = dist.get_world_size()
    rank = dist.get_rank()
    chunk_size = tensor.size(dim) // world_size
    return tensor.narrow(dim, rank * chunk_size, chunk_size)
```

---

## ì‹¤ì „ êµ¬í˜„

### 1. Megatron-LM ìŠ¤íƒ€ì¼

```python
import torch
import torch.distributed as dist

class TransformerLayer(nn.Module):
    def __init__(self, config, world_size):
        super().__init__()
        self.world_size = world_size
        
        # Attention
        self.attention = ParallelAttention(
            config.hidden_size,
            config.num_attention_heads,
            world_size
        )
        
        # MLP
        self.mlp = ParallelMLP(
            config.hidden_size,
            config.intermediate_size,
            world_size
        )
        
        # LayerNorm (replicated)
        self.ln1 = nn.LayerNorm(config.hidden_size)
        self.ln2 = nn.LayerNorm(config.hidden_size)
    
    def forward(self, x):
        # Attention (with residual)
        residual = x
        x = self.ln1(x)
        x = self.attention(x)
        x = x + residual
        
        # MLP (with residual)
        residual = x
        x = self.ln2(x)
        x = self.mlp(x)
        x = x + residual
        
        return x


# ì´ˆê¸°í™”
def initialize_model_parallel(world_size):
    """Model parallel group ìƒì„±"""
    rank = dist.get_rank()
    
    # Model parallel group
    model_parallel_group = dist.new_group(
        ranks=list(range(world_size))
    )
    
    return model_parallel_group


# ì‚¬ìš©
if __name__ == "__main__":
    # ë¶„ì‚° ì´ˆê¸°í™”
    dist.init_process_group("nccl")
    world_size = dist.get_world_size()
    rank = dist.get_rank()
    
    # ëª¨ë¸ ìƒì„±
    model = TransformerLayer(config, world_size).cuda()
    
    # ì¶”ë¡ 
    x = torch.randn(32, 512, 768).cuda()  # Replicated input
    y = model(x)
    
    print(f"GPU {rank}: Output shape {y.shape}")
```

### 2. HuggingFace Accelerate

```python
from transformers import AutoModelForCausalLM
from accelerate import init_empty_weights, load_checkpoint_and_dispatch

# 1. ë¹ˆ ëª¨ë¸ ìƒì„± (ë©”ëª¨ë¦¬ ì•ˆ ì”€)
with init_empty_weights():
    model = AutoModelForCausalLM.from_pretrained("gpt2-xl")

# 2. Tensor parallel ì„¤ì •
device_map = {
    "transformer.h.0": 0,
    "transformer.h.1": 1,
    "transformer.h.2": 2,
    # ...
}

# 3. ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ & ë¶„ì‚°
model = load_checkpoint_and_dispatch(
    model,
    checkpoint="gpt2-xl",
    device_map="auto",  # ìë™ ë¶„ì‚°
    offload_folder="offload"
)

# ì¶”ë¡ 
outputs = model.generate(inputs, max_new_tokens=100)
```

### 3. DeepSpeed Inference

```python
import deepspeed

# ëª¨ë¸ ìƒì„±
model = AutoModelForCausalLM.from_pretrained("facebook/opt-66b")

# DeepSpeedë¡œ wrapping
ds_engine = deepspeed.init_inference(
    model,
    mp_size=4,  # Tensor parallel size
    dtype=torch.float16,
    replace_with_kernel_inject=True,  # ìµœì í™”ëœ ì»¤ë„
    replace_method="auto"
)

# ì¶”ë¡ 
outputs = ds_engine.generate(inputs, max_new_tokens=100)
```

---

## í†µì‹  ìµœì í™”

### 1. Overlapping Computation & Communication

```python
class OverlappedLinear(nn.Module):
    """í†µì‹ ê³¼ ê³„ì‚°ì„ ë™ì‹œì—"""
    def forward(self, x):
        # 1. ê³„ì‚° ì‹œì‘
        local_output = F.linear(x, self.weight)
        
        # 2. ë¹„ë™ê¸° All-reduce (ë°±ê·¸ë¼ìš´ë“œ)
        handle = dist.all_reduce(local_output, async_op=True)
        
        # 3. ë‹¤ë¥¸ ì‘ì—… (ì˜ˆ: bias ì¶”ê°€)
        if self.bias is not None:
            local_output = local_output + self.bias
        
        # 4. í†µì‹  ì™„ë£Œ ëŒ€ê¸°
        handle.wait()
        
        return local_output
```

### 2. Gradient Accumulation

```python
# ì‘ì€ ë°°ì¹˜ë¥¼ ì—¬ëŸ¬ ë²ˆ (í†µì‹  ì¤„ì´ê¸°)
for micro_batch in split_batch(batch, num_micro_batches):
    # Forward
    loss = model(micro_batch)
    
    # Backward (gradient accumulate)
    loss.backward()
    
    # No optimizer step yet!

# ëª¨ë“  micro-batch ëë‚œ í›„ í•œ ë²ˆì—
optimizer.step()
optimizer.zero_grad()
```

### 3. Sequence Parallelism

ê¸´ ì‹œí€€ìŠ¤ë„ ë¶„ì‚°:

```python
# Standard: ëª¨ë“  GPUê°€ ì „ì²´ ì‹œí€€ìŠ¤ ì²˜ë¦¬
x: [batch, seq_len, hidden]

# Sequence Parallel: ì‹œí€€ìŠ¤ë¥¼ ë‚˜ëˆ”
x_local: [batch, seq_len/world_size, hidden]

# LayerNorm, Dropoutë„ split
ln_output = layer_norm(x_local)  # Local
dropout_output = dropout(ln_output)  # Local

# Attentionì€ all-gather í•„ìš”
x_full = all_gather(x_local, dim=1)
attn_output = attention(x_full)
attn_output_local = scatter(attn_output, dim=1)
```

---

## ë©”ëª¨ë¦¬ & í†µì‹  ë¶„ì„

### ë©”ëª¨ë¦¬ ì ˆê°

**175B ëª¨ë¸, 4-way TP:**
```
Standard (1 GPU):
  Parameters: 350 GB
  Activations: 50 GB
  Total: 400 GB (ë¶ˆê°€ëŠ¥!)

Tensor Parallel (4 GPUs):
  Parameters per GPU: 350/4 = 87.5 GB
  Activations per GPU: 50/4 = 12.5 GB
  Total per GPU: 100 GB (A100 ê°€ëŠ¥!)
```

### í†µì‹  ë¹„ìš©

**Per layer:**
```
MLP:
  Column parallel: No communication (forward)
  Row parallel: All-reduce (4dÂ² elements)

Attention:
  Column parallel (QKV): No communication
  Row parallel (Output): All-reduce (4dÂ² elements)

Total per layer: 2 Ã— All-reduce (8dÂ²)
```

**GPT-3 175B, 96 layers:**
```
d = 12,288
Communication per layer: 8 Ã— 12,288Â² = 1.2 GB
Total: 96 Ã— 1.2 GB = 115 GB per forward pass

With A100 (600 GB/s NVLink):
  Communication time: 115 / 600 = 0.19s
  Computation time: ~2s
  Overhead: 9.5% (acceptable!)
```

---

## Tensor Parallelism vs ë‹¤ë¥¸ ë°©ì‹

| ë°©ì‹ | ë©”ëª¨ë¦¬/GPU | í†µì‹ ëŸ‰ | GPU í™œìš©ë¥  | êµ¬í˜„ ë‚œì´ë„ |
|------|-----------|--------|-----------|------------|
| Data Parallel | 100% | Gradient only | 100% | ì‰¬ì›€ |
| Pipeline Parallel | 1/N | Activation | ~50% | ì¤‘ê°„ |
| **Tensor Parallel** | 1/N | Per layer | 90%+ | ì–´ë ¤ì›€ |
| Hybrid (TP+PP) | 1/(NÃ—M) | ë‘˜ ë‹¤ | 80%+ | ë§¤ìš° ì–´ë ¤ì›€ |

---

## ì‹¤ì „ ë²¤ì¹˜ë§ˆí¬

### GPT-3 175B, 8Ã— A100 80GB

| ë°©ì‹ | ì²˜ë¦¬ëŸ‰ (tokens/s) | ë©”ëª¨ë¦¬/GPU |
|------|------------------|-----------|
| Impossible (1 GPU) | N/A | 350 GB |
| TP=8 | 1,240 | 48 GB |
| TP=4 + PP=2 | 1,850 | 52 GB |
| TP=8 + Zero | 2,100 | 45 GB |

### í†µì‹  ìŠ¤ì¼€ì¼ë§

**A100, NVLink:**

| TP Size | ì´ë¡  íš¨ìœ¨ | ì‹¤ì œ íš¨ìœ¨ | í†µì‹  ì˜¤ë²„í—¤ë“œ |
|---------|----------|----------|--------------|
| 2 | 100% | 95% | 5% |
| 4 | 100% | 90% | 10% |
| 8 | 100% | 85% | 15% |

**DGX A100 (NVSwitch):**
- TP=8ê¹Œì§€ ê±°ì˜ linear scaling!

---

## Best Practices

### 1. TP Size ì„ íƒ

```python
# Rule of thumb
if model_size < 20B:
    tp_size = 1  # í•„ìš” ì—†ìŒ
elif model_size < 70B:
    tp_size = 2  # ì ë‹¹
elif model_size < 200B:
    tp_size = 4  # ê¶Œì¥
else:
    tp_size = 8  # ìµœëŒ€
```

### 2. í•˜ì´ë¸Œë¦¬ë“œ ë³‘ë ¬í™”

```python
# ì˜ˆ: 175B ëª¨ë¸, 16 GPUs
config = {
    "tensor_parallel": 4,   # ë ˆì´ì–´ ë‚´ ë¶„ì‚°
    "pipeline_parallel": 2,  # ë ˆì´ì–´ ê°„ ë¶„ì‚°
    "data_parallel": 2       # ë°°ì¹˜ ë¶„ì‚°
}

# ì´ GPU: 4 Ã— 2 Ã— 2 = 16
```

### 3. ë„¤íŠ¸ì›Œí¬ ìµœì í™”

```python
# NVLinkê°€ ìˆìœ¼ë©´
if has_nvlink():
    tensor_parallel_size = 8  # Aggressive
else:
    tensor_parallel_size = 2  # Conservative
```

---

## ì½”ë“œ ì˜ˆì œ: ì²˜ìŒë¶€í„° êµ¬í˜„

```python
import torch
import torch.distributed as dist
import torch.nn as nn

class TensorParallelLinear(nn.Module):
    """
    ì™„ì „í•œ Tensor Parallel Linear êµ¬í˜„
    """
    def __init__(
        self,
        in_features,
        out_features,
        bias=True,
        gather_output=True,
        input_is_parallel=False
    ):
        super().__init__()
        
        self.world_size = dist.get_world_size()
        self.rank = dist.get_rank()
        
        # Outputì„ world_sizeë¡œ ë‚˜ëˆ”
        self.output_size_per_partition = out_features // self.world_size
        
        # Weight (ê° GPUëŠ” ì¼ë¶€ë§Œ ì†Œìœ )
        self.weight = nn.Parameter(torch.empty(
            self.output_size_per_partition,
            in_features
        ))
        
        # Bias (ì˜µì…˜)
        if bias:
            self.bias = nn.Parameter(torch.empty(self.output_size_per_partition))
        else:
            self.register_parameter('bias', None)
        
        # ì´ˆê¸°í™”
        self._initialize_weights()
        
        self.gather_output = gather_output
        self.input_is_parallel = input_is_parallel
    
    def _initialize_weights(self):
        nn.init.xavier_uniform_(self.weight)
        if self.bias is not None:
            nn.init.zeros_(self.bias)
    
    def forward(self, x):
        # x: [batch, seq, in_features] or [batch, seq, in_features/world_size]
        
        # Inputì´ ì´ë¯¸ splitë˜ì–´ ìˆìœ¼ë©´ all-gather
        if self.input_is_parallel:
            # Row parallel: inputì€ split, outputì€ reduce
            input_parallel = x
        else:
            # Column parallel: inputì€ replicate, outputì€ split
            input_parallel = x
        
        # Local matmul
        output_parallel = F.linear(input_parallel, self.weight, self.bias)
        
        # Gather or reduce
        if self.gather_output:
            # All-gather across model parallel group
            output = self._gather(output_parallel)
        elif self.input_is_parallel:
            # All-reduce (row parallel)
            output = self._reduce(output_parallel)
        else:
            # Keep split (column parallel)
            output = output_parallel
        
        return output
    
    def _gather(self, tensor):
        """All-gather operation"""
        world_size = self.world_size
        tensor_list = [torch.empty_like(tensor) for _ in range(world_size)]
        dist.all_gather(tensor_list, tensor)
        return torch.cat(tensor_list, dim=-1)
    
    def _reduce(self, tensor):
        """All-reduce operation"""
        dist.all_reduce(tensor, op=dist.ReduceOp.SUM)
        return tensor


# ì‚¬ìš© ì˜ˆì œ
def example_usage():
    # ë¶„ì‚° ì´ˆê¸°í™”
    dist.init_process_group(backend='nccl')
    
    world_size = dist.get_world_size()
    rank = dist.get_rank()
    
    # ëª¨ë¸ ìƒì„±
    batch_size = 32
    seq_len = 512
    d_model = 1024
    
    # Input (replicated across all GPUs)
    x = torch.randn(batch_size, seq_len, d_model).cuda()
    
    # Column parallel layer
    fc1 = TensorParallelLinear(
        d_model, 
        4 * d_model,
        gather_output=False  # Keep split for next layer
    ).cuda()
    
    # Row parallel layer
    fc2 = TensorParallelLinear(
        4 * d_model,
        d_model,
        input_is_parallel=True,  # Input is already split
        gather_output=True       # Final output needs to be replicated
    ).cuda()
    
    # Forward
    h = fc1(x)  # [batch, seq, 4*d_model/world_size]
    h = F.gelu(h)
    y = fc2(h)  # [batch, seq, d_model]
    
    print(f"GPU {rank}: x.shape={x.shape}, h.shape={h.shape}, y.shape={y.shape}")

if __name__ == "__main__":
    example_usage()
```

---

## ìš”ì•½

**Tensor Parallelism**ì€:

1. **í•˜ë‚˜ì˜ ë ˆì´ì–´ë¥¼ ì—¬ëŸ¬ GPUì— ë¶„ì‚°**
2. **Column/Row parallel** ì „ëµ
3. **ë©”ëª¨ë¦¬: 1/N ì ˆê°**
4. **í†µì‹ : All-reduce (per layer)**
5. **GPU í™œìš©ë¥ : 90%+**

**í•µì‹¬ í¬ì¸íŠ¸:**
- Large modelì— í•„ìˆ˜ (70B+)
- NVLink í•„ìš” (í†µì‹  ë³‘ëª©)
- Pipeline/Data Parallelê³¼ ì¡°í•©
- Megatron-LM, DeepSpeed ì‚¬ìš© ê¶Œì¥

**ë‹¤ìŒ ë‹¨ê³„:**
- Pipeline Parallelism ì¡°í•©
- ZeRO optimizer
- 3D Parallelism

---

## ë‹¤ìŒ ê¸€

**12í¸: Pipeline Parallelism**
- ë ˆì´ì–´ë¥¼ GPUì— ë¶„ë°°
- Micro-batching
- Bubble ìµœì†Œí™”

ê¸°ëŒ€í•´ì£¼ì„¸ìš”! ğŸš€

---

*ì§ˆë¬¸ì´ë‚˜ í”¼ë“œë°±ì€ [GitHub](https://github.com/yhna941)ì—ì„œ í™˜ì˜í•©ë‹ˆë‹¤!*
