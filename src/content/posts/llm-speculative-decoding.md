---
title: "LLM Inference ìµœì í™” #5: Speculative Decoding - ì‘ì€ ëª¨ë¸ë¡œ í° ëª¨ë¸ ê°€ì†í•˜ê¸°"
description: "Draft ëª¨ë¸ë¡œ ì¶”ë¡  ì†ë„ë¥¼ 2-3ë°° ë†’ì´ëŠ” Speculative Decodingì˜ ì›ë¦¬ì™€ ì‹¤ì „ êµ¬í˜„ì„ ì•Œì•„ë´…ë‹ˆë‹¤."
pubDate: 2026-02-06
author: "Yh Na"
tags: ["llm", "optimization", "speculative-decoding", "inference", "speed"]
draft: false
---

# LLM Inference ìµœì í™” #5: Speculative Decoding

LLMì€ í•œ ë²ˆì— **í•œ í† í°ì”©** ìƒì„±í•©ë‹ˆë‹¤. ë³‘ë ¬í™”ê°€ ì•ˆ ë˜ë‹ˆ ëŠë¦½ë‹ˆë‹¤.

**Speculative Decoding**ì€ ì´ ë¬¸ì œë¥¼ í•´ê²°í•©ë‹ˆë‹¤:
- ì‘ì€ ëª¨ë¸ì´ **ì—¬ëŸ¬ í† í°ì„ ì¶”ì¸¡**
- í° ëª¨ë¸ì´ **í•œ ë²ˆì— ê²€ì¦**
- **2-3ë°° ë¹ ë¦„**
- **ê²°ê³¼ëŠ” ë™ì¼** (ë¬´ì†ì‹¤!)

ë§ˆë²• ê°™ì§€ë§Œ ìˆ˜í•™ì ìœ¼ë¡œ ë³´ì¥ë©ë‹ˆë‹¤.

---

## ë¬¸ì œ: AutoregressiveëŠ” ëŠë¦¬ë‹¤

### ìˆœì°¨ ìƒì„±

```
Step 1: "The" â†’ "cat"
Step 2: "The cat" â†’ "is"
Step 3: "The cat is" â†’ "sleeping"
...
```

ê° ë‹¨ê³„ë§ˆë‹¤:
1. ì „ì²´ ëª¨ë¸ ì‹¤í–‰ (70B íŒŒë¼ë¯¸í„°!)
2. 1ê°œ í† í° ìƒì„±
3. ë‹¤ìŒ ë‹¨ê³„

**100 í† í° ìƒì„± = 100ë²ˆ ëª¨ë¸ ì‹¤í–‰**

### GPU í™œìš©ë¥ ì´ ë‚®ìŒ

```
GPU Utilization during inference:
[â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 20%
```

**ì™œ?**
- Memory-bound (ê³„ì‚°ë³´ë‹¤ ë©”ëª¨ë¦¬ ì½ê¸°ê°€ ë³‘ëª©)
- ë°°ì¹˜ í¬ê¸° 1 (í•œ í† í°ì”©)
- ë³‘ë ¬í™” ë¶ˆê°€

---

## í•´ê²°ì±…: Speculative Decoding

### í•µì‹¬ ì•„ì´ë””ì–´

> **ì‘ì€ ëª¨ë¸(draft)ì´ ì—¬ëŸ¬ í† í°ì„ ë¹ ë¥´ê²Œ ì¶”ì¸¡í•˜ê³ ,**  
> **í° ëª¨ë¸(target)ì´ í•œ ë²ˆì— ê²€ì¦í•œë‹¤**

```
Draft model (1B):  "The cat is sleeping on the"  â† ë¹ ë¦„ (6 tokens)
Target model (70B): "The cat is sleeping"  â† ê²€ì¦ (4 tokens ìŠ¹ì¸)

ê²°ê³¼: 1ë²ˆì˜ target ì‹¤í–‰ìœ¼ë¡œ 4 í† í° ìƒì„±!
```

### ì™œ ë¹ ë¥¸ê°€?

**Standard (4 tokens):**
```
Target("The") â†’ "cat"
Target("The cat") â†’ "is"
Target("The cat is") â†’ "sleeping"
Target("The cat is sleeping") â†’ "on"

ì´: 4ë²ˆ ì‹¤í–‰
```

**Speculative (4 tokens):**
```
Draft("The") â†’ "cat is sleeping on the"  (ë¹ ë¦„)
Target("The", candidates=["cat", "is", "sleeping", "on", "the"])
  â†’ ["cat", "is", "sleeping"] ìŠ¹ì¸, ["on", "the"] ê±°ë¶€

ì´: Draft 6ë²ˆ + Target 1ë²ˆ
```

Draftê°€ 70Bë³´ë‹¤ **10-20ë°° ë¹ ë¥´ë‹ˆ** ì „ì²´ì ìœ¼ë¡œ ë¹ ë¦…ë‹ˆë‹¤!

---

## ì•Œê³ ë¦¬ì¦˜

### Step-by-step

**1. Draft ë‹¨ê³„ (ì¶”ì¸¡)**
```python
def draft_phase(prompt, draft_model, K=5):
    """ì‘ì€ ëª¨ë¸ë¡œ Kê°œ í† í° ì¶”ì¸¡"""
    tokens = [prompt]
    
    for _ in range(K):
        next_token = draft_model.sample(tokens)
        tokens.append(next_token)
    
    return tokens  # [prompt, t1, t2, ..., tK]
```

**2. Target ë‹¨ê³„ (ê²€ì¦)**
```python
def target_phase(tokens, target_model):
    """í° ëª¨ë¸ë¡œ í•œ ë²ˆì— ê²€ì¦"""
    # ëª¨ë“  prefixì— ëŒ€í•´ í™•ë¥  ê³„ì‚°
    probs = target_model.forward(tokens)  # [K+1, vocab_size]
    
    accepted = []
    for i in range(len(tokens) - 1):
        draft_token = tokens[i + 1]
        target_prob = probs[i]
        
        if should_accept(draft_token, target_prob):
            accepted.append(draft_token)
        else:
            # ê±°ë¶€: targetì—ì„œ ìƒˆë¡œ ìƒ˜í”Œë§
            new_token = target_model.sample(target_prob)
            accepted.append(new_token)
            break  # ì´í›„ëŠ” ë¬´íš¨
    
    return accepted
```

### ìˆ˜í•™ì  ë³´ì¥

**í•µì‹¬ ì§ˆë¬¸:** ì–´ë–»ê²Œ ê²°ê³¼ê°€ ì •í™•íˆ ê°™ì„ê¹Œ?

**ë‹µ:** Modified Rejection Sampling

**Draft í™•ë¥ :** p(x)  
**Target í™•ë¥ :** q(x)

**Accept í™•ë¥ :**
```
Î±(x) = min(1, q(x) / p(x))
```

**ê±°ë¶€ ì‹œ ì¬ìƒ˜í”Œë§:**
```
q'(x) = max(0, q(x) - p(x)) / Z
```

ì´ë ‡ê²Œ í•˜ë©´ **ìˆ˜í•™ì ìœ¼ë¡œ q(x)ì™€ ë™ì¼í•œ ë¶„í¬**!

---

## êµ¬í˜„ ì˜ˆì œ

### 1. ê¸°ë³¸ êµ¬í˜„

```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

class SpeculativeDecoder:
    def __init__(self, draft_model, target_model, tokenizer):
        self.draft = draft_model
        self.target = target_model
        self.tokenizer = tokenizer
    
    def generate(self, prompt, max_tokens=100, K=5):
        """
        Args:
            prompt: ì…ë ¥ í…ìŠ¤íŠ¸
            max_tokens: ìƒì„±í•  ìµœëŒ€ í† í° ìˆ˜
            K: Draftê°€ ì¶”ì¸¡í•  í† í° ìˆ˜
        """
        tokens = self.tokenizer.encode(prompt, return_tensors='pt').to('cuda')
        generated = []
        
        while len(generated) < max_tokens:
            # 1. Draft phase
            draft_tokens = self.draft_phase(tokens, K)
            
            # 2. Target phase (ê²€ì¦)
            accepted, new_token = self.target_phase(tokens, draft_tokens)
            
            # 3. ìŠ¹ì¸ëœ í† í°ë“¤ ì¶”ê°€
            generated.extend(accepted)
            tokens = torch.cat([tokens, torch.tensor([accepted]).to('cuda')], dim=-1)
            
            # 4. ê±°ë¶€ëœ ê²½ìš° ìƒˆ í† í° ì¶”ê°€
            if new_token is not None:
                generated.append(new_token)
                tokens = torch.cat([tokens, torch.tensor([[new_token]]).to('cuda')], dim=-1)
            
            # EOS ì²´í¬
            if generated[-1] == self.tokenizer.eos_token_id:
                break
        
        return self.tokenizer.decode(generated)
    
    def draft_phase(self, tokens, K):
        """Draft modelë¡œ Kê°œ í† í° ì¶”ì¸¡"""
        draft_tokens = []
        current = tokens.clone()
        
        with torch.no_grad():
            for _ in range(K):
                logits = self.draft(current).logits[:, -1, :]
                next_token = torch.multinomial(
                    torch.softmax(logits, dim=-1), 
                    num_samples=1
                )
                draft_tokens.append(next_token.item())
                current = torch.cat([current, next_token], dim=-1)
        
        return draft_tokens
    
    def target_phase(self, tokens, draft_tokens):
        """Target modelë¡œ ê²€ì¦"""
        # ëª¨ë“  draft tokenì„ í•œ ë²ˆì— ì²˜ë¦¬
        all_tokens = torch.cat([
            tokens,
            torch.tensor([draft_tokens]).to('cuda')
        ], dim=-1)
        
        with torch.no_grad():
            logits = self.target(all_tokens).logits[0]  # [seq_len, vocab_size]
        
        accepted = []
        new_token = None
        
        for i, draft_token in enumerate(draft_tokens):
            # Targetì˜ í™•ë¥  ë¶„í¬
            target_probs = torch.softmax(logits[tokens.shape[1] + i - 1], dim=-1)
            
            # Draftì˜ í™•ë¥  (ì¬ê³„ì‚°)
            draft_logits = self.draft(
                torch.cat([tokens, torch.tensor([draft_tokens[:i]]).to('cuda')], dim=-1)
            ).logits[:, -1, :]
            draft_probs = torch.softmax(draft_logits, dim=-1)
            
            # Rejection sampling
            accept_prob = min(1.0, 
                target_probs[draft_token].item() / 
                (draft_probs[0, draft_token].item() + 1e-10)
            )
            
            if torch.rand(1).item() < accept_prob:
                accepted.append(draft_token)
            else:
                # ê±°ë¶€: targetì—ì„œ ìƒˆë¡œ ìƒ˜í”Œë§
                adjusted_probs = torch.clamp(target_probs - draft_probs[0], min=0)
                adjusted_probs = adjusted_probs / adjusted_probs.sum()
                new_token = torch.multinomial(adjusted_probs, num_samples=1).item()
                break
        
        return accepted, new_token


# ì‚¬ìš©
draft_model = AutoModelForCausalLM.from_pretrained("TinyLlama/TinyLlama-1.1B", device_map="cuda")
target_model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b-hf", device_map="cuda")
tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-hf")

decoder = SpeculativeDecoder(draft_model, target_model, tokenizer)
output = decoder.generate("Once upon a time", max_tokens=100, K=5)
print(output)
```

### 2. ìµœì í™”ëœ ë²„ì „ (ë°°ì¹˜ ì²˜ë¦¬)

```python
class FastSpeculativeDecoder:
    def target_phase_batched(self, tokens, draft_tokens):
        """ë°°ì¹˜ë¡œ í•œ ë²ˆì— ì²˜ë¦¬"""
        K = len(draft_tokens)
        
        # ëª¨ë“  prefixë¥¼ ë°°ì¹˜ë¡œ êµ¬ì„±
        # [prefix, prefix+t1, prefix+t1+t2, ...]
        batch = []
        for i in range(K):
            batch.append(torch.cat([
                tokens,
                torch.tensor([draft_tokens[:i+1]]).to('cuda')
            ], dim=-1))
        
        # Padding
        batch = torch.nn.utils.rnn.pad_sequence(batch, batch_first=True)
        
        # í•œ ë²ˆì— ì‹¤í–‰!
        with torch.no_grad():
            logits = self.target(batch).logits  # [K, seq_len, vocab_size]
        
        # ê° ìœ„ì¹˜ì—ì„œ í™•ë¥  ì¶”ì¶œ
        target_probs = torch.softmax(logits[:, -1, :], dim=-1)  # [K, vocab_size]
        
        # ê²€ì¦ (ë²¡í„°í™”)
        draft_probs = self.get_draft_probs_batched(tokens, draft_tokens)
        accept_probs = torch.minimum(
            torch.ones(K),
            target_probs[torch.arange(K), draft_tokens] / 
            (draft_probs[torch.arange(K), draft_tokens] + 1e-10)
        )
        
        # ì²« ê±°ë¶€ ì§€ì ê¹Œì§€ ìŠ¹ì¸
        random_vals = torch.rand(K)
        accepted_mask = random_vals < accept_probs
        first_reject = torch.where(~accepted_mask)[0]
        
        if len(first_reject) > 0:
            accept_until = first_reject[0].item()
            accepted = draft_tokens[:accept_until]
            # ìƒˆ í† í° ìƒ˜í”Œë§
            adjusted = torch.clamp(
                target_probs[accept_until] - draft_probs[accept_until],
                min=0
            )
            new_token = torch.multinomial(adjusted / adjusted.sum(), 1).item()
        else:
            accepted = draft_tokens
            new_token = None
        
        return accepted, new_token
```

---

## ì„±ëŠ¥ ë¶„ì„

### ì´ë¡ ì  ì†ë„ì—…

**Kê°œ í† í° ì¶”ì¸¡, í‰ê·  Î±ê°œ ìŠ¹ì¸:**

```
Speedup = Î± / (K * t_draft + t_target)

where:
  Î±: í‰ê·  ìŠ¹ì¸ í† í° ìˆ˜ (acceptance rate)
  K: Draft ì¶”ì¸¡ ìˆ˜
  t_draft: Draft ì‹œê°„ (ì‘ìŒ)
  t_target: Target ì‹œê°„ (ê¹€)
```

**ì˜ˆì‹œ:**
- K = 5
- Î± = 3 (60% acceptance)
- t_draft = 0.1ms
- t_target = 10ms

```
Standard: 3 tokens = 30ms
Speculative: 3 tokens = 5*0.1 + 10 = 10.5ms

Speedup: 30 / 10.5 = 2.86x
```

### ì‹¤ì œ ë²¤ì¹˜ë§ˆí¬

**LLaMA-7B (target) + TinyLlama-1B (draft):**

| K | Acceptance Rate | Tokens/sec | Speedup |
|---|-----------------|------------|---------|
| 3 | 65% | 42 | 1.8x |
| 5 | 60% | 58 | 2.5x |
| 7 | 55% | 63 | 2.7x |
| 10 | 50% | 61 | 2.6x |

**ìµœì  K = 5-7**

---

## ê³ ê¸‰ ê¸°ë²•

### 1. Tree-based Speculative Decoding

**ì•„ì´ë””ì–´:** ì—¬ëŸ¬ í›„ë³´ë¥¼ íŠ¸ë¦¬ë¡œ íƒìƒ‰

```
                    "The"
                   /  |  \
                cat  dog  bird
               / |    |     |
             is sat  ran  flew
```

Draftê°€ ì—¬ëŸ¬ ê²½ë¡œë¥¼ ìƒì„± â†’ Targetì´ í•œ ë²ˆì— ê²€ì¦

**ì¥ì :** Acceptance rate â†‘  
**ë‹¨ì :** ë©”ëª¨ë¦¬ â†‘

```python
def tree_draft(prompt, draft_model, tree_depth=2, branching=3):
    """íŠ¸ë¦¬ êµ¬ì¡°ë¡œ í›„ë³´ ìƒì„±"""
    root = TreeNode(prompt)
    queue = [root]
    
    for level in range(tree_depth):
        new_queue = []
        for node in queue:
            # ê° ë…¸ë“œì—ì„œ branchingê°œ í›„ë³´ ìƒì„±
            top_k = draft_model.top_k(node.tokens, k=branching)
            for token in top_k:
                child = TreeNode(node.tokens + [token])
                node.children.append(child)
                new_queue.append(child)
        queue = new_queue
    
    return root
```

### 2. Multi-draft Models

ì—¬ëŸ¬ ì‘ì€ ëª¨ë¸ì„ ì‚¬ìš©:

```python
drafts = [
    TinyLlama_1B,
    TinyLlama_1B_finetuned,
    Pythia_1B
]

# ê° draftê°€ í›„ë³´ ìƒì„±
candidates = []
for draft in drafts:
    candidates.extend(draft.generate(prompt, K=3))

# Targetì´ ëª¨ë“  í›„ë³´ ê²€ì¦
best_path = target.verify(candidates)
```

### 3. Adaptive K

Acceptance rateì— ë”°ë¼ K ì¡°ì •:

```python
class AdaptiveSpeculativeDecoder:
    def __init__(self, draft, target, K_min=3, K_max=10):
        self.K = K_min
        self.K_min = K_min
        self.K_max = K_max
        self.acceptance_history = []
    
    def generate_step(self, tokens):
        # Draft
        draft_tokens = self.draft_phase(tokens, self.K)
        
        # Verify
        accepted, new_token = self.target_phase(tokens, draft_tokens)
        
        # K ì¡°ì •
        acceptance_rate = len(accepted) / self.K
        self.acceptance_history.append(acceptance_rate)
        
        if acceptance_rate > 0.7:
            self.K = min(self.K + 1, self.K_max)
        elif acceptance_rate < 0.4:
            self.K = max(self.K - 1, self.K_min)
        
        return accepted, new_token
```

---

## Draft Model ì„ íƒ

### ê¸°ì¤€

**1. í¬ê¸° ë¹„ìœ¨**
- Target 70B â†’ Draft 1-7B
- 10-70ë°° ì‘ì•„ì•¼ íš¨ê³¼

**2. í’ˆì§ˆ**
- ë„ˆë¬´ ë‚˜ì˜ë©´ acceptance rate â†“
- ì ë‹¹í•œ í’ˆì§ˆ í•„ìš”

**3. ê°™ì€ í† í¬ë‚˜ì´ì €**
- í•„ìˆ˜!

### ì¶”ì²œ ì¡°í•©

| Target | Draft | Speedup |
|--------|-------|---------|
| LLaMA-70B | LLaMA-7B | 2.3x |
| LLaMA-70B | TinyLlama-1B | 2.1x |
| GPT-3.5 | GPT-2 | 1.8x |
| Mixtral-8x7B | Mistral-7B | 2.5x |

### Fine-tuning Draft

Target ìŠ¤íƒ€ì¼ì— ë§ì¶° draftë¥¼ fine-tune:

```python
# Targetì˜ ì¶œë ¥ìœ¼ë¡œ draft í•™ìŠµ
def train_draft_on_target(draft, target, dataset):
    for prompt in dataset:
        with torch.no_grad():
            target_output = target.generate(prompt)
        
        # Draftê°€ target ëª¨ë°©
        loss = draft.train_step(prompt, target_output)
```

**ê²°ê³¼:** Acceptance rate 60% â†’ 75%

---

## ì‹¤ì „ ì˜ˆì œ: ì±—ë´‡ ì„œë¹™

```python
import asyncio
from fastapi import FastAPI
from pydantic import BaseModel

app = FastAPI()

# ëª¨ë¸ ë¡œë“œ
draft = AutoModelForCausalLM.from_pretrained("TinyLlama/TinyLlama-1.1B", device_map="cuda:0")
target = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-70b-hf", device_map="cuda:1")
decoder = SpeculativeDecoder(draft, target, tokenizer)

class Message(BaseModel):
    prompt: str
    max_tokens: int = 100

@app.post("/generate")
async def generate(msg: Message):
    # Speculative decoding
    output = decoder.generate(
        msg.prompt,
        max_tokens=msg.max_tokens,
        K=5
    )
    
    return {"output": output}

# ì‚¬ìš©
# curl -X POST "http://localhost:8000/generate" \
#   -H "Content-Type: application/json" \
#   -d '{"prompt": "Explain AI", "max_tokens": 100}'
```

---

## í•œê³„ì™€ íŠ¸ë ˆì´ë“œì˜¤í”„

### 1. Acceptance Rate ì˜ì¡´

ë‚®ì€ acceptance rate â†’ ì†ë„ì—… ê°ì†Œ

```
Acceptance 30%: 1.5x
Acceptance 50%: 2.2x
Acceptance 70%: 3.0x
```

**ëŒ€ì±…:** Draft fine-tuning

### 2. ë©”ëª¨ë¦¬ ì¦ê°€

Draft + Target ëª¨ë‘ ë©”ëª¨ë¦¬ì—:

```
70B model: 140 GB
+ 7B draft: 14 GB
Total: 154 GB
```

**ëŒ€ì±…:** Quantization (4-bit target)

### 3. Draft Overhead

Draftê°€ ë„ˆë¬´ í¬ë©´ ì˜¤íˆë ¤ ëŠë¦¼:

```
70B + 30B draft: 1.2x (ë³„ë¡œ)
70B + 7B draft: 2.5x (ì¢‹ìŒ)
70B + 1B draft: 2.3x (ì¢‹ìŒ)
```

---

## ë‹¤ë¥¸ ê¸°ë²•ê³¼ ë¹„êµ

| ê¸°ë²• | ì†ë„ì—… | í’ˆì§ˆ | ë©”ëª¨ë¦¬ |
|------|--------|------|--------|
| Speculative Decoding | 2-3x | 100% | 1.1x |
| Flash Attention | 2-4x | 100% | 0.1x |
| Quantization | 2x | 98% | 0.25x |
| Pruning | 1.5x | 95% | 0.5x |
| **All Combined** | **10x+** | 98% | 0.5x |

---

## ìš”ì•½

**Speculative Decoding**ì€:

1. **ì‘ì€ ëª¨ë¸ì´ ì¶”ì¸¡**, í° ëª¨ë¸ì´ ê²€ì¦
2. **ë¬´ì†ì‹¤**: ê²°ê³¼ëŠ” ì •í™•íˆ ë™ì¼
3. **2-3ë°° ì†ë„ í–¥ìƒ**
4. **ë©”ëª¨ë¦¬ ì¦ê°€**: Draft ëª¨ë¸ ì¶”ê°€

**í•µì‹¬:**
- K = 5-7 ìµœì 
- Acceptance rate ì¤‘ìš” (60% ì´ìƒ)
- Draft fine-tuningìœ¼ë¡œ ê°œì„ 

**ì‚¬ìš©ì²˜:**
- ëŒ€ê·œëª¨ ëª¨ë¸ ì„œë¹™ (70B+)
- ë ˆì´í„´ì‹œ ì¤‘ìš”í•œ ì±—ë´‡
- ë¹„ìš© ì ˆê° (ê°™ì€ ì²˜ë¦¬ëŸ‰, ì ì€ GPU)

---

## ë‹¤ìŒ ê¸€

**9í¸: Continuous Batching**
- ë™ì  ë°°ì¹˜ ì²˜ë¦¬
- ì²˜ë¦¬ëŸ‰ ê·¹ëŒ€í™”
- vLLM, TGI ë™ì‘ ì›ë¦¬

ê¸°ëŒ€í•´ì£¼ì„¸ìš”! ğŸš€

---

*ì§ˆë¬¸ì´ë‚˜ í”¼ë“œë°±ì€ [GitHub](https://github.com/yhna941)ì—ì„œ í™˜ì˜í•©ë‹ˆë‹¤!*
