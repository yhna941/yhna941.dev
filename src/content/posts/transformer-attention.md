---
title: "Transformer #1: Attention ë©”ì»¤ë‹ˆì¦˜ì˜ ëª¨ë“  ê²ƒ"
description: "Seq2Seqë¶€í„° Self-Attentionê¹Œì§€, Attention ë©”ì»¤ë‹ˆì¦˜ì˜ ì§„í™”ì™€ ì›ë¦¬ë¥¼ ì™„ì „íˆ ì´í•´í•©ë‹ˆë‹¤."
pubDate: 2026-02-06
author: "Yh Na"
tags: ["transformer", "attention", "deep-learning", "nlp", "pytorch"]
draft: false
---

# Transformer #1: Attention ë©”ì»¤ë‹ˆì¦˜

**"Attention is All You Need"** (2017)

ì´ í•œ ë¬¸ì¥ì´ AIë¥¼ ë°”ê¿¨ìŠµë‹ˆë‹¤.

ì´ë²ˆ ì‹œë¦¬ì¦ˆ:
- Attentionì˜ íƒ„ìƒ
- Self-Attention ìˆ˜í•™
- Multi-Head Attention
- ì™„ì „í•œ êµ¬í˜„

---

## ì™œ Attentionì¸ê°€?

### ë¬¸ì œ: RNNì˜ í•œê³„

**ê¸°ê³„ ë²ˆì—­ (Seq2Seq):**

```
Input:  "I love deep learning"
Output: "ë‚˜ëŠ” ë”¥ëŸ¬ë‹ì„ ì‚¬ë‘í•œë‹¤"
```

**RNN Encoder-Decoder (2014):**

```python
# Encoder: ë¬¸ì¥ â†’ ê³ ì • í¬ê¸° ë²¡í„°
h1 = rnn(embed("I"))
h2 = rnn(h1, embed("love"))
h3 = rnn(h2, embed("deep"))
h4 = rnn(h3, embed("learning"))

context = h4  # ì „ì²´ ë¬¸ì¥ ì •ë³´ë¥¼ í•˜ë‚˜ì˜ ë²¡í„°ì—!

# Decoder: ë²¡í„° â†’ ë²ˆì—­
s1 = rnn(context, embed("<start>"))
y1 = softmax(W @ s1)  # "ë‚˜ëŠ”"

s2 = rnn(s1, embed("ë‚˜ëŠ”"))
y2 = softmax(W @ s2)  # "ë”¥ëŸ¬ë‹ì„"

# ...
```

**ë¬¸ì œ:**

```
Long sentence: "I love deep learning and ..."
                                           â†‘
                                    ì •ë³´ ì†ì‹¤!

context ë²¡í„°ê°€ ëª¨ë“  ê²ƒì„ ë‹´ì•„ì•¼ í•¨
â†’ Bottleneck!
```

### í•´ê²°: Attention

**ì•„ì´ë””ì–´:**

> "ë²ˆì—­í•  ë•Œë§ˆë‹¤ ì…ë ¥ ë¬¸ì¥ì˜ ê´€ë ¨ ë¶€ë¶„ì„ ë‹¤ì‹œ ë³¸ë‹¤!"

```
"ë‚˜ëŠ”" ìƒì„± ì‹œ â†’ "I" ì§‘ì¤‘
"ë”¥ëŸ¬ë‹ì„" ìƒì„± ì‹œ â†’ "deep learning" ì§‘ì¤‘
"ì‚¬ë‘í•œë‹¤" ìƒì„± ì‹œ â†’ "love" ì§‘ì¤‘
```

---

## Attention ë©”ì»¤ë‹ˆì¦˜ (Bahdanau, 2015)

### ìˆ˜í•™

**1. Encoder (ì–‘ë°©í–¥ RNN):**

```python
# Forward
h1_fwd = rnn_fwd(embed("I"))
h2_fwd = rnn_fwd(h1_fwd, embed("love"))
h3_fwd = rnn_fwd(h2_fwd, embed("deep"))
h4_fwd = rnn_fwd(h3_fwd, embed("learning"))

# Backward
h4_bwd = rnn_bwd(embed("learning"))
h3_bwd = rnn_bwd(h4_bwd, embed("deep"))
h2_bwd = rnn_bwd(h3_bwd, embed("love"))
h1_bwd = rnn_bwd(h2_bwd, embed("I"))

# Concatenate
h1 = [h1_fwd; h1_bwd]
h2 = [h2_fwd; h2_bwd]
h3 = [h3_fwd; h3_bwd]
h4 = [h4_fwd; h4_bwd]
```

**2. Attention Scores:**

Decoder state `s_t`ì™€ ê° encoder state `h_i`ì˜ ê´€ë ¨ë„:

$$
e_{t,i} = \text{score}(s_t, h_i) = v^T \tanh(W_1 s_t + W_2 h_i)
$$

**3. Attention Weights (Softmax):**

$$
\alpha_{t,i} = \frac{\exp(e_{t,i})}{\sum_{j=1}^n \exp(e_{t,j})}
$$

**4. Context Vector:**

ê°€ì¤‘ í‰ê· :

$$
c_t = \sum_{i=1}^n \alpha_{t,i} h_i
$$

**5. Decoder:**

Contextë¥¼ ì‚¬ìš©:

$$
s_t = \text{RNN}(s_{t-1}, [y_{t-1}; c_t])
$$

$$
p(y_t) = \text{softmax}(W_o s_t)
$$

### PyTorch êµ¬í˜„

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class BahdanauAttention(nn.Module):
    def __init__(self, hidden_size):
        super().__init__()
        self.W1 = nn.Linear(hidden_size, hidden_size)  # Decoder projection
        self.W2 = nn.Linear(hidden_size, hidden_size)  # Encoder projection
        self.v = nn.Linear(hidden_size, 1)  # Score
    
    def forward(self, decoder_hidden, encoder_outputs):
        """
        decoder_hidden: (batch, hidden_size)
        encoder_outputs: (batch, seq_len, hidden_size)
        """
        batch_size = encoder_outputs.size(0)
        seq_len = encoder_outputs.size(1)
        
        # Expand decoder_hidden to (batch, seq_len, hidden_size)
        decoder_hidden = decoder_hidden.unsqueeze(1).repeat(1, seq_len, 1)
        
        # Score: (batch, seq_len, 1)
        energy = torch.tanh(
            self.W1(decoder_hidden) + self.W2(encoder_outputs)
        )
        scores = self.v(energy).squeeze(2)  # (batch, seq_len)
        
        # Attention weights
        attention_weights = F.softmax(scores, dim=1)  # (batch, seq_len)
        
        # Context vector
        context = torch.bmm(
            attention_weights.unsqueeze(1),  # (batch, 1, seq_len)
            encoder_outputs  # (batch, seq_len, hidden_size)
        ).squeeze(1)  # (batch, hidden_size)
        
        return context, attention_weights

class Seq2SeqWithAttention(nn.Module):
    def __init__(self, vocab_size, embed_size, hidden_size):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embed_size)
        
        # Encoder (Bidirectional GRU)
        self.encoder = nn.GRU(
            embed_size, 
            hidden_size, 
            bidirectional=True, 
            batch_first=True
        )
        
        # Attention
        self.attention = BahdanauAttention(hidden_size * 2)
        
        # Decoder
        self.decoder = nn.GRU(
            embed_size + hidden_size * 2,  # Input + context
            hidden_size * 2,
            batch_first=True
        )
        
        # Output
        self.out = nn.Linear(hidden_size * 2, vocab_size)
    
    def forward(self, src, tgt):
        """
        src: (batch, src_len)
        tgt: (batch, tgt_len)
        """
        # Encode
        src_embedded = self.embedding(src)
        encoder_outputs, hidden = self.encoder(src_embedded)
        # encoder_outputs: (batch, src_len, hidden_size * 2)
        
        # Decode
        tgt_embedded = self.embedding(tgt)
        batch_size = tgt.size(0)
        tgt_len = tgt.size(1)
        
        outputs = []
        decoder_hidden = hidden[-1].unsqueeze(0)  # Last layer
        
        for t in range(tgt_len):
            # Attention
            context, attn_weights = self.attention(
                decoder_hidden.squeeze(0),
                encoder_outputs
            )
            
            # Decoder input: [embedding; context]
            decoder_input = torch.cat([
                tgt_embedded[:, t:t+1],
                context.unsqueeze(1)
            ], dim=2)
            
            # Decode
            output, decoder_hidden = self.decoder(
                decoder_input,
                decoder_hidden
            )
            
            # Predict
            pred = self.out(output.squeeze(1))
            outputs.append(pred)
        
        return torch.stack(outputs, dim=1)

# ì‚¬ìš©
model = Seq2SeqWithAttention(
    vocab_size=10000,
    embed_size=256,
    hidden_size=512
)

src = torch.randint(0, 10000, (32, 20))  # Batch 32, length 20
tgt = torch.randint(0, 10000, (32, 15))  # Batch 32, length 15

output = model(src, tgt)
print(output.shape)  # (32, 15, 10000)
```

---

## Self-Attention (Transformer, 2017)

### í•µì‹¬ ì•„ì´ë””ì–´

**Seq2Seq Attention:**
- Decoder â†’ Encoder ê´€ê³„
- ì‹œí€€ìŠ¤ ê°„ attention

**Self-Attention:**
- í•œ ì‹œí€€ìŠ¤ ë‚´ë¶€ ê´€ê³„
- "ë‹¨ì–´ë“¤ë¼ë¦¬ ì„œë¡œ ë³¸ë‹¤!"

**ì˜ˆì‹œ:**

```
"The animal didn't cross the street because it was too tired"
                                                  â†‘
                                    "it" refers to "animal"
```

Self-Attentionìœ¼ë¡œ "it"ì´ "animal"ê³¼ ê´€ë ¨ ìˆìŒì„ í•™ìŠµ!

### Query, Key, Value (QKV)

**ì •ë³´ ê²€ìƒ‰ ë¹„ìœ :**

```
YouTube ê²€ìƒ‰:
- Query: "ë”¥ëŸ¬ë‹ ê°•ì˜"
- Key: ê° ì˜ìƒì˜ ì œëª©/íƒœê·¸
- Value: ì‹¤ì œ ì˜ìƒ ë‚´ìš©

ê²€ìƒ‰ ê³¼ì •:
1. Queryì™€ Key ë¹„êµ (ìœ ì‚¬ë„)
2. ìœ ì‚¬ë„ ë†’ì€ ì˜ìƒ ì„ íƒ
3. í•´ë‹¹ Value(ì˜ìƒ) ë°˜í™˜
```

**Self-Attention:**

```python
# ê° ë‹¨ì–´ë¥¼ 3ê°œë¡œ ë³€í™˜
Q = X @ W_Q  # Query: "ë‚´ê°€ ì°¾ëŠ” ê²ƒ"
K = X @ W_K  # Key: "ë‚´ê°€ ì œê³µí•˜ëŠ” ê²ƒ"
V = X @ W_V  # Value: "ì‹¤ì œ ë‚´ìš©"
```

### ìˆ˜í•™

**ì…ë ¥:**

$$
X \in \mathbb{R}^{n \times d}
$$

- $n$: ì‹œí€€ìŠ¤ ê¸¸ì´
- $d$: ì°¨ì›

**ë³€í™˜:**

$$
Q = XW_Q, \quad K = XW_K, \quad V = XW_V
$$

- $W_Q, W_K, W_V \in \mathbb{R}^{d \times d_k}$

**Attention Scores:**

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$

**ë‹¨ê³„ë³„:**

1. **Similarity:** $QK^T \in \mathbb{R}^{n \times n}$
   - ëª¨ë“  ë‹¨ì–´ ìŒì˜ ìœ ì‚¬ë„

2. **Scaling:** $\frac{QK^T}{\sqrt{d_k}}$
   - Gradient ì•ˆì •í™”

3. **Weights:** $\text{softmax}(\cdots)$
   - ê° í–‰ì´ í™•ë¥  ë¶„í¬

4. **Output:** $\text{softmax}(\cdots) V \in \mathbb{R}^{n \times d_k}$
   - Weighted sum of values

### PyTorch êµ¬í˜„

```python
class ScaledDotProductAttention(nn.Module):
    def __init__(self, temperature):
        super().__init__()
        self.temperature = temperature  # sqrt(d_k)
    
    def forward(self, q, k, v, mask=None):
        """
        q: (batch, n_heads, seq_len, d_k)
        k: (batch, n_heads, seq_len, d_k)
        v: (batch, n_heads, seq_len, d_v)
        mask: (batch, 1, 1, seq_len) or (batch, 1, seq_len, seq_len)
        """
        # 1. Q @ K^T
        attn = torch.matmul(q, k.transpose(-2, -1))  # (batch, n_heads, seq_len, seq_len)
        
        # 2. Scale
        attn = attn / self.temperature
        
        # 3. Mask (optional)
        if mask is not None:
            attn = attn.masked_fill(mask == 0, -1e9)
        
        # 4. Softmax
        attn = F.softmax(attn, dim=-1)
        
        # 5. @ V
        output = torch.matmul(attn, v)  # (batch, n_heads, seq_len, d_v)
        
        return output, attn

class SelfAttention(nn.Module):
    def __init__(self, d_model, d_k, d_v):
        super().__init__()
        self.d_k = d_k
        
        self.W_q = nn.Linear(d_model, d_k)
        self.W_k = nn.Linear(d_model, d_k)
        self.W_v = nn.Linear(d_model, d_v)
        
        self.attention = ScaledDotProductAttention(temperature=d_k ** 0.5)
    
    def forward(self, x, mask=None):
        """
        x: (batch, seq_len, d_model)
        """
        # Q, K, V
        q = self.W_q(x)  # (batch, seq_len, d_k)
        k = self.W_k(x)
        v = self.W_v(x)
        
        # Attention
        output, attn = self.attention(q, k, v, mask)
        
        return output, attn

# ì‚¬ìš©
d_model = 512
d_k = d_v = 64

attn = SelfAttention(d_model, d_k, d_v)
x = torch.randn(32, 10, d_model)  # Batch 32, seq 10
output, weights = attn(x)

print(output.shape)  # (32, 10, 64)
print(weights.shape)  # (32, 10, 10)
```

### Attention Weights ì‹œê°í™”

```python
import matplotlib.pyplot as plt
import seaborn as sns

def visualize_attention(attention_weights, src_tokens, tgt_tokens):
    """
    attention_weights: (tgt_len, src_len)
    """
    plt.figure(figsize=(10, 8))
    sns.heatmap(
        attention_weights,
        xticklabels=src_tokens,
        yticklabels=tgt_tokens,
        cmap="YlGnBu",
        cbar=True
    )
    plt.xlabel("Source")
    plt.ylabel("Target")
    plt.title("Attention Weights")
    plt.show()

# ì˜ˆì‹œ
src = ["The", "animal", "didn't", "cross", "the", "street"]
tgt = ["ë™ë¬¼ì€", "ê±°ë¦¬ë¥¼", "ê±´ë„ˆì§€", "ì•Šì•˜ë‹¤"]
weights = torch.softmax(torch.randn(4, 6), dim=1).numpy()

visualize_attention(weights, src, tgt)
```

---

## Multi-Head Attention

### ì™œ?

**Single Headì˜ í•œê³„:**

```
"The animal didn't cross the street because it was too tired"

Single attention:
- "it" â†’ "animal" (70%)
- "it" â†’ "street" (30%)

But...
- ì˜ë¯¸ ê´€ê³„: "it" â†’ "animal"
- ë¬¸ë²• ê´€ê³„: "it" â†’ "was"
- ìœ„ì¹˜ ê´€ê³„: "it" â†’ "tired"
```

**Multi-Head:**

> "ì—¬ëŸ¬ ê´€ì ì—ì„œ ë™ì‹œì— ë³¸ë‹¤!"

### ìˆ˜í•™

**$h$ê°œì˜ Head:**

$$
\text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)
$$

$$
\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \ldots, \text{head}_h)W^O
$$

- $W_i^Q, W_i^K, W_i^V \in \mathbb{R}^{d_{model} \times d_k}$
- $W^O \in \mathbb{R}^{hd_v \times d_{model}}$
- ë³´í†µ $d_k = d_v = d_{model} / h$

**ì˜ˆì‹œ (8 heads, d_model=512):**

```
d_k = 512 / 8 = 64

Head 1: (batch, seq_len, 64)
Head 2: (batch, seq_len, 64)
...
Head 8: (batch, seq_len, 64)

Concat: (batch, seq_len, 512)
Project: (batch, seq_len, 512)
```

### PyTorch êµ¬í˜„

```python
class MultiHeadAttention(nn.Module):
    def __init__(self, n_heads, d_model, dropout=0.1):
        super().__init__()
        assert d_model % n_heads == 0
        
        self.n_heads = n_heads
        self.d_model = d_model
        self.d_k = d_model // n_heads
        
        # Linear projections
        self.W_q = nn.Linear(d_model, d_model)
        self.W_k = nn.Linear(d_model, d_model)
        self.W_v = nn.Linear(d_model, d_model)
        self.W_o = nn.Linear(d_model, d_model)
        
        # Attention
        self.attention = ScaledDotProductAttention(temperature=self.d_k ** 0.5)
        
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, q, k, v, mask=None):
        """
        q, k, v: (batch, seq_len, d_model)
        """
        batch_size = q.size(0)
        
        # 1. Linear projections
        q = self.W_q(q)  # (batch, seq_len, d_model)
        k = self.W_k(k)
        v = self.W_v(v)
        
        # 2. Split into heads
        # (batch, seq_len, d_model) â†’ (batch, seq_len, n_heads, d_k)
        # â†’ (batch, n_heads, seq_len, d_k)
        q = q.view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)
        k = k.view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)
        v = v.view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)
        
        # 3. Attention for each head
        output, attn = self.attention(q, k, v, mask)
        # output: (batch, n_heads, seq_len, d_k)
        
        # 4. Concat heads
        # (batch, n_heads, seq_len, d_k) â†’ (batch, seq_len, n_heads, d_k)
        # â†’ (batch, seq_len, d_model)
        output = output.transpose(1, 2).contiguous().view(
            batch_size, -1, self.d_model
        )
        
        # 5. Final projection
        output = self.W_o(output)
        output = self.dropout(output)
        
        return output, attn

# ì‚¬ìš©
mha = MultiHeadAttention(n_heads=8, d_model=512)
x = torch.randn(32, 10, 512)

output, attn = mha(x, x, x)
print(output.shape)  # (32, 10, 512)
print(attn.shape)  # (32, 8, 10, 10)
```

---

## Masking

### 1. Padding Mask

**ë¬¸ì œ:**

```
Batch:
- "I love you" (3 words)
- "Deep learning is awesome" (4 words)

Padded:
- "I love you <pad>"
- "Deep learning is awesome"
```

Paddingì— attention ì£¼ë©´ ì•ˆ ë¨!

**í•´ê²°:**

```python
def create_padding_mask(seq):
    """
    seq: (batch, seq_len)
    return: (batch, 1, 1, seq_len)
    """
    # 0ì€ <pad>
    mask = (seq != 0).unsqueeze(1).unsqueeze(2)
    return mask

# ì‚¬ìš©
seq = torch.tensor([[1, 2, 3, 0], [4, 5, 6, 7]])
mask = create_padding_mask(seq)
print(mask.shape)  # (2, 1, 1, 4)
```

### 2. Look-Ahead Mask (Decoder)

**ë¬¸ì œ:**

```
ë²ˆì—­ ì¤‘:
"I love deep learning"
â†’ "ë‚˜ëŠ” ë”¥ëŸ¬ë‹ì„ ì‚¬ë‘í•œë‹¤"

"ë”¥ëŸ¬ë‹ì„" ìƒì„± ì‹œ:
- "ë‚˜ëŠ”" ë´ì•¼ í•¨ âœ…
- "ë”¥ëŸ¬ë‹ì„" ë³´ë©´ ì•ˆ ë¨! âŒ (ë¯¸ë˜)
- "ì‚¬ë‘í•œë‹¤" ë³´ë©´ ì•ˆ ë¨! âŒ (ë¯¸ë˜)
```

**í•´ê²°:**

```python
def create_look_ahead_mask(size):
    """
    return: (size, size) upper triangular matrix
    """
    mask = torch.triu(torch.ones(size, size), diagonal=1)
    return mask == 0

# ì‚¬ìš©
mask = create_look_ahead_mask(4)
print(mask)
# tensor([[ True, False, False, False],
#         [ True,  True, False, False],
#         [ True,  True,  True, False],
#         [ True,  True,  True,  True]])
```

---

## ì„±ëŠ¥ ë¹„êµ

### RNN vs Attention

**ê³„ì‚° ë³µì¡ë„:**

```
RNN: O(n) sequential operations
     â†’ ë³‘ë ¬í™” ë¶ˆê°€

Self-Attention: O(1) sequential
                 â†’ ì™„ì „ ë³‘ë ¬í™”!
```

**ì‹¤ì œ ì†ë„ (n=512, d=512):**

```
RNN:
- Forward: 0.5s
- Backward: 1.0s
- Total: 1.5s

Self-Attention:
- Forward: 0.1s
- Backward: 0.2s
- Total: 0.3s

5ë°° ë¹ ë¦„!
```

**ë©”ëª¨ë¦¬:**

```
RNN: O(nÂ·d)
Self-Attention: O(nÂ²Â·d)

ë‹¨, nì´ ì‘ìœ¼ë©´ Self-Attention ìœ ë¦¬
```

---

## ìš”ì•½

**Attentionì˜ ì§„í™”:**

```
Seq2Seq (2014)
â†’ Bahdanau Attention (2015)
â†’ Self-Attention (2017)
â†’ Transformer!
```

**í•µì‹¬ ê°œë…:**

1. **Attention**: ê´€ë ¨ ë¶€ë¶„ì— ì§‘ì¤‘
2. **Self-Attention**: ì‹œí€€ìŠ¤ ë‚´ë¶€ ê´€ê³„
3. **QKV**: Query, Key, Value
4. **Multi-Head**: ì—¬ëŸ¬ ê´€ì 
5. **Masking**: Padding, Look-ahead

**ìˆ˜ì‹:**

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$

**ë‹¤ìŒ ê¸€:**
- **Transformer êµ¬ì¡°**: Encoder, Decoder ì™„ì „ ë¶„í•´
- **Positional Encoding**: ìœ„ì¹˜ ì •ë³´
- **Training Tips**: í•™ìŠµ ê¸°ë²•

ê¸°ëŒ€í•´ì£¼ì„¸ìš”! ğŸš€

---

*ì§ˆë¬¸ì´ë‚˜ í”¼ë“œë°±ì€ [GitHub](https://github.com/yhna941)ì—ì„œ í™˜ì˜í•©ë‹ˆë‹¤!*
