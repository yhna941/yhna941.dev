---
title: "LLM Post-training #4: RLAIF - AI í”¼ë“œë°±ìœ¼ë¡œ í™•ì¥ ê°€ëŠ¥í•œ ì •ë ¬"
description: "Reinforcement Learning from AI Feedback (RLAIF)ë¡œ ì¸ê°„ ë¼ë²¨ëŸ¬ ì—†ì´ ëŒ€ê·œëª¨ë¡œ ëª¨ë¸ì„ ì •ë ¬í•˜ëŠ” ë°©ë²•ì„ ì•Œì•„ë´…ë‹ˆë‹¤."
pubDate: 2026-02-06
author: "Yh Na"
tags: ["llm", "rlaif", "alignment", "post-training", "ai-feedback"]
draft: false
---

# LLM Post-training #4: RLAIF

RLHFì˜ ë³‘ëª©:
- **ì¸ê°„ ë¼ë²¨ëŸ¬ ë¹„ìš©** ($100K+)
- **í™•ì¥ì„±** (ì–¸ì–´/ë„ë©”ì¸ë§ˆë‹¤ ë°˜ë³µ)
- **ì†ë„** (ì‚¬ëŒì€ ëŠë¦¼)

**RLAIF (RL from AI Feedback)**ëŠ” í•´ê²°ì±…:
- **AIê°€ í”¼ë“œë°±** (GPT-4, Claude ë“±)
- **ë¬´í•œ í™•ì¥** (ë°ì´í„° ë¬´ì œí•œ)
- **ë¹ ë¦„** (API í˜¸ì¶œ)
- **ì €ë ´** ($1K)

ê²°ê³¼:
- Claude 2: Constitutional AI (RLAIF ë³€í˜•)
- Llama 3: AI feedback í™œìš©
- Google: RLAIF = RLHF ì„±ëŠ¥

---

## í•µì‹¬ ì•„ì´ë””ì–´

### RLHF (Human)

```
1. ì¸ê°„ì´ ë‹µë³€ A, B ë¹„êµ
   "Aê°€ ë” ìœ ìš©í•¨"
   
2. Reward model í•™ìŠµ
   r(A) > r(B)
   
3. RLë¡œ reward ìµœëŒ€í™”
```

**ë¬¸ì œ:** ì¸ê°„ í•„ìš” (ë¹„ìš©, ì‹œê°„)

### RLAIF (AI)

```
1. AIê°€ ë‹µë³€ A, B ë¹„êµ
   "A is more helpful because..."
   
2. Reward model í•™ìŠµ
   r(A) > r(B)
   
3. RLë¡œ reward ìµœëŒ€í™”
```

**ì¥ì :** AI ë¬´ì œí•œ (í™•ì¥ ê°€ëŠ¥)

---

## íŒŒì´í”„ë¼ì¸

### 1ë‹¨ê³„: AI Annotator

```python
def ai_annotate(prompt, response_A, response_B):
    """AIê°€ ì„ í˜¸ë„ íŒë‹¨"""
    
    annotation_prompt = f"""
You are an expert evaluator. Compare two responses.

Prompt: {prompt}

Response A: {response_A}

Response B: {response_B}

Which response is better? Consider:
- Helpfulness
- Harmlessness
- Honesty
- Clarity

Answer with "A" or "B" and explain why.
"""
    
    # AI ëª¨ë¸ í˜¸ì¶œ (GPT-4, Claude ë“±)
    result = ai_model.generate(annotation_prompt)
    
    # Parse
    if "Response A" in result or result.startswith("A"):
        preference = "A"
    else:
        preference = "B"
    
    return {
        "preference": preference,
        "explanation": result
    }
```

### 2ë‹¨ê³„: Reward Model í•™ìŠµ

```python
# RLHFì™€ ë™ì¼!
class RewardModel(nn.Module):
    # ... (ì´ì „ê³¼ ê°™ìŒ)

# AI preferenceë¡œ í•™ìŠµ
for batch in ai_preference_data:
    prompt = batch["prompt"]
    response_A = batch["response_A"]
    response_B = batch["response_B"]
    preference = batch["preference"]  # AIê°€ ì„ íƒ
    
    reward_A = reward_model(prompt + response_A)
    reward_B = reward_model(prompt + response_B)
    
    if preference == "A":
        loss = -log_sigmoid(reward_A - reward_B)
    else:
        loss = -log_sigmoid(reward_B - reward_A)
    
    loss.backward()
```

### 3ë‹¨ê³„: RL Fine-tuning

```python
# RLHF/GRPOì™€ ë™ì¼
ppo_trainer = PPOTrainer(
    model=policy_model,
    ref_model=ref_model,
    reward_model=reward_model  # AI feedbackë¡œ í•™ìŠµëœ ê²ƒ!
)

ppo_trainer.train()
```

---

## AI Annotator ì„¤ê³„

### 1. Zero-shot

```python
prompt = """
Compare these responses. Which is better?

Prompt: {user_question}
Response A: {response_A}
Response B: {response_B}

Choose A or B.
"""

# ê°„ë‹¨í•˜ì§€ë§Œ ì¼ê´€ì„± ë‚®ìŒ
```

### 2. Few-shot

```python
prompt = """
You are an expert evaluator.

Example 1:
Prompt: "What is 2+2?"
Response A: "4"
Response B: "Idk"
Better: A (correct and concise)

Example 2:
Prompt: "Explain AI"
Response A: "AI is artificial intelligence..."
Response B: "AI AI AI AI" 
Better: A (informative)

Now evaluate:
Prompt: {user_question}
Response A: {response_A}
Response B: {response_B}

Which is better and why?
"""

# ì¼ê´€ì„± í–¥ìƒ!
```

### 3. Chain-of-Thought

```python
prompt = """
Evaluate step-by-step:

1. Helpfulness: Which response better answers the question?
2. Harmlessness: Which is safer?
3. Honesty: Which is more truthful?
4. Clarity: Which is clearer?

Response A: {response_A}
Response B: {response_B}

Analysis:
1. Helpfulness: [your analysis]
2. Harmlessness: [your analysis]
3. Honesty: [your analysis]
4. Clarity: [your analysis]

Conclusion: [A or B] is better because...
"""

# ìµœê³  í’ˆì§ˆ!
```

---

## Constitutional AI (Anthropic)

Claudeì˜ í•µì‹¬ ê¸°ìˆ !

### ì›ë¦¬

**Constitution**: ê·œì¹™ ì§‘í•©

```python
CONSTITUTION = [
    "Choose the response that is more helpful and harmless",
    "Avoid responses that are illegal or unethical",
    "Prefer responses that are honest and acknowledge uncertainty",
    "Choose responses that are clearer and more informative"
]
```

### ì•Œê³ ë¦¬ì¦˜

```python
def constitutional_ai(response, constitution):
    """
    Constitutional AI feedback
    """
    critiques = []
    
    # 1. Critique phase
    for principle in constitution:
        critique_prompt = f"""
Principle: {principle}

Response: {response}

Does this response violate the principle?
If yes, how should it be revised?
"""
        critique = ai_model.generate(critique_prompt)
        critiques.append(critique)
    
    # 2. Revision phase
    revision_prompt = f"""
Original response: {response}

Critiques:
{'\n'.join(critiques)}

Revise the response to address all critiques:
"""
    
    revised_response = ai_model.generate(revision_prompt)
    
    return revised_response


# ë°˜ë³µ ê°œì„ 
response = initial_response
for _ in range(3):  # 3 iterations
    response = constitutional_ai(response, CONSTITUTION)
```

### Self-improvement

```python
# 1. ëª¨ë¸ì´ ìê¸° ì¶œë ¥ í‰ê°€
response_A = model.generate(prompt)
response_B = model.generate(prompt)

# 2. ëª¨ë¸ì´ ìê¸° ë¹„êµ
preference = model.evaluate(response_A, response_B, CONSTITUTION)

# 3. Preference dataë¡œ í•™ìŠµ
# (Bootstrapping!)
```

---

## ì‹¤ì „ êµ¬í˜„

### ì „ì²´ íŒŒì´í”„ë¼ì¸

```python
import anthropic
from transformers import AutoModelForCausalLM
from trl import PPOTrainer

class RLAIFTrainer:
    def __init__(
        self,
        policy_model,
        ref_model,
        ai_judge_model="claude-3-opus",
        constitution=None
    ):
        self.policy = policy_model
        self.ref = ref_model
        self.ai_judge = anthropic.Anthropic()
        self.constitution = constitution or DEFAULT_CONSTITUTION
    
    def generate_preference_data(self, prompts, num_pairs=2):
        """AIë¡œ preference data ìƒì„±"""
        preference_data = []
        
        for prompt in prompts:
            # 1. Generate responses
            responses = []
            for _ in range(num_pairs):
                response = self.policy.generate(prompt)
                responses.append(response)
            
            # 2. AI judges
            for i in range(len(responses)):
                for j in range(i+1, len(responses)):
                    response_A = responses[i]
                    response_B = responses[j]
                    
                    # AI annotation
                    preference = self.ai_annotate(
                        prompt,
                        response_A,
                        response_B
                    )
                    
                    preference_data.append({
                        "prompt": prompt,
                        "chosen": response_A if preference == "A" else response_B,
                        "rejected": response_B if preference == "A" else response_A
                    })
        
        return preference_data
    
    def ai_annotate(self, prompt, response_A, response_B):
        """AIê°€ ì„ í˜¸ë„ íŒë‹¨ (Constitutional AI ìŠ¤íƒ€ì¼)"""
        
        eval_prompt = f"""
You are an expert evaluator following these principles:

{chr(10).join(f"- {p}" for p in self.constitution)}

Compare these responses:

User: {prompt}

Response A: {response_A}

Response B: {response_B}

Which response better follows the principles? 
Answer with "A" or "B" and explain.
"""
        
        # Claude API í˜¸ì¶œ
        message = self.ai_judge.messages.create(
            model="claude-3-opus-20240229",
            max_tokens=500,
            messages=[{"role": "user", "content": eval_prompt}]
        )
        
        result = message.content[0].text
        
        # Parse
        preference = "A" if "Response A" in result[:50] else "B"
        
        return preference
    
    def train_reward_model(self, preference_data):
        """Reward model í•™ìŠµ"""
        reward_model = RewardModel(self.policy)
        
        optimizer = torch.optim.AdamW(reward_model.parameters(), lr=1e-5)
        
        for epoch in range(3):
            for batch in DataLoader(preference_data, batch_size=4):
                # ... (ì´ì „ê³¼ ë™ì¼)
                pass
        
        return reward_model
    
    def train_policy(self, reward_model, prompts):
        """RLë¡œ policy í•™ìŠµ"""
        ppo_trainer = PPOTrainer(
            model=self.policy,
            ref_model=self.ref,
            reward_model=reward_model
        )
        
        for epoch in range(3):
            for prompt in prompts:
                # ... (PPO/GRPO)
                pass


# ì‚¬ìš©
DEFAULT_CONSTITUTION = [
    "Be helpful and informative",
    "Be harmless and avoid toxic content",
    "Be honest and acknowledge limitations",
    "Be clear and well-structured"
]

policy_model = AutoModelForCausalLM.from_pretrained("llama2-7b-sft")
ref_model = AutoModelForCausalLM.from_pretrained("llama2-7b-sft")

trainer = RLAIFTrainer(
    policy_model=policy_model,
    ref_model=ref_model,
    ai_judge_model="claude-3-opus",
    constitution=DEFAULT_CONSTITUTION
)

# 1. Generate preference data
prompts = load_prompts()
preference_data = trainer.generate_preference_data(prompts, num_pairs=4)

# 2. Train reward model
reward_model = trainer.train_reward_model(preference_data)

# 3. Train policy
trainer.train_policy(reward_model, prompts)
```

---

## AI Judge ì„ íƒ

### GPT-4

```python
import openai

def gpt4_judge(prompt, response_A, response_B):
    response = openai.ChatCompletion.create(
        model="gpt-4-turbo",
        messages=[{
            "role": "user",
            "content": f"Compare:\nA: {response_A}\nB: {response_B}\nWhich is better?"
        }]
    )
    
    return "A" if "A" in response.choices[0].message.content[:10] else "B"
```

**ì¥ì :** ê°•ë ¥, ë‹¤ëª©ì   
**ë‹¨ì :** ë¹„ìš© ($0.01/1K tokens)

### Claude

```python
import anthropic

def claude_judge(prompt, response_A, response_B):
    client = anthropic.Anthropic()
    
    message = client.messages.create(
        model="claude-3-opus-20240229",
        messages=[{
            "role": "user",
            "content": f"Compare:\nA: {response_A}\nB: {response_B}"
        }]
    )
    
    return "A" if "A" in message.content[0].text[:10] else "B"
```

**ì¥ì :** Constitutional AIì— ìµœì   
**ë‹¨ì :** ë¹„ìš© ë¹„ìŠ·

### Open-source (ìì²´ ëª¨ë¸)

```python
def self_judge(model, prompt, response_A, response_B):
    """ìê¸° ìì‹ ì´ í‰ê°€"""
    judge_prompt = f"Compare:\nA: {response_A}\nB: {response_B}\nBetter:"
    
    output = model.generate(judge_prompt, max_tokens=5)
    
    return "A" if "A" in output else "B"
```

**ì¥ì :** ë¬´ë£Œ, ë¹ ë¦„  
**ë‹¨ì :** í’ˆì§ˆ ë‚®ì„ ìˆ˜ ìˆìŒ

---

## ë²¤ì¹˜ë§ˆí¬

### Google ì—°êµ¬ (2023)

**ê²°ê³¼: RLAIF â‰ˆ RLHF**

| ë©”íŠ¸ë¦­ | Human Feedback | AI Feedback |
|--------|---------------|-------------|
| Win Rate | 50% | 49.8% |
| Helpfulness | 7.8/10 | 7.7/10 |
| Harmlessness | 8.5/10 | 8.6/10 |
| Cost | $50K | $500 |

**AI feedbackìœ¼ë¡œ ì¶©ë¶„!**

### Claude (Constitutional AI)

| ëª¨ë¸ | Method | Harmlessness | Helpfulness |
|------|--------|--------------|-------------|
| Claude 1 | RLHF | 75% | 82% |
| **Claude 2** | **Constitutional AI** | **95%** | **88%** |

**Constitutional AIê°€ ë” ë‚˜ìŒ!**

---

## ë¹„ìš© ë¹„êµ

### RLHF (Human)

```
ë¼ë²¨ëŸ¬: 10ëª…
ì‹œê°„: 40ì‹œê°„/ì£¼, 4ì£¼
ë¹„ìš©: $25/hour

ì´: 10 Ã— 40 Ã— 4 Ã— $25 = $40,000

+ ë°ì´í„° ìˆ˜ì§‘ í”Œë«í¼: $10,000

= $50,000
```

### RLAIF (AI)

```
API ë¹„ìš©:
- 100K comparisons
- $0.03 per comparison (GPT-4)

ì´: 100K Ã— $0.03 = $3,000

+ ê°œë°œ ì‹œê°„: $2,000

= $5,000

(10ë°° ì €ë ´!)
```

---

## í•œê³„ì™€ í•´ê²°

### 1. AI Judge í¸í–¥

**ë¬¸ì œ:** AIë„ í¸í–¥ ìˆìŒ

```python
# GPT-4ëŠ” verbose ì„ í˜¸
Response A: "The capital is Paris."
Response B: "The capital of France, located in the northern part..."
GPT-4 prefers: B  (ê¸¸ì´ í¸í–¥!)
```

**í•´ê²°:**
```python
# Multiple judges
judgments = [
    gpt4_judge(A, B),
    claude_judge(A, B),
    llama_judge(A, B)
]

# Majority vote
final = max(set(judgments), key=judgments.count)
```

### 2. Self-preference Bias

**ë¬¸ì œ:** ëª¨ë¸ì´ ìê¸° ì¶œë ¥ ì„ í˜¸

```python
# Llamaê°€ Llama ì¶œë ¥ ì„ í˜¸
llama_output vs gpt_output
â†’ Llama judge â†’ Llama wins (í¸í–¥!)
```

**í•´ê²°:**
```python
# Blind evaluation (ì¶œì²˜ ìˆ¨ê¹€)
# Cross-evaluation (ë‹¤ë¥¸ ëª¨ë¸ì´ í‰ê°€)
```

### 3. Reward Hacking

**ë¬¸ì œ:** AI judge ì†ì´ê¸°

```python
# AIê°€ "helpful" í‚¤ì›Œë“œ ì„ í˜¸ ë°œê²¬
Model learns: "I'm happy to help! ..."
(ë‚´ìš© ê´€ê³„ì—†ì´)
```

**í•´ê²°:**
```python
# Diverse judges
# Adversarial testing
# Human spot-check
```

---

## ê³ ê¸‰ ê¸°ë²•

### 1. Iterative RLAIF

```python
# Round 1
preference_v1 = generate_with_gpt4()
model_v1 = train(preference_v1)

# Round 2 (ëª¨ë¸ ê°œì„ )
preference_v2 = generate_with_model_v1()  # Self-improvement
model_v2 = train(preference_v2)

# Round 3
preference_v3 = generate_with_ensemble([gpt4, claude, model_v2])
model_v3 = train(preference_v3)
```

### 2. Hierarchical Feedback

```python
# Multi-level constitution
LEVEL_1 = ["Safety first"]
LEVEL_2 = ["Helpfulness", "Clarity"]
LEVEL_3 = ["Style", "Tone"]

# Sequential evaluation
score = 0
if passes(LEVEL_1):
    score += evaluate(LEVEL_2)
    if passes(LEVEL_2):
        score += evaluate(LEVEL_3)
```

### 3. Synthetic Data Augmentation

```python
# AIê°€ ë°ì´í„° ìƒì„±
prompts = gpt4.generate_diverse_prompts(num=10000)

# AIê°€ ë‹µë³€ ìƒì„±
for prompt in prompts:
    good_response = gpt4.generate(prompt, temperature=0.7, principle="helpful")
    bad_response = gpt4.generate(prompt, temperature=0.9, principle="harmful")
    
    preference_data.append({
        "prompt": prompt,
        "chosen": good_response,
        "rejected": bad_response
    })
```

---

## ì‹¤ì „ íŒ

### 1. Constitution ì„¤ê³„

```python
# ì¢‹ì€ constitution
GOOD = [
    "Provide accurate, factual information",  # êµ¬ì²´ì 
    "Acknowledge when uncertain",  # ì¸¡ì • ê°€ëŠ¥
    "Use clear, simple language"  # ëª…í™•
]

# ë‚˜ìœ constitution
BAD = [
    "Be good",  # ë„ˆë¬´ ëª¨í˜¸
    "Don't be bad",  # ë¶€ì •í˜•
    "Make everyone happy"  # ë¶ˆê°€ëŠ¥
]
```

### 2. Judge Calibration

```python
# Human baselineê³¼ ë¹„êµ
human_prefs = load_human_preferences()
ai_prefs = generate_ai_preferences()

# Agreement rate
agreement = (human_prefs == ai_prefs).mean()

if agreement < 0.7:
    # AI judge ê°œì„  í•„ìš”
    calibrate_judge()
```

### 3. Cost ìµœì í™”

```python
# Cascade evaluation
def cascade_judge(A, B):
    # 1. Cheap model first
    cheap_result = llama_judge(A, B)
    confidence = cheap_result['confidence']
    
    if confidence > 0.9:
        return cheap_result['preference']
    
    # 2. Expensive model if uncertain
    return gpt4_judge(A, B)

# 90% cases: cheap
# 10% cases: expensive
# â†’ 10ë°° cost reduction
```

---

## ìš”ì•½

**RLAIF**ëŠ”:

1. **AIê°€ í”¼ë“œë°±**: ì¸ê°„ ë¶ˆí•„ìš”
2. **í™•ì¥ ê°€ëŠ¥**: ë¬´ì œí•œ ë°ì´í„°
3. **ì €ë ´**: 10ë°° ì´ìƒ ($50K â†’ $5K)
4. **ë¹ ë¦„**: API í˜¸ì¶œ

**í•µì‹¬:**
- Constitutional AI (Claude)
- AI judge (GPT-4, Claude)
- Self-improvement

**ì„±ëŠ¥:**
- RLAIF â‰ˆ RLHF (Google ì—°êµ¬)
- Constitutional AI > RLHF (Claude)

**í•œê³„:**
- AI judge í¸í–¥
- Self-preference bias
- Reward hacking

**í•´ê²°:**
- Multiple judges
- Blind evaluation
- Human spot-check

---

## RLHF ì‹œë¦¬ì¦ˆ ì™„ê²°! ğŸ‰

**Post-training ì™„ì „ ì •ë³µ (1-4í¸):**

1. **RLHF**: ì¸ê°„ í”¼ë“œë°± (í‘œì¤€)
2. **DPO**: Reward model ë¶ˆí•„ìš”
3. **GRPO**: Group baseline (íš¨ìœ¨)
4. **RLAIF**: AI í”¼ë“œë°± (í™•ì¥)

**ë¹„êµ:**

| ë°©ë²• | ë¹„ìš© | ì‹œê°„ | ìƒ˜í”Œ íš¨ìœ¨ | ì„±ëŠ¥ |
|------|------|------|----------|------|
| RLHF | $$$$$ | ëŠë¦¼ | ë³´í†µ | â­â­â­â­â­ |
| DPO | $$$ | ë¹ ë¦„ | ë³´í†µ | â­â­â­â­ |
| GRPO | $$$$ | ë¹ ë¦„ | ë†’ìŒ | â­â­â­â­â­ |
| **RLAIF** | **$** | **ë§¤ìš° ë¹ ë¦„** | **ë†’ìŒ** | â­â­â­â­â­ |

**ì¶”ì²œ:**
- ë¦¬ì†ŒìŠ¤ í’ë¶€: RLHF or GRPO
- ë¹ ë¥´ê²Œ ì‹œì‘: DPO
- í™•ì¥ í•„ìš”: **RLAIF** âœ…

---

## ë‹¤ìŒ ì‹œë¦¬ì¦ˆ

**System Design** - ëŒ€ê·œëª¨ ì‹œìŠ¤í…œ ì„¤ê³„!

ê¸°ëŒ€í•´ì£¼ì„¸ìš”! ğŸš€

---

*ì§ˆë¬¸ì´ë‚˜ í”¼ë“œë°±ì€ [GitHub](https://github.com/yhna941)ì—ì„œ í™˜ì˜í•©ë‹ˆë‹¤!*
