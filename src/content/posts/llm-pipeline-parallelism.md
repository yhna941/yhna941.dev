---
title: "LLM Inference ìµœì í™” #9: Pipeline Parallelism - ë ˆì´ì–´ë¥¼ íŒŒì´í”„ë¼ì¸ì²˜ëŸ¼ í˜ë ¤ë³´ë‚´ê¸°"
description: "ì—¬ëŸ¬ GPUì— ë ˆì´ì–´ë¥¼ ìˆœì°¨ ë°°ì¹˜í•˜ê³  micro-batchingìœ¼ë¡œ íš¨ìœ¨ì„ ê·¹ëŒ€í™”í•˜ëŠ” Pipeline Parallelismì„ ì•Œì•„ë´…ë‹ˆë‹¤."
pubDate: 2026-02-06
author: "Yh Na"
tags: ["llm", "optimization", "parallelism", "pipeline", "distributed"]
draft: false
---

# LLM Inference ìµœì í™” #9: Pipeline Parallelism

Tensor Parallelismì€ í†µì‹ ì´ ë§ìŠµë‹ˆë‹¤. ë ˆì´ì–´ë§ˆë‹¤ All-reduceê°€ í•„ìš”í•˜ì£ .

**Pipeline Parallelism**ì€ ë‹¤ë¥¸ ì ‘ê·¼ì…ë‹ˆë‹¤:
- **ë ˆì´ì–´ë¥¼ GPUì— ìˆœì°¨ ë°°ì¹˜**
- **Activationë§Œ GPU ê°„ ì „ì†¡**
- **Micro-batchingìœ¼ë¡œ ë³‘ë ¬í™”**
- **í†µì‹ : ë ˆì´ì–´ë‹¹ 1ë²ˆ**

ê²°ê³¼:
- í†µì‹ ëŸ‰: Tensor Parallelì˜ 1/10
- GPU í™œìš©ë¥ : 50-80% (bubble ìµœì†Œí™” í•„ìš”)
- êµ¬í˜„: ë¹„êµì  ê°„ë‹¨

---

## ë¬¸ì œ: Naive Pipelineì˜ Bubble

### Naive ë°©ì‹

```
GPU 0: [Layer 1-8]
GPU 1: [Layer 9-16]
GPU 2: [Layer 17-24]
GPU 3: [Layer 25-32]

Timeline:
GPU 0: [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ]â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘
GPU 1: â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘[â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ]â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘
GPU 2: â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘[â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ]â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘
GPU 3: â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘[â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ]

Bubble: 75% idle time!
```

**ë¬¸ì œ:**
- GPUê°€ ìˆœì°¨ì ìœ¼ë¡œë§Œ ë™ì‘
- 3ê°œ GPUëŠ” ë†€ê³  ìˆìŒ
- í™œìš©ë¥ : 25%

---

## í•´ê²°ì±…: Micro-batching

### í•µì‹¬ ì•„ì´ë””ì–´

> **ë°°ì¹˜ë¥¼ ì‘ì€ micro-batchë¡œ ë‚˜ëˆ„ê³ , íŒŒì´í”„ë¼ì¸ì²˜ëŸ¼ í˜ë ¤ë³´ë‚¸ë‹¤**

```
Batch 32 â†’ 4 micro-batches (size 8 each)

Timeline:
GPU 0: [MB1][MB2][MB3][MB4]
GPU 1:     [MB1][MB2][MB3][MB4]
GPU 2:         [MB1][MB2][MB3][MB4]
GPU 3:             [MB1][MB2][MB3][MB4]

Bubble reduced!
```

### ì˜ˆì‹œ (4 GPUs, 4 micro-batches)

```
Time:  0  1  2  3  4  5  6  7
GPU 0: 1  2  3  4  .  .  .  .
GPU 1: .  1  2  3  4  .  .  .
GPU 2: .  .  1  2  3  4  .  .
GPU 3: .  .  .  1  2  3  4  .

Legend:
  1,2,3,4: Micro-batch ID
  .: Bubble (idle)

Bubble: 3/(7) = 43% (better!)
```

### Bubble ê³„ì‚°

```
num_stages = 4 (GPUs)
num_microbatches = m

Bubble fraction = (num_stages - 1) / (m + num_stages - 1)

m=1:  3/4 = 75%
m=4:  3/7 = 43%
m=8:  3/11 = 27%
m=16: 3/19 = 16%
```

**mì„ ëŠ˜ë¦¬ë©´ bubble ê°ì†Œ!**

---

## GPipe Schedule

### 1F1B (One Forward One Backward)

íš¨ìœ¨ì ì¸ ìŠ¤ì¼€ì¤„:

```python
# GPipe schedule
def gpipe_schedule(num_stages, num_microbatches):
    schedule = []
    
    # Warmup: Fill pipeline (forward only)
    for i in range(num_stages - 1):
        schedule.append(('F', i))  # Forward micro-batch i
    
    # Steady: 1F1B (one forward, one backward)
    for i in range(num_microbatches - num_stages + 1):
        schedule.append(('F', i + num_stages - 1))
        schedule.append(('B', i))
    
    # Cooldown: Empty pipeline (backward only)
    for i in range(num_stages - 1):
        schedule.append(('B', num_microbatches - num_stages + 1 + i))
    
    return schedule

# Example: 4 stages, 8 micro-batches
schedule = gpipe_schedule(4, 8)
# Stage 0: F0 F1 F2 F3 B0 F4 B1 F5 B2 F6 B3 F7 B4 B5 B6 B7
# Stage 1:    F0 F1 F2 B0 F3 B1 F4 B2 F5 B3 F6 B4 F7 B5 B6 B7
# ...
```

### ë©”ëª¨ë¦¬ íš¨ìœ¨

**GPipe:**
- Forward passì˜ activationì„ ëª¨ë‘ ì €ì¥ (backward ìœ„í•´)
- ë©”ëª¨ë¦¬: O(num_microbatches Ã— layers_per_stage)

**ë¬¸ì œ:** mì´ í¬ë©´ ë©”ëª¨ë¦¬ ë¶€ì¡±!

---

## PipeDream-Flush (Improved)

### ê°œì„ ëœ ìŠ¤ì¼€ì¤„

```python
def pipedream_flush_schedule(num_stages, num_microbatches):
    """
    ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ìŠ¤ì¼€ì¤„
    Forwardì™€ Backwardë¥¼ ë¹¨ë¦¬ ì—°ê²°
    """
    schedule = []
    in_flight = 0  # Forward done, backward not done
    
    for step in range(num_microbatches + num_stages - 1):
        # Forward if possible
        if step < num_microbatches:
            schedule.append(('F', step))
            in_flight += 1
        
        # Backward if possible
        backward_idx = step - (num_stages - 1)
        if backward_idx >= 0:
            schedule.append(('B', backward_idx))
            in_flight -= 1
    
    return schedule
```

**íŠ¹ì§•:**
- Activationì„ ë¹¨ë¦¬ í•´ì œ (ë©”ëª¨ë¦¬ â†“)
- Bubbleì€ ë¹„ìŠ·

---

## êµ¬í˜„

### 1. ê¸°ë³¸ Pipeline Stage

```python
import torch
import torch.distributed.rpc as rpc

class PipelineStage(nn.Module):
    def __init__(self, layers, stage_id, num_stages):
        super().__init__()
        self.layers = nn.ModuleList(layers)
        self.stage_id = stage_id
        self.num_stages = num_stages
        self.is_first = (stage_id == 0)
        self.is_last = (stage_id == num_stages - 1)
    
    def forward(self, x):
        """ë‹¨ì¼ micro-batch forward"""
        for layer in self.layers:
            x = layer(x)
        return x
    
    def forward_microbatch(self, micro_batch):
        """Micro-batch ì²˜ë¦¬"""
        # Forward
        output = self.forward(micro_batch)
        
        # ë‹¤ìŒ stageë¡œ ì „ì†¡
        if not self.is_last:
            next_stage = self.stage_id + 1
            rpc.rpc_async(
                f"worker{next_stage}",
                PipelineStage.forward_microbatch,
                args=(output,)
            )
        
        return output


def create_pipeline(model, num_stages):
    """ëª¨ë¸ì„ stagesë¡œ ë¶„í• """
    layers = list(model.children())
    layers_per_stage = len(layers) // num_stages
    
    stages = []
    for i in range(num_stages):
        start_idx = i * layers_per_stage
        end_idx = start_idx + layers_per_stage if i < num_stages - 1 else len(layers)
        stage_layers = layers[start_idx:end_idx]
        stages.append(PipelineStage(stage_layers, i, num_stages))
    
    return stages
```

### 2. Micro-batch ì²˜ë¦¬

```python
class PipelineParallelEngine:
    def __init__(self, model, num_stages, num_microbatches):
        self.stages = create_pipeline(model, num_stages)
        self.num_microbatches = num_microbatches
        self.stage_id = dist.get_rank()
    
    def split_batch(self, batch, num_splits):
        """ë°°ì¹˜ë¥¼ micro-batchesë¡œ ë¶„í• """
        batch_size = batch.size(0)
        micro_batch_size = batch_size // num_splits
        
        micro_batches = []
        for i in range(num_splits):
            start = i * micro_batch_size
            end = start + micro_batch_size
            micro_batches.append(batch[start:end])
        
        return micro_batches
    
    def forward(self, batch):
        """Pipeline forward pass"""
        micro_batches = self.split_batch(batch, self.num_microbatches)
        
        # í˜„ì¬ stage
        stage = self.stages[self.stage_id]
        
        # Activation ì €ì¥ (backward ìœ„í•´)
        activations = []
        
        # GPipe schedule ì‹¤í–‰
        schedule = gpipe_schedule(len(self.stages), self.num_microbatches)
        
        for action, mb_idx in schedule:
            if action == 'F':
                # Forward
                if self.stage_id == 0:
                    # First stage: Use input
                    input_mb = micro_batches[mb_idx]
                else:
                    # Receive from previous stage
                    input_mb = self.recv_activation()
                
                output_mb = stage(input_mb)
                activations.append(output_mb)
                
                if self.stage_id < len(self.stages) - 1:
                    # Send to next stage
                    self.send_activation(output_mb)
            
            elif action == 'B':
                # Backward
                if self.stage_id == len(self.stages) - 1:
                    # Last stage: Compute loss gradient
                    grad_output = self.compute_loss_gradient(activations[mb_idx])
                else:
                    # Receive gradient from next stage
                    grad_output = self.recv_gradient()
                
                # Backward pass
                activation = activations[mb_idx]
                activation.backward(grad_output)
                
                if self.stage_id > 0:
                    # Send gradient to previous stage
                    self.send_gradient(activation.grad)
        
        # Aggregate results
        if self.stage_id == len(self.stages) - 1:
            outputs = torch.cat([a.detach() for a in activations], dim=0)
            return outputs
    
    def send_activation(self, tensor):
        """ë‹¤ìŒ stageë¡œ activation ì „ì†¡"""
        next_rank = self.stage_id + 1
        dist.send(tensor, dst=next_rank)
    
    def recv_activation(self):
        """ì´ì „ stageì—ì„œ activation ìˆ˜ì‹ """
        prev_rank = self.stage_id - 1
        tensor = torch.empty_like(...)  # Shape must be known
        dist.recv(tensor, src=prev_rank)
        return tensor
    
    def send_gradient(self, tensor):
        """ì´ì „ stageë¡œ gradient ì „ì†¡"""
        prev_rank = self.stage_id - 1
        dist.send(tensor, dst=prev_rank)
    
    def recv_gradient(self):
        """ë‹¤ìŒ stageì—ì„œ gradient ìˆ˜ì‹ """
        next_rank = self.stage_id + 1
        tensor = torch.empty_like(...)
        dist.recv(tensor, src=next_rank)
        return tensor
```

### 3. DeepSpeed Pipeline

```python
from deepspeed.pipe import PipelineModule, LayerSpec

# ëª¨ë¸ ì •ì˜ (layers as specs)
layers = [
    LayerSpec(nn.Linear, 768, 3072),
    LayerSpec(nn.GELU),
    LayerSpec(nn.Linear, 3072, 768),
    # ... repeat 32 times
]

# Pipeline ìƒì„±
model = PipelineModule(
    layers=layers,
    num_stages=4,  # 4 GPUs
    partition_method='uniform'  # or 'parameters'
)

# DeepSpeed ì´ˆê¸°í™”
engine, optimizer, _, _ = deepspeed.initialize(
    model=model,
    config={
        "train_batch_size": 32,
        "train_micro_batch_size_per_gpu": 8,
        "gradient_accumulation_steps": 1,
        "pipeline": {
            "pipe_partitioned": True,
            "grad_partitioned": True
        }
    }
)

# í•™ìŠµ
for batch in dataloader:
    loss = engine.train_batch(batch)
```

---

## Interleaved Pipeline (GPipe-1F1B)

### ë¬¸ì œ: Bubbleì´ ì—¬ì „íˆ í¼

```
4 stages, 8 micro-batches:
Bubble = 3/11 = 27%
```

### í•´ê²°ì±…: Interleaving

ê° GPUê°€ ì—¬ëŸ¬ stageë¥¼ ë‹´ë‹¹:

```
# Standard
GPU 0: Layers 1-8
GPU 1: Layers 9-16
GPU 2: Layers 17-24
GPU 3: Layers 25-32

# Interleaved (2-way)
GPU 0: Layers 1-4, 17-20
GPU 1: Layers 5-8, 21-24
GPU 2: Layers 9-12, 25-28
GPU 3: Layers 13-16, 29-32
```

**íš¨ê³¼:**
- Bubble ê°ì†Œ
- ë©”ëª¨ë¦¬ ì•½ê°„ ì¦ê°€

```python
def create_interleaved_pipeline(model, num_stages, num_model_chunks):
    """
    Interleaved pipeline ìƒì„±
    
    Args:
        num_stages: GPU ê°œìˆ˜
        num_model_chunks: ê° GPUê°€ ë‹´ë‹¹í•˜ëŠ” chunk ìˆ˜
    """
    layers = list(model.children())
    total_chunks = num_stages * num_model_chunks
    layers_per_chunk = len(layers) // total_chunks
    
    stage_layers = [[] for _ in range(num_stages)]
    
    for chunk_id in range(total_chunks):
        stage_id = chunk_id % num_stages
        start_idx = chunk_id * layers_per_chunk
        end_idx = start_idx + layers_per_chunk
        stage_layers[stage_id].extend(layers[start_idx:end_idx])
    
    return stage_layers
```

---

## ë©”ëª¨ë¦¬ ê´€ë¦¬

### Activation Checkpointing

ë©”ëª¨ë¦¬ ì¤„ì´ê¸°:

```python
class CheckpointedLayer(nn.Module):
    def __init__(self, layer):
        super().__init__()
        self.layer = layer
    
    def forward(self, x):
        # Activationì„ ì €ì¥ ì•ˆ í•¨
        # Backward ì‹œ recompute
        return torch.utils.checkpoint.checkpoint(self.layer, x)
```

**Trade-off:**
- ë©”ëª¨ë¦¬: â†“â†“
- ê³„ì‚°: â†‘ (recompute)
- ì „ì²´ ì†ë„: ë¹„ìŠ· (ë©”ëª¨ë¦¬ ë³‘ëª© í•´ì†Œ)

### Selective Checkpointing

ì¼ë¶€ë§Œ checkpoint:

```python
def create_checkpointed_pipeline(layers, checkpoint_every=4):
    """Nê°œ layerë§ˆë‹¤ checkpoint"""
    wrapped = []
    for i, layer in enumerate(layers):
        if i % checkpoint_every == 0:
            wrapped.append(CheckpointedLayer(layer))
        else:
            wrapped.append(layer)
    return wrapped
```

---

## í†µì‹  ìµœì í™”

### 1. Tensor Compression

Activation ì••ì¶•:

```python
def compress_activation(tensor, bits=8):
    """8-bitë¡œ ì••ì¶•í•´ì„œ ì „ì†¡"""
    # Quantize
    scale = tensor.abs().max() / 127
    quantized = (tensor / scale).round().to(torch.int8)
    
    # Send quantized + scale
    return quantized, scale

def decompress_activation(quantized, scale):
    """ë³µì›"""
    return quantized.float() * scale
```

### 2. Overlapping

í†µì‹ ê³¼ ê³„ì‚° ê²¹ì¹˜ê¸°:

```python
def overlapped_forward(stage, input_tensor):
    """
    ê³„ì‚°ê³¼ í†µì‹  ë™ì‹œ ì§„í–‰
    """
    # 1. ì´ì „ stageì—ì„œ ìˆ˜ì‹  ì‹œì‘ (ë¹„ë™ê¸°)
    recv_handle = dist.irecv(input_tensor, src=stage.prev_rank, async_op=True)
    
    # 2. ë‹¤ë¥¸ ì‘ì—… (ì˜ˆ: normalization)
    # ...
    
    # 3. ìˆ˜ì‹  ì™„ë£Œ ëŒ€ê¸°
    recv_handle.wait()
    
    # 4. Forward
    output = stage(input_tensor)
    
    # 5. ë‹¤ìŒ stageë¡œ ì „ì†¡ ì‹œì‘ (ë¹„ë™ê¸°)
    send_handle = dist.isend(output, dst=stage.next_rank, async_op=True)
    
    # 6. ë‹¤ë¥¸ ì‘ì—…
    # ...
    
    # 7. ì „ì†¡ ì™„ë£Œ ëŒ€ê¸°
    send_handle.wait()
    
    return output
```

---

## Hybrid: Pipeline + Tensor Parallelism

ìµœê³ ì˜ ì„±ëŠ¥:

```
16 GPUs = 4 pipeline stages Ã— 4 tensor parallel per stage

GPU  0,1,2,3:  Layers 1-8   (TP=4)
GPU  4,5,6,7:  Layers 9-16  (TP=4)
GPU 8,9,10,11: Layers 17-24 (TP=4)
GPU 12,13,14,15: Layers 25-32 (TP=4)
```

**ì¥ì :**
- Pipeline: í†µì‹  ì ìŒ
- Tensor Parallel: Bubble ì ìŒ
- ìµœê³  íš¨ìœ¨!

```python
# DeepSpeed 3D parallelism
config = {
    "pipeline": {
        "stages": 4,
        "micro_batches": 16
    },
    "tensor_parallel": {
        "size": 4
    },
    "data_parallel": {
        "size": 2
    }
}

# ì´ GPU: 4 Ã— 4 Ã— 2 = 32
```

---

## ë²¤ì¹˜ë§ˆí¬

### GPT-3 175B, 64 GPUs

| ë°©ì‹ | ì²˜ë¦¬ëŸ‰ (samples/s) | GPU íš¨ìœ¨ |
|------|-------------------|---------|
| Tensor Parallel (64) | 85 | 60% |
| Pipeline (8 stages) | 120 | 45% |
| **Hybrid (PP=8, TP=8)** | **280** | **75%** |

### Bubble ë¹„êµ

| Schedule | Bubble | ë©”ëª¨ë¦¬ |
|----------|--------|--------|
| Naive | 75% | ë‚®ìŒ |
| GPipe | 27% (m=8) | ë†’ìŒ |
| PipeDream-Flush | 27% | ì¤‘ê°„ |
| Interleaved | 15% | ë†’ìŒ |

---

## ì‹¤ì „ ì˜ˆì œ

### Megatron-LM Style

```python
from megatron import get_args
from megatron.model import GPTModel
from megatron.training import train

# ì„¤ì •
args = get_args()
args.pipeline_model_parallel_size = 4
args.tensor_model_parallel_size = 2
args.micro_batch_size = 4
args.global_batch_size = 32

# ëª¨ë¸
model = GPTModel(
    num_layers=32,
    hidden_size=4096,
    num_attention_heads=32,
    vocab_size=50257,
    max_position_embeddings=2048
)

# í•™ìŠµ
train(model)
```

### Fairscale

```python
from fairscale.nn import Pipe

# ëª¨ë¸ â†’ Sequential
layers = [
    nn.Linear(768, 3072),
    nn.GELU(),
    nn.Linear(3072, 768),
    # ... 32 times
]

model = nn.Sequential(*layers)

# Pipeline wrapping
model = Pipe(
    model,
    balance=[8, 8, 8, 8],  # Layers per GPU
    chunks=8,              # Micro-batches
    checkpoint='except_last'  # Activation checkpointing
)

# ì‚¬ìš©
outputs = model(inputs)
```

---

## Best Practices

### 1. Micro-batch í¬ê¸° ì„ íƒ

```python
# Rule of thumb
num_microbatches = 4 Ã— num_pipeline_stages

# ì˜ˆ: 8 stages â†’ 32 micro-batches
```

### 2. Balance ì¡°ì •

```python
# ê° stageê°€ ë¹„ìŠ·í•œ ì‹œê°„ ì†Œìš”í•˜ë„ë¡
balance = [7, 8, 9, 8]  # Layer counts

# ìë™ balancing
balance = auto_balance(model, num_stages, profile=True)
```

### 3. Checkpoint ì „ëµ

```python
# í° ë ˆì´ì–´ëŠ” checkpoint
if layer.num_parameters() > threshold:
    layer = CheckpointedLayer(layer)
```

---

## ìš”ì•½

**Pipeline Parallelism**ì€:

1. **ë ˆì´ì–´ë¥¼ GPUì— ìˆœì°¨ ë°°ì¹˜**
2. **Micro-batching**ìœ¼ë¡œ ë³‘ë ¬í™”
3. **í†µì‹ : Activationë§Œ** (Tensor Parallelë³´ë‹¤ ì ìŒ)
4. **Bubble: 15-30%** (ìµœì í™” ì‹œ)
5. **êµ¬í˜„: ë¹„êµì  ê°„ë‹¨**

**í•µì‹¬ ê¸°ë²•:**
- GPipe schedule (1F1B)
- Interleaved pipeline
- Activation checkpointing
- Hybrid (PP + TP)

**ì‚¬ìš©ì²˜:**
- Large model (70B+)
- í†µì‹  ëŒ€ì—­í­ ë‚®ì„ ë•Œ
- Tensor Parallelê³¼ ì¡°í•©

---

## ë‹¤ìŒ ê¸€

**13í¸: Model Compression**
- Pruning (ê°€ì§€ì¹˜ê¸°)
- Distillation (ì§€ì‹ ì¦ë¥˜)
- ì •í™•ë„ ìœ ì§€í•˜ë©° í¬ê¸° ì¤„ì´ê¸°

ê¸°ëŒ€í•´ì£¼ì„¸ìš”! ğŸš€

---

*ì§ˆë¬¸ì´ë‚˜ í”¼ë“œë°±ì€ [GitHub](https://github.com/yhna941)ì—ì„œ í™˜ì˜í•©ë‹ˆë‹¤!*
