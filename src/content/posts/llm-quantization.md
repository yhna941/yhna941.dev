---
title: "LLM Inference ìµœì í™” #7: Model Quantization - ë©”ëª¨ë¦¬ë¥¼ 4ë°° ì¤„ì´ê³  ì†ë„ëŠ” 2ë°° ì˜¬ë¦¬ê¸°"
description: "INT8/INT4 ì–‘ìí™”ë¡œ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì„ íš¨ìœ¨ì ìœ¼ë¡œ ì‹¤í–‰í•˜ëŠ” GPTQ, AWQ, QLoRAì˜ ì›ë¦¬ì™€ ì‹¤ì „ì„ ì•Œì•„ë´…ë‹ˆë‹¤."
pubDate: 2026-02-06
author: "Yh Na"
tags: ["llm", "optimization", "quantization", "gptq", "awq", "qlora"]
draft: false
---

# LLM Inference ìµœì í™” #7: Model Quantization

LLaMA-70BëŠ” **140GB**ì˜ ë©”ëª¨ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤ (FP16). A100 80GBë¡œë„ ëª¨ìëë‹ˆë‹¤.

**Quantization**ì€ ì´ë¥¼ í•´ê²°í•©ë‹ˆë‹¤:
- **INT8**: 70GB (2ë°° ì ˆê°)
- **INT4**: 35GB (4ë°° ì ˆê°)
- **ì†ë„**: 2-3ë°° í–¥ìƒ
- **ì •í™•ë„**: 1-2% ì†ì‹¤

RTX 4090 24GBë¡œë„ 70B ëª¨ë¸ì„ ëŒë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤!

---

## ì–‘ìí™”ë€?

### ê¸°ë³¸ ê°œë…

**Float16 (16-bit):**
```
ë²”ìœ„: -65,504 ~ 65,504
ì •ë°€ë„: ~3 decimal places
ë©”ëª¨ë¦¬: 2 bytes
```

**INT8 (8-bit):**
```
ë²”ìœ„: -128 ~ 127
ì •ë°€ë„: integer only
ë©”ëª¨ë¦¬: 1 byte (50% ì ˆê°)
```

**INT4 (4-bit):**
```
ë²”ìœ„: -8 ~ 7
ì •ë°€ë„: integer only
ë©”ëª¨ë¦¬: 0.5 byte (75% ì ˆê°)
```

### ë³€í™˜ ê³¼ì •

**Quantization:**
```python
# FP16 â†’ INT8
def quantize(weight_fp16, scale):
    return round(weight_fp16 / scale).clamp(-128, 127)

# Example
weight = 0.523  # FP16
scale = 0.01
quantized = round(0.523 / 0.01) = 52  # INT8
```

**Dequantization (ë³µì›):**
```python
def dequantize(weight_int8, scale):
    return weight_int8 * scale

# Example
dequantized = 52 * 0.01 = 0.52  # ~0.523 (ì•½ê°„ ì†ì‹¤)
```

---

## Symmetric vs Asymmetric

### Symmetric Quantization

**ë²”ìœ„ê°€ ëŒ€ì¹­:**
```
FP16: [-1.0, 1.0]
INT8: [-128, 127]

scale = max(abs(W)) / 127
Q(w) = round(w / scale)
```

**ì¥ì :** ê°„ë‹¨, ë¹ ë¦„  
**ë‹¨ì :** ë²”ìœ„ ë‚­ë¹„ (ìŒìˆ˜/ì–‘ìˆ˜ ë¶ˆê· í˜• ì‹œ)

### Asymmetric Quantization

**Zero-point ì¶”ê°€:**
```
FP16: [0.2, 1.8]  â† ìŒìˆ˜ ì—†ìŒ
INT8: [-128, 127]

scale = (max - min) / 255
zero_point = -round(min / scale)

Q(w) = round(w / scale) + zero_point
```

**ì¥ì :** ë²”ìœ„ ìµœëŒ€ í™œìš©  
**ë‹¨ì :** ê³„ì‚° ë³µì¡

---

## Post-Training Quantization (PTQ)

í•™ìŠµ ì—†ì´ ì–‘ìí™”í•©ë‹ˆë‹¤.

### 1. Naive Quantization

ê°€ì¥ ê°„ë‹¨í•œ ë°©ë²•:

```python
import torch

def naive_quantize(model):
    """ëª¨ë“  ê°€ì¤‘ì¹˜ë¥¼ INT8ë¡œ"""
    for name, param in model.named_parameters():
        if 'weight' in name:
            # Scale ê³„ì‚°
            scale = param.abs().max() / 127
            
            # ì–‘ìí™”
            quantized = torch.round(param / scale).clamp(-128, 127).to(torch.int8)
            
            # ì €ì¥
            setattr(model, f'{name}_scale', scale)
            setattr(model, f'{name}_quantized', quantized)
    
    return model

# ì¶”ë¡  ì‹œ
def forward_quantized(x, weight_quantized, scale):
    # Dequantize
    weight = weight_quantized.float() * scale
    
    # ê³„ì‚°
    return x @ weight.T
```

**ë¬¸ì œ:** ì •í™•ë„ í¬ê²Œ í•˜ë½ (5-10%)

### 2. Calibration-based

ëŒ€í‘œ ë°ì´í„°ë¡œ í†µê³„ ìˆ˜ì§‘:

```python
def calibrate_quantization(model, calibration_data):
    """ë°ì´í„°ë¡œ ìµœì  scale ì°¾ê¸°"""
    activations = {}
    
    # Forward passë¡œ activation ìˆ˜ì§‘
    def hook(module, input, output):
        activations[module] = output.detach()
    
    hooks = []
    for module in model.modules():
        if isinstance(module, torch.nn.Linear):
            hooks.append(module.register_forward_hook(hook))
    
    # Calibration
    with torch.no_grad():
        for batch in calibration_data:
            model(batch)
    
    # Remove hooks
    for hook in hooks:
        hook.remove()
    
    # Scale ê³„ì‚° (percentile ì‚¬ìš©)
    scales = {}
    for module, act in activations.items():
        # 99.9 percentileë¡œ outlier ì œê±°
        scale = torch.quantile(act.abs(), 0.999) / 127
        scales[module] = scale
    
    return scales
```

---

## Advanced Quantization Methods

### 1. GPTQ (GPT-Quantization)

**í•µì‹¬ ì•„ì´ë””ì–´:** Layer-wise quantization with Hessian

```python
# GPTQ ì•Œê³ ë¦¬ì¦˜ (simplified)
def gptq_quantize(model, calibration_data):
    for layer in model.layers:
        # 1. Hessian ê³„ì‚° (2nd order)
        H = compute_hessian(layer, calibration_data)
        
        # 2. Quantization error ìµœì†Œí™”
        for i in range(layer.weight.shape[0]):
            # Optimal rounding
            w = layer.weight[i]
            q = round(w / scale)
            error = w - q * scale
            
            # Errorë¥¼ ë‹¤ë¥¸ weightì— ë¶„ì‚°
            layer.weight[i+1:] -= H_inv @ error
```

**íŠ¹ì§•:**
- **ì •í™•ë„:** ë§¤ìš° ë†’ìŒ (< 1% ì†ì‹¤)
- **ì†ë„:** ëŠë¦¼ (Hessian ê³„ì‚°)
- **ë©”ëª¨ë¦¬:** INT4ê¹Œì§€ ê°€ëŠ¥

**ì‚¬ìš©:**
```python
from transformers import AutoModelForCausalLM, GPTQConfig

model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-2-7b-hf",
    quantization_config=GPTQConfig(bits=4, dataset="c4", group_size=128)
)
```

### 2. AWQ (Activation-aware Weight Quantization)

**í•µì‹¬ ì•„ì´ë””ì–´:** ì¤‘ìš”í•œ ê°€ì¤‘ì¹˜ëŠ” ì •ë°€í•˜ê²Œ

```python
def awq_quantize(weights, activations):
    # 1. Importance ê³„ì‚°
    importance = compute_importance(weights, activations)
    
    # 2. Top-këŠ” ë†’ì€ ì •ë°€ë„
    top_k_idx = importance.topk(k=int(0.01 * len(importance))).indices
    
    # 3. ë‚˜ë¨¸ì§€ë§Œ ì–‘ìí™”
    quantized = weights.clone()
    for i in range(len(weights)):
        if i not in top_k_idx:
            quantized[i] = quantize(weights[i], scale)
    
    return quantized
```

**Scaling factor:**
```python
# Channel-wise scaling
s = (weights.abs().max(dim=0) / activations.abs().max(dim=0)) ** 0.5

scaled_weights = weights / s
scaled_activations = activations * s

# ì´ì œ ì–‘ìí™”
quantized_weights = quantize(scaled_weights)
```

**íŠ¹ì§•:**
- **ì •í™•ë„:** GPTQì™€ ë¹„ìŠ·
- **ì†ë„:** ë§¤ìš° ë¹ ë¦„ (no Hessian)
- **ì¶”ë¡ :** ë¹ ë¦„ (scaleë§Œ)

**ì‚¬ìš©:**
```python
from awq import AutoAWQForCausalLM

model = AutoAWQForCausalLM.from_pretrained(
    "meta-llama/Llama-2-7b-hf"
)
model.quantize(tokenizer, quant_config={"bits": 4, "group_size": 128})
model.save_quantized("llama2-7b-awq")
```

### 3. QLoRA (Quantized LoRA)

**í•µì‹¬ ì•„ì´ë””ì–´:** 4-bit base + FP16 LoRA adapters

```python
# Base model: 4-bit
base_model = load_in_4bit(model_path)

# LoRA: FP16
lora_A = nn.Parameter(torch.zeros(rank, in_features, dtype=torch.float16))
lora_B = nn.Parameter(torch.zeros(out_features, rank, dtype=torch.float16))

# Forward
def forward(x):
    # Base (4-bit, frozen)
    base_out = base_model(x)  # Dequantize on-the-fly
    
    # LoRA (FP16, trainable)
    lora_out = (x @ lora_A.T) @ lora_B.T
    
    return base_out + lora_out
```

**íŠ¹ì§•:**
- **ë©”ëª¨ë¦¬:** ê·¹ë„ë¡œ ì ìŒ (70Bë„ 24GBì—ì„œ í•™ìŠµ!)
- **ì •í™•ë„:** Full fine-tuningê³¼ ë™ì¼
- **ì†ë„:** LoRA ë•ë¶„ì— ë¹ ë¦„

**ì‚¬ìš©:**
```python
from transformers import BitsAndBytesConfig
from peft import LoraConfig, get_peft_model

# 4-bit ë¡œë“œ
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True
)

model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-2-70b-hf",
    quantization_config=bnb_config
)

# LoRA ì ìš©
lora_config = LoraConfig(r=16, lora_alpha=32, target_modules=["q_proj", "v_proj"])
model = get_peft_model(model, lora_config)

# í•™ìŠµ
trainer.train()
```

---

## Quantization Granularity

### Per-tensor

ì „ì²´ í…ì„œì— í•˜ë‚˜ì˜ scale:

```python
scale = weight.abs().max() / 127
quantized = round(weight / scale)
```

**ì¥ì :** ê°„ë‹¨  
**ë‹¨ì :** Outlierì— ë¯¼ê°

### Per-channel

ì±„ë„(í–‰)ë§ˆë‹¤ ë‹¤ë¥¸ scale:

```python
# weight: [out_channels, in_channels]
scales = weight.abs().max(dim=1, keepdim=True) / 127
quantized = torch.round(weight / scales).clamp(-128, 127)
```

**ì¥ì :** ì •í™•ë„ â†‘  
**ë‹¨ì :** Scale ì €ì¥ ê³µê°„ â†‘

### Group-wise

ì±„ë„ì„ ê·¸ë£¹ìœ¼ë¡œ:

```python
group_size = 128
num_groups = in_channels // group_size

for g in range(num_groups):
    start = g * group_size
    end = start + group_size
    group = weight[:, start:end]
    
    scale = group.abs().max() / 127
    quantized[:, start:end] = round(group / scale)
```

**ì¥ì :** ì •í™•ë„ + íš¨ìœ¨ ë°¸ëŸ°ìŠ¤  
**ë‹¨ì :** ë³µì¡

---

## INT8 vs INT4

### INT8

**ì •í™•ë„:**
- Perplexity: < 1% ì¦ê°€
- ê±°ì˜ ë¬´ì†ì‹¤

**ì†ë„:**
- 1.5-2x ë¹ ë¦„

**ë©”ëª¨ë¦¬:**
- 2ë°° ì ˆê°

**ì§€ì›:**
- ê±°ì˜ ëª¨ë“  í•˜ë“œì›¨ì–´

### INT4

**ì •í™•ë„:**
- Perplexity: 1-2% ì¦ê°€
- GPTQ/AWQ ì‚¬ìš© ì‹œ < 1%

**ì†ë„:**
- 2-3x ë¹ ë¦„

**ë©”ëª¨ë¦¬:**
- 4ë°° ì ˆê°

**ì§€ì›:**
- ìµœì‹  GPU (Ampere+)

---

## ì‹¤ì „ êµ¬í˜„

### 1. bitsandbytes (QLoRA)

```python
from transformers import AutoModelForCausalLM, BitsAndBytesConfig

# 4-bit ì„¤ì •
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True,  # Nested quantization
    bnb_4bit_quant_type="nf4"  # Normal Float 4
)

model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-2-70b-hf",
    quantization_config=quantization_config,
    device_map="auto"
)

# ì¶”ë¡ 
inputs = tokenizer("Hello", return_tensors="pt").to("cuda")
outputs = model.generate(**inputs, max_new_tokens=100)
```

### 2. AutoGPTQ

```python
from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig

# Quantize
quantize_config = BaseQuantizeConfig(
    bits=4,
    group_size=128,
    desc_act=False  # Activation quantization
)

model = AutoGPTQForCausalLM.from_pretrained(
    "meta-llama/Llama-2-7b-hf",
    quantize_config
)

# Calibration
model.quantize(calibration_dataset)

# ì €ì¥
model.save_quantized("llama2-7b-gptq-4bit")

# ë¡œë“œ
model = AutoGPTQForCausalLM.from_quantized(
    "llama2-7b-gptq-4bit",
    device="cuda:0",
    use_safetensors=True
)
```

### 3. AutoAWQ

```python
from awq import AutoAWQForCausalLM

model = AutoAWQForCausalLM.from_pretrained("meta-llama/Llama-2-7b-hf")

# Quantize
model.quantize(
    tokenizer,
    quant_config={
        "zero_point": True,
        "q_group_size": 128,
        "w_bit": 4,
        "version": "GEMM"
    }
)

# ì €ì¥
model.save_quantized("llama2-7b-awq")
model.push_to_hub("your-name/llama2-7b-awq")

# ë¡œë“œ & ì¶”ë¡ 
model = AutoAWQForCausalLM.from_quantized("llama2-7b-awq", fuse_layers=True)
```

---

## Mixed Precision

ì „ëµì ìœ¼ë¡œ ì •ë°€ë„ ì¡°í•©:

```python
# Sensitive layers: FP16
# Other layers: INT4

quantization_config = {
    "layers.0-10": "fp16",      # ì´ˆë°˜ ë ˆì´ì–´
    "layers.11-20": "int8",     # ì¤‘ê°„
    "layers.21-31": "int4",     # í›„ë°˜
    "lm_head": "fp16"           # ì¶œë ¥ ë ˆì´ì–´
}
```

**ì˜ˆì‹œ: LLaMA-70B**
```python
# Embedding: FP16 (ì¤‘ìš”)
# Attention: INT4 (ëŒ€ë¶€ë¶„)
# MLP: INT4
# Layer Norm: FP16 (ì‘ìŒ)
# Output: FP16 (ì¤‘ìš”)

total_memory = (
    0.5 GB  # Embeddings (FP16)
    + 34 GB  # Attention+MLP (INT4)
    + 0.1 GB  # LayerNorm (FP16)
    + 0.3 GB  # Output (FP16)
    = 35 GB  # RTX 4090ìœ¼ë¡œ ê°€ëŠ¥!
)
```

---

## ë²¤ì¹˜ë§ˆí¬

### ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰

**LLaMA-70B:**

| ì •ë°€ë„ | ë©”ëª¨ë¦¬ (GB) | GPU |
|--------|------------|-----|
| FP32 | 280 | N/A |
| FP16 | 140 | 2x A100 80GB |
| INT8 | 70 | A100 80GB |
| INT4 (GPTQ) | 35 | A100 40GB, RTX 4090 |
| INT4 (QLoRA) | 35 | RTX 4090 |

### ì†ë„

**LLaMA-7B, A100:**

| ì •ë°€ë„ | Tokens/sec | Speedup |
|--------|-----------|---------|
| FP16 | 42 | 1x |
| INT8 | 68 | 1.6x |
| INT4 (GPTQ) | 94 | 2.2x |
| INT4 (AWQ) | 103 | 2.5x |

### ì •í™•ë„

**LLaMA-7B, WikiText-2 Perplexity:**

| ì •ë°€ë„ | Perplexity | Delta |
|--------|-----------|-------|
| FP16 | 5.68 | 0% |
| INT8 (Calibration) | 5.71 | +0.5% |
| INT4 (Naive) | 7.82 | +37.7% âŒ |
| INT4 (GPTQ) | 5.74 | +1.1% âœ… |
| INT4 (AWQ) | 5.72 | +0.7% âœ… |

**GPTQ/AWQëŠ” ì •í™•ë„ ìœ ì§€!**

---

## ê³ ê¸‰ ê¸°ë²•

### 1. SmoothQuant

Activation + Weight ë™ì‹œ ì–‘ìí™”:

```python
# Activation outlier ë¬¸ì œ
activations = [0.1, 0.2, 0.15, 12.5]  # Outlier!

# Scale ì¡°ì •
s = sqrt(max(abs(W)) / max(abs(X)))
W' = W / s
X' = X * s

# ì´ì œ ë‘˜ ë‹¤ ì–‘ìí™” ê°€ëŠ¥
W_quant = quantize(W')
X_quant = quantize(X')
```

### 2. LLM.int8()

**Mixed INT8/FP16:**

```python
# Outlier feature (< 0.1%)ëŠ” FP16 ìœ ì§€
def llm_int8_forward(x, weight):
    # Outlier ê°ì§€
    outlier_idx = (x.abs() > threshold).any(dim=0)
    
    # ë¶„ë¦¬
    x_outlier = x[:, outlier_idx]
    x_normal = x[:, ~outlier_idx]
    
    w_outlier = weight[:, outlier_idx]
    w_normal = weight[:, ~outlier_idx]
    
    # ê³„ì‚°
    out_outlier = x_outlier @ w_outlier.T  # FP16
    out_normal = quantized_matmul(x_normal, quantize(w_normal))  # INT8
    
    return out_outlier + out_normal
```

### 3. GGUF/GGML (llama.cpp)

**CPUì— ìµœì í™”:**

```bash
# ë‹¤ì–‘í•œ quantization ì§€ì›
Q4_0: 4-bit, fastest
Q4_K_M: 4-bit, mixed
Q5_K_M: 5-bit, balanced
Q8_0: 8-bit, best quality

# ì‚¬ìš©
./llama.cpp/main \
  -m llama-2-7b-Q4_K_M.gguf \
  -p "Once upon a time" \
  -n 100
```

---

## ì‹¤ì „ ì˜ˆì œ: QLoRA Fine-tuning

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from trl import SFTTrainer

# 1. 4-bitë¡œ ëª¨ë¸ ë¡œë“œ
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True
)

model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-2-70b-hf",
    quantization_config=bnb_config,
    device_map="auto",
    trust_remote_code=True
)

tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-70b-hf")

# 2. ëª¨ë¸ ì¤€ë¹„
model = prepare_model_for_kbit_training(model)

# 3. LoRA ì„¤ì •
lora_config = LoraConfig(
    r=64,
    lora_alpha=16,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)

model = get_peft_model(model, lora_config)

# 4. í•™ìŠµ
training_args = TrainingArguments(
    output_dir="./llama2-70b-qlora",
    per_device_train_batch_size=1,
    gradient_accumulation_steps=16,
    num_train_epochs=3,
    learning_rate=2e-4,
    fp16=False,
    bf16=True,
    logging_steps=10,
    optim="paged_adamw_32bit",  # QLoRA optimizer
    save_strategy="epoch"
)

trainer = SFTTrainer(
    model=model,
    train_dataset=dataset,
    tokenizer=tokenizer,
    args=training_args,
    max_seq_length=512
)

trainer.train()

# 5. ì €ì¥ (LoRA adapterë§Œ, ëª‡ MB)
model.save_pretrained("./llama2-70b-qlora-adapter")
```

**ë©”ëª¨ë¦¬:** 24GB (RTX 4090ìœ¼ë¡œ 70B í•™ìŠµ!)

---

## Best Practices

### 1. ì–´ë–¤ ë°©ë²• ì„ íƒ?

**ì¶”ë¡ ë§Œ:**
- **ë¹ ë¥¸ ì–‘ìí™”**: AWQ
- **ìµœê³  ì •í™•ë„**: GPTQ
- **CPU ì¶”ë¡ **: GGUF

**Fine-tuning:**
- **ëŒ€ê·œëª¨ ëª¨ë¸**: QLoRA
- **ì •í™•ë„ ì¤‘ìš”**: LoRA (no quantization)

### 2. Calibration Dataset

```python
# Good: Domain-specific
calibration_data = load_dataset("your_domain")

# Better: Diverse
calibration_data = load_dataset("c4", split="train[:1000]")

# Best: Task-relevant
calibration_data = your_training_data[:1000]
```

### 3. Validation

```python
# í•­ìƒ ê²€ì¦!
def validate_quantized_model(original, quantized, test_data):
    orig_ppl = compute_perplexity(original, test_data)
    quant_ppl = compute_perplexity(quantized, test_data)
    
    degradation = (quant_ppl - orig_ppl) / orig_ppl * 100
    print(f"Perplexity degradation: {degradation:.2f}%")
    
    if degradation > 5:
        print("âš ï¸ Too much quality loss!")
    else:
        print("âœ… Acceptable quality")
```

---

## ìš”ì•½

**Quantization**ì€:

1. **ë©”ëª¨ë¦¬**: 2-4ë°° ì ˆê° (INT8/INT4)
2. **ì†ë„**: 1.5-3ë°° í–¥ìƒ
3. **ì •í™•ë„**: 1-2% ì†ì‹¤ (GPTQ/AWQ)
4. **ì ‘ê·¼ì„±**: ì†Œí˜• GPUë¡œ ëŒ€í˜• ëª¨ë¸ ì‹¤í–‰

**ë°©ë²• ë¹„êµ:**

| ë°©ë²• | ì •í™•ë„ | ì†ë„ | ë©”ëª¨ë¦¬ | ì‚¬ìš©ì²˜ |
|------|--------|------|--------|--------|
| GPTQ | â­â­â­â­â­ | â­â­â­â­ | â­â­â­â­â­ | ì¶”ë¡  |
| AWQ | â­â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­â­ | ì¶”ë¡  (ì¶”ì²œ!) |
| QLoRA | â­â­â­â­â­ | â­â­â­ | â­â­â­â­â­ | Fine-tuning |
| GGUF | â­â­â­â­ | â­â­â­â­ | â­â­â­â­â­ | CPU ì¶”ë¡  |

**í•µì‹¬:** ëª¨ë“  í”„ë¡œë•ì…˜ LLMì— í•„ìˆ˜!

---

## ì‹œë¦¬ì¦ˆ ì™„ë£Œ! ğŸ‰

**LLM Inference ìµœì í™” ì™„ì „ ì •ë³µ:**

1. **Paged Attention**: ë©”ëª¨ë¦¬ 10ë°° ì ˆì•½
2. **KV Caching**: ì†ë„ 50-100ë°°
3. **LoRA**: Fine-tuning 10ë°° íš¨ìœ¨
4. **Flash Attention**: ë©”ëª¨ë¦¬ + ì†ë„ ëª¨ë‘
5. **Speculative Decoding**: 2-3ë°° ê°€ì†
6. **Continuous Batching**: ì²˜ë¦¬ëŸ‰ ê·¹ëŒ€í™”
7. **Quantization**: ë©”ëª¨ë¦¬ 4ë°°, ì†ë„ 2ë°°

**ì¡°í•©í•˜ë©´?**
- vLLM (Paged + Continuous + Flash): **10-20ë°° ì²˜ë¦¬ëŸ‰**
- QLoRA (Quantization + LoRA): **24GBë¡œ 70B í•™ìŠµ**
- AWQ + Flash + Speculative: **50-100ë°° ë¹ ë¥¸ ì¶”ë¡ **

ì´ì œ ì—¬ëŸ¬ë¶„ë„ íš¨ìœ¨ì ì¸ LLMì„ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤! ğŸš€

---

*ì§ˆë¬¸ì´ë‚˜ í”¼ë“œë°±ì€ [GitHub](https://github.com/yhna941)ì—ì„œ í™˜ì˜í•©ë‹ˆë‹¤!*

*ì „ì²´ ì‹œë¦¬ì¦ˆê°€ ë„ì›€ì´ ë˜ì…¨ë‹¤ë©´ â­ Star ë¶€íƒë“œë¦½ë‹ˆë‹¤!*
