---
title: "Diffusion Models #1: ê¸°ì´ˆ - DDPMê³¼ ë…¸ì´ì¦ˆ ì œê±° ì›ë¦¬"
description: "Stable Diffusionì˜ í•µì‹¬ì¸ Diffusion ëª¨ë¸ì˜ ìˆ˜í•™ì  ì›ë¦¬ì™€ êµ¬í˜„ì„ ì™„ì „íˆ ì´í•´í•©ë‹ˆë‹¤."
pubDate: 2026-02-06
author: "Yh Na"
tags: ["diffusion", "ddpm", "generative-models", "stable-diffusion", "pytorch"]
draft: false
---

# Diffusion Models #1: ê¸°ì´ˆ

**"ë…¸ì´ì¦ˆì—ì„œ ì´ë¯¸ì§€ë¥¼ ë§Œë“ ë‹¤"**

```
Random Noise â†’ ... â†’ Beautiful Image
[Static]           [Mona Lisa]
```

ì´ê²Œ ì–´ë–»ê²Œ ê°€ëŠ¥í• ê¹Œ?

ì´ë²ˆ ê¸€:
- Diffusionì˜ ì§ê´€
- Forward/Reverse Process
- DDPM ìˆ˜í•™
- ì™„ì „í•œ êµ¬í˜„

---

## Diffusionì´ë€?

### ë¹„ìœ : ë¬¼ê° í™•ì‚°

**Forward (í™•ì‚°):**

```
ê¹¨ë—í•œ ë¬¼ â†’ ë¬¼ê° í•œ ë°©ìš¸ â†’ ì ì  í¼ì§ â†’ ì™„ì „íˆ ì„ì„
[Clear]                                     [Muddy]
```

**Reverse (ì—­í™•ì‚°):**

```
ì„ì¸ ë¬¼ â†’ ì ì  ë¶„ë¦¬ â†’ ë¬¼ê° ì‘ì§‘ â†’ ê¹¨ë—í•œ ë¬¼
[Muddy]                              [Clear]
```

### ì´ë¯¸ì§€ ìƒì„±

**Forward Process (í•™ìŠµ ì‹œ):**

```
ì‹¤ì œ ì´ë¯¸ì§€ xâ‚€
â†’ ì•½ê°„ ë…¸ì´ì¦ˆ xâ‚
â†’ ë” ë§ì€ ë…¸ì´ì¦ˆ xâ‚‚
â†’ ...
â†’ ì™„ì „í•œ ë…¸ì´ì¦ˆ xâ‚œ
```

**Reverse Process (ìƒì„± ì‹œ):**

```
Random Noise xâ‚œ
â†’ ì•½ê°„ ëœ ë…¸ì´ì¦ˆ xâ‚œâ‚‹â‚
â†’ ë” ì„ ëª… xâ‚œâ‚‹â‚‚
â†’ ...
â†’ ì‹¤ì œ ì´ë¯¸ì§€ xâ‚€
```

---

## Forward Process (í™•ì‚°)

### ìˆ˜í•™

**í•œ ìŠ¤í…ì”© ë…¸ì´ì¦ˆ ì¶”ê°€:**

$$
q(x_t | x_{t-1}) = \mathcal{N}(x_t; \sqrt{1-\beta_t} x_{t-1}, \beta_t I)
$$

- $x_t$: t ì‹œì ì˜ ì´ë¯¸ì§€
- $\beta_t$: ë…¸ì´ì¦ˆ ìŠ¤ì¼€ì¤„ (0.0001 â†’ 0.02)
- $\mathcal{N}$: ì •ê·œë¶„í¬

**ì‰½ê²Œ:**

```python
x_t = sqrt(1 - Î²_t) * x_{t-1} + sqrt(Î²_t) * Îµ

ì—¬ê¸°ì„œ:
- x_{t-1}: ì´ì „ ì´ë¯¸ì§€
- Î²_t: ë…¸ì´ì¦ˆ ì–‘
- Îµ ~ N(0, I): ëœë¤ ë…¸ì´ì¦ˆ
```

### ì¤‘ìš”í•œ ì„±ì§ˆ: í•œ ë²ˆì— ì í”„!

**T ìŠ¤í… ë°˜ë³µ ëŒ€ì‹ :**

$$
q(x_t | x_0) = \mathcal{N}(x_t; \sqrt{\bar{\alpha}_t} x_0, (1-\bar{\alpha}_t) I)
$$

ì—¬ê¸°ì„œ:
- $\alpha_t = 1 - \beta_t$
- $\bar{\alpha}_t = \prod_{s=1}^t \alpha_s$

**ì½”ë“œ:**

```python
import torch
import torch.nn as nn
import numpy as np

class ForwardDiffusion:
    def __init__(self, T=1000):
        """
        T: Total timesteps
        """
        self.T = T
        
        # Beta schedule (linear)
        self.betas = torch.linspace(0.0001, 0.02, T)
        
        # Alpha
        self.alphas = 1 - self.betas
        self.alphas_cumprod = torch.cumprod(self.alphas, dim=0)
        
        # For convenience
        self.sqrt_alphas_cumprod = torch.sqrt(self.alphas_cumprod)
        self.sqrt_one_minus_alphas_cumprod = torch.sqrt(1 - self.alphas_cumprod)
    
    def q_sample(self, x_0, t, noise=None):
        """
        Sample x_t from q(x_t | x_0)
        
        x_0: (batch, channels, height, width)
        t: (batch,) - timestep for each sample
        """
        if noise is None:
            noise = torch.randn_like(x_0)
        
        # Extract coefficients for each timestep
        sqrt_alpha_prod = self.sqrt_alphas_cumprod[t]
        sqrt_one_minus_alpha_prod = self.sqrt_one_minus_alphas_cumprod[t]
        
        # Reshape for broadcasting
        while len(sqrt_alpha_prod.shape) < len(x_0.shape):
            sqrt_alpha_prod = sqrt_alpha_prod.unsqueeze(-1)
            sqrt_one_minus_alpha_prod = sqrt_one_minus_alpha_prod.unsqueeze(-1)
        
        # x_t = sqrt(Î±Ì…_t) * x_0 + sqrt(1 - Î±Ì…_t) * Îµ
        x_t = sqrt_alpha_prod * x_0 + sqrt_one_minus_alpha_prod * noise
        
        return x_t, noise

# ì‚¬ìš©
diffusion = ForwardDiffusion(T=1000)

# ì›ë³¸ ì´ë¯¸ì§€
x_0 = torch.randn(4, 3, 32, 32)  # Batch=4, RGB, 32x32

# t=500ì—ì„œ ìƒ˜í”Œë§
t = torch.tensor([500, 500, 500, 500])
x_t, noise = diffusion.q_sample(x_0, t)

print(x_t.shape)  # (4, 3, 32, 32)
```

### ì‹œê°í™”

```python
import matplotlib.pyplot as plt

def visualize_forward_process(image, diffusion, steps=[0, 50, 100, 250, 500, 999]):
    """ì´ë¯¸ì§€ê°€ ì ì  ë…¸ì´ì¦ˆë¡œ ë³€í•˜ëŠ” ê³¼ì •"""
    fig, axes = plt.subplots(1, len(steps), figsize=(15, 3))
    
    for idx, t in enumerate(steps):
        t_tensor = torch.tensor([t])
        x_t, _ = diffusion.q_sample(image.unsqueeze(0), t_tensor)
        
        # Denormalize and show
        img = x_t.squeeze(0).permute(1, 2, 0).numpy()
        img = (img - img.min()) / (img.max() - img.min())
        
        axes[idx].imshow(img)
        axes[idx].set_title(f't={t}')
        axes[idx].axis('off')
    
    plt.show()
```

---

## Reverse Process (ìƒì„±)

### ëª©í‘œ

**ë°°ìš°ê³  ì‹¶ì€ ê²ƒ:**

$$
p_\theta(x_{t-1} | x_t)
$$

"ë…¸ì´ì¦ˆì—ì„œ ì´ì „ ë‹¨ê³„ë¥¼ ì˜ˆì¸¡"

### ë¬¸ì œ: ì§ì ‘ í•™ìŠµ ë¶ˆê°€ëŠ¥

**ì´ìœ :**

```
p(x_{t-1} | x_t)ë¥¼ ì§ì ‘ ëª¨ë¸ë§? 
â†’ x_të§Œ ë³´ê³  x_{t-1} ì˜ˆì¸¡? ì •ë³´ ë¶€ì¡±!

í•´ê²°ì±…:
x_0 (ì›ë³¸)ë„ ì¡°ê±´ìœ¼ë¡œ!
â†’ p(x_{t-1} | x_t, x_0)
```

### í•µì‹¬ í†µì°° (Ho et al., 2020)

**ì¡°ê±´ë¶€ ë¶„í¬ëŠ” ì •ê·œë¶„í¬:**

$$
q(x_{t-1} | x_t, x_0) = \mathcal{N}(x_{t-1}; \tilde{\mu}_t(x_t, x_0), \tilde{\beta}_t I)
$$

ì—¬ê¸°ì„œ:

$$
\tilde{\mu}_t(x_t, x_0) = \frac{\sqrt{\bar{\alpha}_{t-1}} \beta_t}{1 - \bar{\alpha}_t} x_0 + \frac{\sqrt{\alpha_t}(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_t} x_t
$$

**ë¬¸ì œ:** $x_0$ë¥¼ ëª¨ë¦„!

**í•´ê²°:** $x_0$ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ëª¨ë¸ í•™ìŠµ!

### Noise Prediction (DDPM)

**$x_0$ ì§ì ‘ ì˜ˆì¸¡ ëŒ€ì‹ , ë…¸ì´ì¦ˆ $\epsilon$ ì˜ˆì¸¡:**

$$
x_0 = \frac{1}{\sqrt{\bar{\alpha}_t}}(x_t - \sqrt{1-\bar{\alpha}_t} \epsilon)
$$

**ëª¨ë¸:**

```python
Îµ_Î¸(x_t, t) â†’ ì˜ˆì¸¡ëœ ë…¸ì´ì¦ˆ
```

**Loss:**

$$
\mathcal{L} = \mathbb{E}_{t, x_0, \epsilon} \left[ \| \epsilon - \epsilon_\theta(x_t, t) \|^2 \right]
$$

"ì‹¤ì œ ë…¸ì´ì¦ˆì™€ ì˜ˆì¸¡ ë…¸ì´ì¦ˆì˜ ì°¨ì´"

---

## U-Net êµ¬ì¡°

**DDPMì˜ backbone:**

```
        Encoder                    Decoder
Input â”€â”€â”€â”€â”€â”€â”                    â”Œâ”€â”€â”€â”€â”€â”€ Output
    â”‚       â”‚                    â”‚       â”‚
  Conv   â”Œâ”€â”€â–¼â”€â”€â”              â”Œâ”€â”€â–¼â”€â”€â”  Conv
    â”‚    â”‚Down â”‚              â”‚ Up  â”‚    â”‚
  Conv   â”‚     â”‚              â”‚     â”‚  Conv
    â”‚    â””â”€â”€â”¬â”€â”€â”˜              â””â”€â”€â–²â”€â”€â”˜    â”‚
  Pool      â”‚                    â”‚     Upsample
    â”‚    â”Œâ”€â”€â–¼â”€â”€â”    Middle   â”Œâ”€â”€â”´â”€â”€â”    â”‚
    â””â”€â”€â”€â”€â”¤Down â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚ Up  â”œâ”€â”€â”€â”€â”˜
         â””â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”˜
         
Skip connections (concat)
```

**êµ¬í˜„:**

```python
class TimeEmbedding(nn.Module):
    """Sinusoidal time embedding"""
    def __init__(self, dim):
        super().__init__()
        self.dim = dim
    
    def forward(self, t):
        """
        t: (batch,)
        return: (batch, dim)
        """
        half_dim = self.dim // 2
        emb = np.log(10000) / (half_dim - 1)
        emb = torch.exp(torch.arange(half_dim) * -emb).to(t.device)
        emb = t[:, None] * emb[None, :]
        emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=-1)
        return emb

class ResBlock(nn.Module):
    """Residual block with time embedding"""
    def __init__(self, in_channels, out_channels, time_dim):
        super().__init__()
        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, padding=1)
        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, padding=1)
        
        self.time_mlp = nn.Linear(time_dim, out_channels)
        
        self.norm1 = nn.GroupNorm(8, out_channels)
        self.norm2 = nn.GroupNorm(8, out_channels)
        
        self.act = nn.SiLU()
        
        if in_channels != out_channels:
            self.skip = nn.Conv2d(in_channels, out_channels, 1)
        else:
            self.skip = nn.Identity()
    
    def forward(self, x, t_emb):
        """
        x: (batch, in_channels, H, W)
        t_emb: (batch, time_dim)
        """
        h = self.conv1(x)
        h = self.norm1(h)
        h = self.act(h)
        
        # Add time embedding
        t_emb = self.time_mlp(t_emb)
        h = h + t_emb[:, :, None, None]
        
        h = self.conv2(h)
        h = self.norm2(h)
        h = self.act(h)
        
        return h + self.skip(x)

class UNet(nn.Module):
    """U-Net for noise prediction"""
    def __init__(self, in_channels=3, out_channels=3, time_dim=256):
        super().__init__()
        
        # Time embedding
        self.time_mlp = nn.Sequential(
            TimeEmbedding(time_dim),
            nn.Linear(time_dim, time_dim),
            nn.SiLU()
        )
        
        # Encoder
        self.enc1 = ResBlock(in_channels, 64, time_dim)
        self.enc2 = ResBlock(64, 128, time_dim)
        self.enc3 = ResBlock(128, 256, time_dim)
        
        self.pool = nn.MaxPool2d(2)
        
        # Middle
        self.middle = ResBlock(256, 256, time_dim)
        
        # Decoder
        self.up3 = nn.ConvTranspose2d(256, 256, 2, stride=2)
        self.dec3 = ResBlock(256 + 256, 128, time_dim)  # +256 from skip
        
        self.up2 = nn.ConvTranspose2d(128, 128, 2, stride=2)
        self.dec2 = ResBlock(128 + 128, 64, time_dim)
        
        self.up1 = nn.ConvTranspose2d(64, 64, 2, stride=2)
        self.dec1 = ResBlock(64 + 64, 64, time_dim)
        
        # Output
        self.out = nn.Conv2d(64, out_channels, 1)
    
    def forward(self, x, t):
        """
        x: (batch, 3, H, W) - noisy image
        t: (batch,) - timestep
        return: (batch, 3, H, W) - predicted noise
        """
        # Time embedding
        t_emb = self.time_mlp(t)
        
        # Encoder
        e1 = self.enc1(x, t_emb)
        e2 = self.enc2(self.pool(e1), t_emb)
        e3 = self.enc3(self.pool(e2), t_emb)
        
        # Middle
        m = self.middle(self.pool(e3), t_emb)
        
        # Decoder
        d3 = self.up3(m)
        d3 = torch.cat([d3, e3], dim=1)  # Skip connection
        d3 = self.dec3(d3, t_emb)
        
        d2 = self.up2(d3)
        d2 = torch.cat([d2, e2], dim=1)
        d2 = self.dec2(d2, t_emb)
        
        d1 = self.up1(d2)
        d1 = torch.cat([d1, e1], dim=1)
        d1 = self.dec1(d1, t_emb)
        
        # Output (predicted noise)
        out = self.out(d1)
        return out

# ëª¨ë¸ í¬ê¸°
model = UNet()
params = sum(p.numel() for p in model.parameters())
print(f"Parameters: {params:,}")  # ~35M
```

---

## Training

```python
class DDPMTrainer:
    def __init__(self, model, diffusion, device='cuda'):
        self.model = model.to(device)
        self.diffusion = diffusion
        self.device = device
    
    def train_step(self, x_0):
        """
        x_0: (batch, 3, H, W) - real images
        """
        batch_size = x_0.size(0)
        
        # 1. Random timestep for each sample
        t = torch.randint(0, self.diffusion.T, (batch_size,), device=self.device)
        
        # 2. Add noise
        noise = torch.randn_like(x_0)
        x_t, _ = self.diffusion.q_sample(x_0, t, noise)
        
        # 3. Predict noise
        noise_pred = self.model(x_t, t)
        
        # 4. Loss (MSE)
        loss = F.mse_loss(noise_pred, noise)
        
        return loss
    
    def train(self, dataloader, epochs=100, lr=1e-4):
        optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)
        
        for epoch in range(epochs):
            total_loss = 0
            
            for images, _ in dataloader:
                images = images.to(self.device)
                
                # Train step
                loss = self.train_step(images)
                
                # Backward
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()
                
                total_loss += loss.item()
            
            avg_loss = total_loss / len(dataloader)
            print(f"Epoch {epoch+1}: Loss = {avg_loss:.4f}")

# Train
diffusion = ForwardDiffusion(T=1000)
model = UNet()
trainer = DDPMTrainer(model, diffusion)

trainer.train(train_loader, epochs=100)
```

---

## Sampling (ìƒì„±)

```python
@torch.no_grad()
def ddpm_sample(model, diffusion, shape=(4, 3, 32, 32), device='cuda'):
    """
    Generate images from noise
    shape: (batch, channels, height, width)
    """
    model.eval()
    
    # Start from pure noise
    x_t = torch.randn(shape, device=device)
    
    # Reverse process
    for t in reversed(range(diffusion.T)):
        # Current timestep
        t_batch = torch.full((shape[0],), t, device=device, dtype=torch.long)
        
        # Predict noise
        noise_pred = model(x_t, t_batch)
        
        # Compute x_{t-1}
        alpha_t = diffusion.alphas[t]
        alpha_bar_t = diffusion.alphas_cumprod[t]
        
        # Mean
        mean = (1 / torch.sqrt(alpha_t)) * (
            x_t - ((1 - alpha_t) / torch.sqrt(1 - alpha_bar_t)) * noise_pred
        )
        
        if t > 0:
            # Add noise (except last step)
            beta_t = diffusion.betas[t]
            noise = torch.randn_like(x_t)
            x_t = mean + torch.sqrt(beta_t) * noise
        else:
            x_t = mean
    
    return x_t

# Generate images
samples = ddpm_sample(model, diffusion, shape=(16, 3, 32, 32))
print(samples.shape)  # (16, 3, 32, 32)
```

### ì‹œê°í™”

```python
import torchvision

def visualize_samples(samples):
    """Display generated images"""
    # Denormalize
    samples = (samples + 1) / 2  # [-1, 1] â†’ [0, 1]
    samples = samples.clamp(0, 1)
    
    # Make grid
    grid = torchvision.utils.make_grid(samples, nrow=4)
    
    # Show
    plt.figure(figsize=(12, 12))
    plt.imshow(grid.permute(1, 2, 0).cpu())
    plt.axis('off')
    plt.show()

# Generate and show
samples = ddpm_sample(model, diffusion, shape=(16, 3, 32, 32))
visualize_samples(samples)
```

---

## DDIM (ë¹ ë¥¸ ìƒ˜í”Œë§)

**DDPM ë¬¸ì œ:** 1000 ìŠ¤í… í•„ìš” â†’ ëŠë¦¼!

**DDIM (2020):** 50 ìŠ¤í…ìœ¼ë¡œ ê°™ì€ í’ˆì§ˆ!

```python
@torch.no_grad()
def ddim_sample(model, diffusion, shape, steps=50, eta=0.0):
    """
    Fast sampling with DDIM
    steps: number of sampling steps (much less than T)
    eta: 0=deterministic, 1=stochastic (DDPM)
    """
    # Select timesteps
    skip = diffusion.T // steps
    timesteps = torch.arange(0, diffusion.T, skip).flip(0)
    
    x_t = torch.randn(shape, device='cuda')
    
    for i, t in enumerate(timesteps):
        t_batch = torch.full((shape[0],), t, dtype=torch.long, device='cuda')
        
        # Predict noise
        noise_pred = model(x_t, t_batch)
        
        # Get Î±
        alpha_bar_t = diffusion.alphas_cumprod[t]
        
        if i < len(timesteps) - 1:
            t_prev = timesteps[i + 1]
            alpha_bar_prev = diffusion.alphas_cumprod[t_prev]
        else:
            alpha_bar_prev = torch.tensor(1.0)
        
        # Predict x_0
        x_0_pred = (x_t - torch.sqrt(1 - alpha_bar_t) * noise_pred) / torch.sqrt(alpha_bar_t)
        
        # DDIM update
        sigma = eta * torch.sqrt((1 - alpha_bar_prev) / (1 - alpha_bar_t)) * \
                torch.sqrt(1 - alpha_bar_t / alpha_bar_prev)
        
        direction = torch.sqrt(1 - alpha_bar_prev - sigma**2) * noise_pred
        
        x_t = torch.sqrt(alpha_bar_prev) * x_0_pred + direction
        
        if sigma > 0:
            x_t += sigma * torch.randn_like(x_t)
    
    return x_t

# 20ë°° ë¹ ë¦„!
samples = ddim_sample(model, diffusion, shape=(16, 3, 32, 32), steps=50)
```

---

## ìš”ì•½

**Diffusion Models:**

1. **Forward**: ì´ë¯¸ì§€ â†’ ë…¸ì´ì¦ˆ (í™•ì‚°)
2. **Reverse**: ë…¸ì´ì¦ˆ â†’ ì´ë¯¸ì§€ (ì—­í™•ì‚°)
3. **í•™ìŠµ**: ë…¸ì´ì¦ˆ ì˜ˆì¸¡ ëª¨ë¸
4. **ìƒì„±**: ë°˜ë³µì  ë…¸ì´ì¦ˆ ì œê±°

**í•µì‹¬ ìˆ˜ì‹:**

$$
\mathcal{L} = \mathbb{E}\left[ \| \epsilon - \epsilon_\theta(x_t, t) \|^2 \right]
$$

**ì¥ì :**
- ê³ í’ˆì§ˆ ìƒì„±
- í•™ìŠµ ì•ˆì •ì 
- Likelihood ê³„ì‚° ê°€ëŠ¥

**ë‹¨ì :**
- ìƒ˜í”Œë§ ëŠë¦¼ (DDIMìœ¼ë¡œ í•´ê²°)

**ë‹¤ìŒ ê¸€:**
- **Stable Diffusion**: Latent Diffusion, CLIP
- **Conditional Generation**: Text-to-Image
- **ControlNet**: ì¡°ê±´ë¶€ ì œì–´

ê¸°ëŒ€í•´ì£¼ì„¸ìš”! ğŸš€

---

*ì§ˆë¬¸ì´ë‚˜ í”¼ë“œë°±ì€ [GitHub](https://github.com/yhna941)ì—ì„œ í™˜ì˜í•©ë‹ˆë‹¤!*
