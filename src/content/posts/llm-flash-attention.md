---
title: "LLM Inference ìµœì í™” #4: Flash Attention - ë©”ëª¨ë¦¬ë„ ì¤„ì´ê³  ì†ë„ë„ ì˜¬ë¦¬ê³ "
description: "Attentionì˜ ë©”ëª¨ë¦¬ ë³µì¡ë„ë¥¼ O(NÂ²)ì—ì„œ O(N)ìœ¼ë¡œ ì¤„ì´ëŠ” Flash Attentionì˜ ì›ë¦¬ì™€ ì‹¤ì „ ì‚¬ìš©ë²•ì„ ì•Œì•„ë´…ë‹ˆë‹¤."
pubDate: 2026-02-06
author: "Yh Na"
tags: ["llm", "optimization", "flash-attention", "cuda", "memory"]
draft: false
---

# LLM Inference ìµœì í™” #4: Flash Attention

Standard attentionì˜ ë¬¸ì œëŠ” **ë©”ëª¨ë¦¬**ì…ë‹ˆë‹¤. ì‹œí€€ìŠ¤ ê¸¸ì´ê°€ 2ë°° ëŠ˜ë©´ ë©”ëª¨ë¦¬ëŠ” **4ë°°** ì¦ê°€í•©ë‹ˆë‹¤. O(NÂ²) ë³µì¡ë„ì£ .

**Flash Attention**ì€ ì´ê±¸ O(N)ìœ¼ë¡œ ì¤„ì…ë‹ˆë‹¤. ì–´ë–»ê²Œ? **ë©”ëª¨ë¦¬ ê³„ì¸µì„ ì´í•´**í•˜ê³ , **ì¬ê³„ì‚°**ì„ ì˜ë¦¬í•˜ê²Œ ì”ë‹ˆë‹¤.

ê²°ê³¼:
- ë©”ëª¨ë¦¬: **10-20ë°° ì ˆì•½**
- ì†ë„: **2-4ë°° ë¹ ë¦„**
- ê¸´ ì‹œí€€ìŠ¤: **ê°€ëŠ¥í•´ì§** (4K â†’ 64K)

---

## ë¬¸ì œ: Standard Attentionì˜ ë©”ëª¨ë¦¬ í­íƒ„

### Attention ìˆ˜ì‹

```
Q, K, V = [seq_len, d_model] ê°ê°
S = QK^T / âˆšd                    # [seq_len, seq_len]  â† ë¬¸ì œ!
P = softmax(S)                   # [seq_len, seq_len]  â† ë¬¸ì œ!
O = PV                           # [seq_len, d_model]
```

### ë©”ëª¨ë¦¬ ê³„ì‚°

**ì‹œí€€ìŠ¤ ê¸¸ì´ 2048, fp16:**

```
S: [2048, 2048] Ã— 2 bytes = 8 MB
P: [2048, 2048] Ã— 2 bytes = 8 MB
ì´: 16 MB (per head)
```

32 heads Ã— 32 layers = **16 GB**

**ë°°ì¹˜ í¬ê¸° 16ì´ë©´? 256 GB!**

### ë¬¸ì œì˜ ê·¼ë³¸

Attention matrix Sì™€ Pë¥¼ **ì „ì²´ ë©”ëª¨ë¦¬ì— ì €ì¥**í•©ë‹ˆë‹¤.

```python
# Standard attention
S = Q @ K.T / sqrt(d)      # Materialize: [N, N]
P = softmax(S)             # Materialize: [N, N]
O = P @ V                  # Result: [N, d]
```

**N=4096ì´ë©´ S, PëŠ” ê°ê° 32MB (fp16)**

---

## GPU ë©”ëª¨ë¦¬ ê³„ì¸µ

ì´í•´ì˜ í•µì‹¬ì€ **ë©”ëª¨ë¦¬ ì†ë„**ì…ë‹ˆë‹¤.

```
HBM (High Bandwidth Memory):
  - í¬ê¸°: 40-80 GB
  - ì†ë„: ~1.5 TB/s
  - ëŠë¦¼!

SRAM (On-chip):
  - í¬ê¸°: ~20 MB (per SM)
  - ì†ë„: ~19 TB/s
  - ë¹ ë¦„! (10ë°°+)
```

**Standard attentionì€ HBMì— S, Pë¥¼ ì“°ê³  ì½ìŠµë‹ˆë‹¤** â†’ ëŠë¦¼!

**Flash Attentionì€ SRAMë§Œ ì”ë‹ˆë‹¤** â†’ ë¹ ë¦„!

---

## Flash Attention í•µì‹¬ ì•„ì´ë””ì–´

### 1. Tiling (íƒ€ì¼ë§)

í° í–‰ë ¬ì„ ì‘ì€ **ë¸”ë¡(tile)**ë¡œ ë‚˜ëˆ•ë‹ˆë‹¤.

```
Q: [N, d] â†’ blocks: [B_q, d]  (B_q = N / num_blocks)
K: [N, d] â†’ blocks: [B_k, d]
V: [N, d] â†’ blocks: [B_k, d]

í•œ ë²ˆì— í•˜ë‚˜ì˜ ë¸”ë¡ë§Œ SRAMì— ë¡œë“œ
```

### 2. Recomputation (ì¬ê³„ì‚°)

ì¤‘ê°„ ê²°ê³¼(S, P)ë¥¼ **ì €ì¥ ì•ˆ í•˜ê³  ì¬ê³„ì‚°**í•©ë‹ˆë‹¤.

```
Forward: S, P ì €ì¥ ì•ˆ í•¨ (SRAMì—ì„œë§Œ ê³„ì‚°)
Backward: S, P ë‹¤ì‹œ ê³„ì‚° (Q, K, Vì—ì„œ)
```

**Trade-off:**
- ë©”ëª¨ë¦¬: â†“â†“â†“ (S, P ì•ˆ ì €ì¥)
- ê³„ì‚°: â†‘ (ì¬ê³„ì‚°)
- ì´ ì†ë„: â†‘ (ë©”ëª¨ë¦¬ I/Oê°€ ë³‘ëª©ì´ë¼ ê³„ì‚° ì¦ê°€ëŠ” ê´œì°®ìŒ)

### 3. Online Softmax

Softmaxë¥¼ **ìŠ¤íŠ¸ë¦¬ë°**ìœ¼ë¡œ ê³„ì‚°í•©ë‹ˆë‹¤.

**Standard softmax:**
```python
# ì „ì²´ í–‰ í•„ìš”
S_row = [s_1, s_2, ..., s_N]
max_val = max(S_row)
exp_vals = [exp(s_i - max_val) for s_i in S_row]
sum_exp = sum(exp_vals)
P_row = [e / sum_exp for e in exp_vals]
```

**ë¬¸ì œ:** ì „ì²´ í–‰ì„ ë©”ëª¨ë¦¬ì— ì €ì¥ í•„ìš”

**Online softmax:**
```python
# ë¸”ë¡ì”© ì²˜ë¦¬
max_val = -inf
sum_exp = 0

for block in blocks:
    old_max = max_val
    max_val = max(max_val, max(block))
    
    # ì´ì „ ê°’ë“¤ rescale
    sum_exp = sum_exp * exp(old_max - max_val)
    
    # í˜„ì¬ ë¸”ë¡ ì¶”ê°€
    sum_exp += sum(exp(block - max_val))

# ìµœì¢… ì •ê·œí™”
P_row = exp_vals / sum_exp
```

ë¸”ë¡ ë‹¨ìœ„ë¡œ ì²˜ë¦¬ ê°€ëŠ¥!

---

## Flash Attention ì•Œê³ ë¦¬ì¦˜

### Pseudo-code

```python
def flash_attention(Q, K, V, block_size):
    N, d = Q.shape
    O = zeros(N, d)
    l = zeros(N)  # sum of exp (for softmax)
    m = fill(-inf, N)  # max value (for softmax)
    
    # Që¥¼ ë¸”ë¡ìœ¼ë¡œ ë‚˜ëˆ”
    for Q_block in split(Q, block_size):
        # O, l, mì˜ í•´ë‹¹ ë¸”ë¡
        O_block = zeros(block_size, d)
        l_block = zeros(block_size)
        m_block = fill(-inf, block_size)
        
        # K, Vë¥¼ ë¸”ë¡ìœ¼ë¡œ ìˆœíšŒ
        for K_block, V_block in zip(split(K, block_size), split(V, block_size)):
            # Attention scores (SRAMì—ì„œë§Œ)
            S_block = Q_block @ K_block.T / sqrt(d)
            
            # Online softmax update
            m_new = max(m_block, max(S_block, axis=1))
            
            # Rescale ì´ì „ ê°’ë“¤
            scale = exp(m_block - m_new)
            O_block = O_block * scale[:, None]
            l_block = l_block * scale
            
            # í˜„ì¬ ë¸”ë¡ ì¶”ê°€
            P_block = exp(S_block - m_new[:, None])
            O_block += P_block @ V_block
            l_block += sum(P_block, axis=1)
            
            m_block = m_new
        
        # ìµœì¢… ì •ê·œí™”
        O_block = O_block / l_block[:, None]
        
        # ê¸€ë¡œë²Œ ì¶œë ¥ì— ì“°ê¸°
        write_to(O, O_block)
    
    return O
```

### í•µì‹¬ íŠ¸ë¦­

1. **S, P ì €ì¥ ì•ˆ í•¨**: SRAMì—ì„œë§Œ ê³„ì‚°
2. **ë¸”ë¡ ë‹¨ìœ„ ì²˜ë¦¬**: ì‘ì€ ë¸”ë¡ë§Œ SRAMì— ë¡œë“œ
3. **Online softmax**: ë¸”ë¡ì”© softmax ì—…ë°ì´íŠ¸
4. **Rescaling**: ìƒˆ max ê°’ì— ë§ì¶° ì´ì „ ê°’ ì¡°ì •

---

## CUDA êµ¬í˜„ í•µì‹¬

### 1. ë©”ëª¨ë¦¬ ë°°ì¹˜

```cuda
__global__ void flash_attention_kernel(
    const float* Q,  // [batch, heads, N, d]
    const float* K,
    const float* V,
    float* O,
    int N, int d, int block_size
) {
    // Shared memory (SRAM)
    __shared__ float Q_smem[BLOCK_SIZE][HEAD_DIM];
    __shared__ float K_smem[BLOCK_SIZE][HEAD_DIM];
    __shared__ float V_smem[BLOCK_SIZE][HEAD_DIM];
    __shared__ float S_smem[BLOCK_SIZE][BLOCK_SIZE];
    
    // ê° ìŠ¤ë ˆë“œ ë¸”ë¡ì´ Qì˜ í•œ ë¸”ë¡ ì²˜ë¦¬
    int q_block_idx = blockIdx.x;
    
    // Q ë¸”ë¡ì„ shared memoryì— ë¡œë“œ
    load_block_to_smem(Q, Q_smem, q_block_idx);
    
    // ì¶œë ¥ ëˆ„ì ìš©
    float O_local[HEAD_DIM] = {0};
    float l_local = 0.0f;
    float m_local = -INFINITY;
    
    // K, V ë¸”ë¡ë“¤ ìˆœíšŒ
    for (int k_block_idx = 0; k_block_idx < num_k_blocks; k_block_idx++) {
        // K, V ë¸”ë¡ ë¡œë“œ
        load_block_to_smem(K, K_smem, k_block_idx);
        load_block_to_smem(V, V_smem, k_block_idx);
        __syncthreads();
        
        // S = Q @ K^T (shared memoryì—ì„œ)
        compute_attention_scores(Q_smem, K_smem, S_smem);
        __syncthreads();
        
        // Online softmax & output update
        update_output_online(
            S_smem, V_smem,
            O_local, &l_local, &m_local
        );
    }
    
    // ìµœì¢… ì •ê·œí™” & ê¸€ë¡œë²Œ ë©”ëª¨ë¦¬ì— ì“°ê¸°
    normalize_and_write(O, O_local, l_local);
}
```

### 2. Warp-level ìµœì í™”

```cuda
// Warp reduction for max
__device__ float warp_reduce_max(float val) {
    for (int offset = 16; offset > 0; offset /= 2) {
        val = fmaxf(val, __shfl_down_sync(0xffffffff, val, offset));
    }
    return val;
}

// Warp reduction for sum
__device__ float warp_reduce_sum(float val) {
    for (int offset = 16; offset > 0; offset /= 2) {
        val += __shfl_down_sync(0xffffffff, val, offset);
    }
    return val;
}
```

---

## ì‹¤ì „ ì‚¬ìš©: PyTorch

### ì„¤ì¹˜

```bash
pip install flash-attn --no-build-isolation
```

### ê¸°ë³¸ ì‚¬ìš©

```python
import torch
from flash_attn import flash_attn_func

# ì…ë ¥ ì¤€ë¹„
batch_size = 4
num_heads = 32
seq_len = 2048
head_dim = 128

Q = torch.randn(batch_size, seq_len, num_heads, head_dim, device='cuda', dtype=torch.float16)
K = torch.randn(batch_size, seq_len, num_heads, head_dim, device='cuda', dtype=torch.float16)
V = torch.randn(batch_size, seq_len, num_heads, head_dim, device='cuda', dtype=torch.float16)

# Flash Attention
output = flash_attn_func(
    Q, K, V,
    causal=True,  # Causal masking (GPT-style)
    softmax_scale=1.0 / (head_dim ** 0.5)
)

# output: [batch, seq_len, num_heads, head_dim]
```

### Transformer Layerì— í†µí•©

```python
import torch.nn as nn
from flash_attn.modules.mha import FlashSelfAttention

class TransformerBlock(nn.Module):
    def __init__(self, d_model, num_heads):
        super().__init__()
        # Flash Attention ì‚¬ìš©
        self.attn = FlashSelfAttention(
            causal=True,
            softmax_scale=None,  # ìë™ ê³„ì‚°
            attention_dropout=0.1
        )
        
        self.ln1 = nn.LayerNorm(d_model)
        self.ln2 = nn.LayerNorm(d_model)
        self.mlp = nn.Sequential(
            nn.Linear(d_model, 4 * d_model),
            nn.GELU(),
            nn.Linear(4 * d_model, d_model)
        )
    
    def forward(self, x):
        # x: [batch, seq_len, d_model]
        
        # Attention
        attn_out = self.attn(self.ln1(x))
        x = x + attn_out
        
        # MLP
        mlp_out = self.mlp(self.ln2(x))
        x = x + mlp_out
        
        return x
```

### HuggingFace Transformers í†µí•©

```python
from transformers import AutoModelForCausalLM

# Flash Attention ìë™ ì‚¬ìš© (ìµœì‹  ë²„ì „)
model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-2-7b-hf",
    torch_dtype=torch.float16,
    device_map="auto",
    attn_implementation="flash_attention_2"  # ì—¬ê¸°!
)

# ì¶”ë¡ 
inputs = tokenizer("Hello world", return_tensors="pt").to("cuda")
outputs = model.generate(**inputs, max_new_tokens=100)
```

---

## Flash Attention 2

2023ë…„ì— ë‚˜ì˜¨ ê°œì„  ë²„ì „ì…ë‹ˆë‹¤.

### ì£¼ìš” ê°œì„ 

**1. Work partitioning**
- GPU ì›Œí¬ë¡œë“œë¥¼ ë” íš¨ìœ¨ì ìœ¼ë¡œ ë¶„ì‚°
- Warp ë‹¨ìœ„ ë³‘ë ¬í™”

**2. Non-matmul FLOPs ì¤„ì„**
- Softmax, rescaling ìµœì í™”

**3. Low-occupancy ê°œì„ **
- ì‘ì€ ë°°ì¹˜/í—¤ë“œì—ì„œë„ ë¹ ë¦„

### ì„±ëŠ¥ ë¹„êµ

**LLaMA-7B, seq_len=2048:**

| ë²„ì „ | ì†ë„ (ms) | ë©”ëª¨ë¦¬ (GB) |
|------|----------|------------|
| Standard | 45 | 16 |
| Flash Attention 1 | 18 (2.5x) | 2 (8x) |
| Flash Attention 2 | 12 (3.75x) | 2 (8x) |

---

## í•œê³„ì™€ íŠ¸ë ˆì´ë“œì˜¤í”„

### 1. Recomputation ì˜¤ë²„í—¤ë“œ

ForwardëŠ” ë¹ ë¥¸ë°, **BackwardëŠ” ê³„ì‚° 2ë°°**ì…ë‹ˆë‹¤.

```
Standard:
  Forward: S, P ì €ì¥
  Backward: S, P ì½ì–´ì„œ gradient ê³„ì‚°

Flash Attention:
  Forward: S, P ì €ì¥ ì•ˆ í•¨
  Backward: S, P ì¬ê³„ì‚° + gradient ê³„ì‚°
```

**í•˜ì§€ë§Œ:** ë©”ëª¨ë¦¬ I/O ì ˆì•½ì´ ë” ì»¤ì„œ **ì „ì²´ì ìœ¼ë¡  ë¹ ë¦„**

### 2. ê¸´ ì‹œí€€ìŠ¤ì—ì„œë§Œ ë¹›ë‚¨

ì§§ì€ ì‹œí€€ìŠ¤(< 512)ì—ì„œëŠ” ì˜¤ë²„í—¤ë“œê°€ í´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
# ì‹œí€€ìŠ¤ ê¸¸ì´ë³„ ì†ë„ì—…
seq_len=256:   1.2x
seq_len=512:   1.5x
seq_len=1024:  2.0x
seq_len=2048:  3.0x
seq_len=4096:  4.0x
```

### 3. FP16/BF16ë§Œ ì§€ì›

FP32ëŠ” ì§€ì› ì•ˆ ë©ë‹ˆë‹¤. (CUDA ìµœì í™” ë•Œë¬¸)

---

## ê³ ê¸‰ ê¸°ë²•

### 1. Flash Attention + KV Cache

```python
from flash_attn import flash_attn_with_kvcache

# Prefill (ì „ì²´ ì‹œí€€ìŠ¤)
cache_k = torch.empty(batch, seqlen_k, num_heads, head_dim, device='cuda', dtype=torch.float16)
cache_v = torch.empty_like(cache_k)

output = flash_attn_with_kvcache(
    q, k, v,
    cache_k, cache_v,
    cache_seqlens=None,  # ì²˜ìŒ
    causal=True
)

# Decode (ìƒˆ í† í°)
new_q = q[:, -1:, :, :]
new_k = k[:, -1:, :, :]
new_v = v[:, -1:, :, :]

output = flash_attn_with_kvcache(
    new_q, new_k, new_v,
    cache_k, cache_v,
    cache_seqlens=prev_seqlen,  # ì´ì „ ê¸¸ì´
    causal=True
)
```

### 2. Multi-Query Attention (MQA)

```python
# Q: [batch, seq, num_heads, head_dim]
# K, V: [batch, seq, 1, head_dim]  â† 1ê°œ head

output = flash_attn_func(
    Q, K, V,
    causal=True
)
# ìë™ìœ¼ë¡œ K, Vë¥¼ num_headsë§Œí¼ broadcast
```

### 3. Grouped-Query Attention (GQA)

```python
# Q: [batch, seq, 32, 128]  â† 32 heads
# K, V: [batch, seq, 4, 128]  â† 4 groups

output = flash_attn_func(
    Q, K, V,
    causal=True
)
# Qì˜ 8ê°œ headë‹¹ K, Vì˜ 1ê°œ group ì‚¬ìš©
```

---

## ë²¤ì¹˜ë§ˆí¬

### A100 80GB, LLaMA-7B

**Forward pass (ms):**

| Seq Len | Standard | Flash v1 | Flash v2 |
|---------|----------|----------|----------|
| 512 | 8.2 | 6.5 | 5.1 |
| 1024 | 18.5 | 9.2 | 7.3 |
| 2048 | 45.3 | 18.1 | 12.4 |
| 4096 | 125.7 | 42.8 | 28.6 |
| 8192 | OOM | 98.3 | 65.2 |

**ë©”ëª¨ë¦¬ (GB):**

| Seq Len | Standard | Flash |
|---------|----------|-------|
| 2048 | 16 | 2 |
| 4096 | 64 | 4 |
| 8192 | OOM | 8 |

---

## ì‹¤ì „ ì˜ˆì œ: ê¸´ ë¬¸ì„œ ì²˜ë¦¬

```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

# Flash Attention 2 ì‚¬ìš©
model = AutoModelForCausalLM.from_pretrained(
    "mistralai/Mistral-7B-v0.1",
    torch_dtype=torch.float16,
    device_map="auto",
    attn_implementation="flash_attention_2"
)

tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-v0.1")

# ê¸´ ë¬¸ì„œ (16K í† í°)
long_document = """
[ì—¬ê¸°ì— 16,000 ë‹¨ì–´ì§œë¦¬ ë¬¸ì„œ]
"""

prompt = f"""Summarize this document:

{long_document}

Summary:"""

inputs = tokenizer(prompt, return_tensors="pt", truncation=False).to("cuda")

# ì¶”ë¡  (Flash Attention ë•ë¶„ì— ê°€ëŠ¥!)
with torch.no_grad():
    outputs = model.generate(
        **inputs,
        max_new_tokens=500,
        temperature=0.7,
        top_p=0.9
    )

summary = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(summary)
```

Standard attentionì´ë©´ OOM! Flash Attentionì€ ê°€ëŠ¥!

---

## ë‹¤ë¥¸ ìµœì í™” ê¸°ë²•ê³¼ ë¹„êµ

| ê¸°ë²• | ë©”ëª¨ë¦¬ | ì†ë„ | ì •í™•ë„ |
|------|--------|------|--------|
| Standard Attention | 1x | 1x | 100% |
| Flash Attention | 0.1x | 2-4x | 100% |
| Sparse Attention | 0.3x | 1.5x | 98% |
| Linear Attention | 0.05x | 3x | 90% |
| Flash + Sparse | 0.05x | 5x | 98% |

**Flash Attentionì´ ìµœê³ **: ì†ë„, ë©”ëª¨ë¦¬, ì •í™•ë„ ëª¨ë‘ ìš°ìˆ˜!

---

## ë¯¸ë˜: Flash Attention 3

ì—°êµ¬ ì¤‘ì¸ ë°©í–¥:

**1. Asymmetric Attention**
- Queryì™€ Key/Valueë¥¼ ë‹¤ë¥¸ ì •ë°€ë„ë¡œ

**2. Hierarchical Attention**
- ê¸´ ì‹œí€€ìŠ¤ë¥¼ ê³„ì¸µì ìœ¼ë¡œ

**3. Hardware-aware íŠœë‹**
- H100, Hopper ì•„í‚¤í…ì²˜ ìµœì í™”

---

## ìš”ì•½

**Flash Attention**ì€:

1. **O(NÂ²) â†’ O(N)** ë©”ëª¨ë¦¬ ë³µì¡ë„
2. **Tiling + Recomputation** ì „ëµ
3. **Online Softmax** ìŠ¤íŠ¸ë¦¬ë° ê³„ì‚°
4. **SRAM í™œìš©** (HBM íšŒí”¼)

**ê²°ê³¼:**
- ë©”ëª¨ë¦¬: **8-10ë°° ì ˆì•½**
- ì†ë„: **2-4ë°° í–¥ìƒ**
- ê¸´ ì‹œí€€ìŠ¤: **ê°€ëŠ¥** (4K â†’ 64K)

**ì‚¬ìš©ì²˜:**
- ëª¨ë“  Transformer ëª¨ë¸
- ê¸´ ë¬¸ì„œ ì²˜ë¦¬
- ê³ í•´ìƒë„ ì´ë¯¸ì§€ (Vision Transformer)
- ë©”ëª¨ë¦¬ ì œì•½ í™˜ê²½

**í•µì‹¬**: ì•Œê³ ë¦¬ì¦˜ + í•˜ë“œì›¨ì–´ ì´í•´ = ê·¹ì  ì„±ëŠ¥ í–¥ìƒ!

---

## ë‹¤ìŒ ê¸€

**8í¸: Speculative Decoding**
- ì¶”ë¡  ì†ë„ 2-3ë°° í–¥ìƒ
- ì‘ì€ ëª¨ë¸ë¡œ í° ëª¨ë¸ ê°€ì†
- ë¬´ì†ì‹¤ ìµœì í™”

ê¸°ëŒ€í•´ì£¼ì„¸ìš”! ğŸš€

---

*ì§ˆë¬¸ì´ë‚˜ í”¼ë“œë°±ì€ [GitHub](https://github.com/yhna941)ì—ì„œ í™˜ì˜í•©ë‹ˆë‹¤!*
