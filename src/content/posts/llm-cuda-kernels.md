---
title: "LLM Inference ìµœì í™” #11: CUDA Kernel ìµœì í™” - ì§ì ‘ ì§œëŠ” ê³ ì„±ëŠ¥ ì—°ì‚°"
description: "PyTorchë³´ë‹¤ 10ë°° ë¹ ë¥¸ custom CUDA kernelì„ ì‘ì„±í•˜ëŠ” ë°©ë²•ê³¼ ë©”ëª¨ë¦¬/warp ìµœì í™” ê¸°ë²•ì„ ì•Œì•„ë´…ë‹ˆë‹¤."
pubDate: 2026-02-06
author: "Yh Na"
tags: ["llm", "cuda", "optimization", "kernel", "gpu"]
draft: false
---

# LLM Inference ìµœì í™” #11: CUDA Kernel ìµœì í™”

PyTorchëŠ” í¸ë¦¬í•˜ì§€ë§Œ **í•­ìƒ ìµœì ì€ ì•„ë‹™ë‹ˆë‹¤**. íŠ¹íˆ custom ì—°ì‚°ì€ ëŠë¦½ë‹ˆë‹¤.

**Custom CUDA kernel**ì„ ì§ì ‘ ì§œë©´:
- **10-100ë°° ë¹ ë¥¼ ìˆ˜ ìˆìŒ**
- ë©”ëª¨ë¦¬ íš¨ìœ¨ ê·¹ëŒ€í™”
- í•˜ë“œì›¨ì–´ ìµœëŒ€ í™œìš©

Flash Attention, FasterTransformer ëª¨ë‘ custom kernelì…ë‹ˆë‹¤.

ì´ë²ˆ ê¸€ì—ì„œ **ì§ì ‘ ì§œë³´ê² ìŠµë‹ˆë‹¤**!

---

## GPU ì•„í‚¤í…ì²˜ ì´í•´

### ë©”ëª¨ë¦¬ ê³„ì¸µ

```
Global Memory (HBM):
  - í¬ê¸°: 80 GB
  - ì†ë„: 1.5 TB/s
  - ë ˆì´í„´ì‹œ: 400-800 cycles

Shared Memory (SRAM):
  - í¬ê¸°: 164 KB per SM
  - ì†ë„: 19 TB/s
  - ë ˆì´í„´ì‹œ: ~20 cycles

Registers:
  - í¬ê¸°: 256 KB per SM
  - ì†ë„: ìµœê³ 
  - ë ˆì´í„´ì‹œ: 1 cycle
```

**í•µì‹¬:** Global memoryëŠ” ëŠë¦¼! Shared memory í™œìš©ì´ í•„ìˆ˜.

### Execution ëª¨ë¸

```
Grid (ì „ì²´ ì‘ì—…)
â”œâ”€â”€ Block 0 (SM 0ì—ì„œ ì‹¤í–‰)
â”‚   â”œâ”€â”€ Warp 0 (32 threads)
â”‚   â”œâ”€â”€ Warp 1 (32 threads)
â”‚   â””â”€â”€ ...
â”œâ”€â”€ Block 1 (SM 1ì—ì„œ ì‹¤í–‰)
â””â”€â”€ ...

Thread hierarchy:
- Grid: ìˆ˜ì²œ-ìˆ˜ë§Œ blocks
- Block: 128-1024 threads
- Warp: 32 threads (SIMT)
```

---

## ì²« CUDA Kernel: Vector Add

### Naive êµ¬í˜„

```cuda
// vector_add.cu
__global__ void vector_add(float* a, float* b, float* c, int n) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    
    if (idx < n) {
        c[idx] = a[idx] + b[idx];
    }
}

// í˜¸ì¶œ
int n = 1000000;
int threads = 256;
int blocks = (n + threads - 1) / threads;

vector_add<<<blocks, threads>>>(d_a, d_b, d_c, n);
```

**ì„±ëŠ¥:** ~500 GB/s (ì´ë¡ ì¹˜ 1.5 TB/sì˜ 33%)

### ì™œ ëŠë¦´ê¹Œ?

**ë¬¸ì œ 1: Uncoalesced memory access**
```
Thread 0: a[0]
Thread 1: a[1024]  â† ì—°ì† ì•ˆ ë¨!
Thread 2: a[2048]
```

**í•´ê²°:** Threadsê°€ ì—°ì† ë©”ëª¨ë¦¬ ì ‘ê·¼
```
Thread 0: a[0]
Thread 1: a[1]    â† ì—°ì†!
Thread 2: a[2]
```

---

## Coalesced Memory Access

### ìµœì í™” ë²„ì „

```cuda
__global__ void vector_add_coalesced(float* a, float* b, float* c, int n) {
    // Coalesced access
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    int stride = blockDim.x * gridDim.x;
    
    // Grid-stride loop
    for (int i = idx; i < n; i += stride) {
        c[i] = a[i] + b[i];
    }
}
```

**ì„±ëŠ¥:** ~1.2 TB/s (ì´ë¡ ì¹˜ì˜ 80%) âœ…

### ê·œì¹™

1. **ì—°ì† threadsê°€ ì—°ì† ë©”ëª¨ë¦¬ ì ‘ê·¼**
2. **128-byte aligned** (32 floats Ã— 4 bytes)
3. **32 threads (warp) ë‹¨ìœ„ë¡œ access**

---

## Matrix Multiplication (GEMM)

ê°€ì¥ ì¤‘ìš”í•œ ì—°ì‚°! LLMì˜ 90% ì‹œê°„.

### Naive êµ¬í˜„

```cuda
__global__ void matmul_naive(
    const float* A,  // [M, K]
    const float* B,  // [K, N]
    float* C,        // [M, N]
    int M, int K, int N
) {
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    int col = blockIdx.x * blockDim.x + threadIdx.x;
    
    if (row < M && col < N) {
        float sum = 0.0f;
        for (int k = 0; k < K; k++) {
            sum += A[row * K + k] * B[k * N + col];
        }
        C[row * N + col] = sum;
    }
}
```

**ë¬¸ì œ:**
- Global memory ì ‘ê·¼ ë§ìŒ (Kë²ˆ)
- ì¬ì‚¬ìš© ì—†ìŒ

**ì„±ëŠ¥:** ~50 GFLOPS (ì´ë¡ ì¹˜ 19,500 GFLOPSì˜ 0.25%!)

### Tiled GEMM (Shared Memory)

```cuda
#define TILE_SIZE 32

__global__ void matmul_tiled(
    const float* A, const float* B, float* C,
    int M, int K, int N
) {
    // Shared memory (íƒ€ì¼)
    __shared__ float As[TILE_SIZE][TILE_SIZE];
    __shared__ float Bs[TILE_SIZE][TILE_SIZE];
    
    int row = blockIdx.y * TILE_SIZE + threadIdx.y;
    int col = blockIdx.x * TILE_SIZE + threadIdx.x;
    
    float sum = 0.0f;
    
    // íƒ€ì¼ ë‹¨ìœ„ë¡œ ìˆœíšŒ
    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; t++) {
        // 1. Global â†’ Shared (coalesced!)
        if (row < M && t * TILE_SIZE + threadIdx.x < K) {
            As[threadIdx.y][threadIdx.x] = A[row * K + t * TILE_SIZE + threadIdx.x];
        } else {
            As[threadIdx.y][threadIdx.x] = 0.0f;
        }
        
        if (col < N && t * TILE_SIZE + threadIdx.y < K) {
            Bs[threadIdx.y][threadIdx.x] = B[(t * TILE_SIZE + threadIdx.y) * N + col];
        } else {
            Bs[threadIdx.y][threadIdx.x] = 0.0f;
        }
        
        __syncthreads();  // ëª¨ë“  threads ëŒ€ê¸°
        
        // 2. Shared memoryì—ì„œ ê³„ì‚° (ë¹ ë¦„!)
        for (int k = 0; k < TILE_SIZE; k++) {
            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];
        }
        
        __syncthreads();  // ë‹¤ìŒ íƒ€ì¼ ì „ ëŒ€ê¸°
    }
    
    // 3. ê²°ê³¼ ì“°ê¸°
    if (row < M && col < N) {
        C[row * N + col] = sum;
    }
}
```

**ìµœì í™”:**
- Global memory ì ‘ê·¼: Kë²ˆ â†’ K/TILE_SIZEë²ˆ
- Shared memory ì¬ì‚¬ìš©

**ì„±ëŠ¥:** ~1,000 GFLOPS (ì´ë¡ ì¹˜ì˜ 5%)

### Further ìµœì í™”: Register Tiling

```cuda
#define BM 128  // Block tile M
#define BN 128  // Block tile N
#define BK 8    // Block tile K
#define TM 8    // Thread tile M
#define TN 8    // Thread tile N

__global__ void matmul_optimized(
    const float* A, const float* B, float* C,
    int M, int K, int N
) {
    // Shared memory
    __shared__ float As[BM][BK];
    __shared__ float Bs[BK][BN];
    
    // Register tiling (ê° threadê°€ TMÃ—TN ë‹´ë‹¹)
    float thread_results[TM][TN] = {0};
    float reg_a[TM];
    float reg_b[TN];
    
    int thread_row = threadIdx.y;
    int thread_col = threadIdx.x;
    int block_row = blockIdx.y * BM;
    int block_col = blockIdx.x * BN;
    
    // íƒ€ì¼ ìˆœíšŒ
    for (int k_block = 0; k_block < K; k_block += BK) {
        // Load A tile to shared memory
        for (int i = 0; i < BM; i += blockDim.y) {
            for (int j = 0; j < BK; j += blockDim.x) {
                int row = block_row + i + thread_row;
                int col = k_block + j + thread_col;
                if (row < M && col < K) {
                    As[i + thread_row][j + thread_col] = A[row * K + col];
                }
            }
        }
        
        // Load B tile to shared memory
        for (int i = 0; i < BK; i += blockDim.y) {
            for (int j = 0; j < BN; j += blockDim.x) {
                int row = k_block + i + thread_row;
                int col = block_col + j + thread_col;
                if (row < K && col < N) {
                    Bs[i + thread_row][j + thread_col] = B[row * N + col];
                }
            }
        }
        
        __syncthreads();
        
        // Compute (registerì—ì„œ!)
        for (int k = 0; k < BK; k++) {
            // Load from shared to registers
            for (int i = 0; i < TM; i++) {
                reg_a[i] = As[thread_row * TM + i][k];
            }
            for (int j = 0; j < TN; j++) {
                reg_b[j] = Bs[k][thread_col * TN + j];
            }
            
            // Outer product
            for (int i = 0; i < TM; i++) {
                for (int j = 0; j < TN; j++) {
                    thread_results[i][j] += reg_a[i] * reg_b[j];
                }
            }
        }
        
        __syncthreads();
    }
    
    // Write results
    for (int i = 0; i < TM; i++) {
        for (int j = 0; j < TN; j++) {
            int row = block_row + thread_row * TM + i;
            int col = block_col + thread_col * TN + j;
            if (row < M && col < N) {
                C[row * N + col] = thread_results[i][j];
            }
        }
    }
}
```

**ì„±ëŠ¥:** ~8,000 GFLOPS (ì´ë¡ ì¹˜ì˜ 41%)

cuBLASëŠ” ~15,000 GFLOPS (77%)ê¹Œì§€ ë‚˜ì˜´!

---

## Warp-level Primitives

### Warp Reduction

```cuda
__device__ float warp_reduce_sum(float val) {
    /**
     * Warp ë‚´ ëª¨ë“  threadsì˜ í•©
     * Shuffle instruction ì‚¬ìš© (ë§¤ìš° ë¹ ë¦„!)
     */
    #pragma unroll
    for (int offset = 16; offset > 0; offset /= 2) {
        val += __shfl_down_sync(0xffffffff, val, offset);
    }
    return val;
}

__global__ void vector_sum(const float* input, float* output, int n) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    int lane = threadIdx.x % 32;  // Warp lane
    
    // ê° threadê°€ ê°’ ì½ê¸°
    float val = (idx < n) ? input[idx] : 0.0f;
    
    // Warp reduction
    val = warp_reduce_sum(val);
    
    // Warpì˜ ì²« threadë§Œ ê²°ê³¼ ì“°ê¸°
    if (lane == 0) {
        atomicAdd(output, val);
    }
}
```

### Warp-level Matrix Multiply

```cuda
#include <mma.h>
using namespace nvcuda;

__global__ void wmma_gemm(
    const half* A, const half* B, float* C,
    int M, int N, int K
) {
    // Tensor Core ì‚¬ìš©!
    // 16Ã—16Ã—16 matrix multiply in 1 instruction
    
    wmma::fragment<wmma::matrix_a, 16, 16, 16, half, wmma::row_major> a_frag;
    wmma::fragment<wmma::matrix_b, 16, 16, 16, half, wmma::col_major> b_frag;
    wmma::fragment<wmma::accumulator, 16, 16, 16, float> c_frag;
    
    // Initialize
    wmma::fill_fragment(c_frag, 0.0f);
    
    int warp_row = (blockIdx.y * blockDim.y + threadIdx.y) / 32;
    int warp_col = (blockIdx.x * blockDim.x + threadIdx.x) / 32;
    
    // Tile loop
    for (int i = 0; i < K; i += 16) {
        // Load
        wmma::load_matrix_sync(a_frag, A + warp_row * 16 * K + i, K);
        wmma::load_matrix_sync(b_frag, B + i * N + warp_col * 16, N);
        
        // Compute (Tensor Core!)
        wmma::mma_sync(c_frag, a_frag, b_frag, c_frag);
    }
    
    // Store
    wmma::store_matrix_sync(C + warp_row * 16 * N + warp_col * 16, c_frag, N, wmma::mem_row_major);
}
```

**Tensor Core ì„±ëŠ¥:** ~19,500 GFLOPS (100%!)

---

## Attention Kernel

Flash Attention ê°„ë‹¨ ë²„ì „:

```cuda
__global__ void simple_attention_kernel(
    const float* Q,  // [batch, heads, seq, head_dim]
    const float* K,
    const float* V,
    float* O,
    int batch, int heads, int seq_len, int head_dim
) {
    __shared__ float Q_shared[32][64];
    __shared__ float K_shared[32][64];
    __shared__ float V_shared[32][64];
    
    int batch_idx = blockIdx.z;
    int head_idx = blockIdx.y;
    int q_idx = blockIdx.x;
    int tid = threadIdx.x;
    
    // Load Q (í˜„ì¬ query)
    if (tid < head_dim) {
        int offset = ((batch_idx * heads + head_idx) * seq_len + q_idx) * head_dim;
        Q_shared[0][tid] = Q[offset + tid];
    }
    __syncthreads();
    
    float max_score = -INFINITY;
    float sum_exp = 0.0f;
    float output[64] = {0};
    
    // K, Vë¥¼ ë¸”ë¡ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
    for (int k_block = 0; k_block < seq_len; k_block += 32) {
        // Load K, V blocks
        for (int i = 0; i < 32; i++) {
            if (k_block + i < seq_len && tid < head_dim) {
                int offset = ((batch_idx * heads + head_idx) * seq_len + k_block + i) * head_dim;
                K_shared[i][tid] = K[offset + tid];
                V_shared[i][tid] = V[offset + tid];
            }
        }
        __syncthreads();
        
        // Compute attention scores
        for (int i = 0; i < 32 && k_block + i < seq_len; i++) {
            float score = 0.0f;
            for (int d = 0; d < head_dim; d++) {
                score += Q_shared[0][d] * K_shared[i][d];
            }
            score /= sqrtf((float)head_dim);
            
            // Online softmax
            float old_max = max_score;
            max_score = fmaxf(max_score, score);
            float exp_score = expf(score - max_score);
            
            // Rescale previous
            float scale = expf(old_max - max_score);
            sum_exp = sum_exp * scale + exp_score;
            for (int d = 0; d < head_dim; d++) {
                output[d] = output[d] * scale + exp_score * V_shared[i][d];
            }
        }
        __syncthreads();
    }
    
    // Normalize & write
    if (tid < head_dim) {
        int offset = ((batch_idx * heads + head_idx) * seq_len + q_idx) * head_dim;
        O[offset + tid] = output[tid] / sum_exp;
    }
}
```

---

## PyTorch í†µí•©

### C++ Extension

```cpp
// attention.cpp
#include <torch/extension.h>

torch::Tensor attention_forward(
    torch::Tensor Q,
    torch::Tensor K,
    torch::Tensor V
) {
    auto O = torch::empty_like(Q);
    
    const int batch = Q.size(0);
    const int heads = Q.size(1);
    const int seq_len = Q.size(2);
    const int head_dim = Q.size(3);
    
    dim3 blocks(seq_len, heads, batch);
    dim3 threads(head_dim);
    
    simple_attention_kernel<<<blocks, threads>>>(
        Q.data_ptr<float>(),
        K.data_ptr<float>(),
        V.data_ptr<float>(),
        O.data_ptr<float>(),
        batch, heads, seq_len, head_dim
    );
    
    return O;
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def("forward", &attention_forward, "Attention forward");
}
```

### setup.py

```python
from setuptools import setup
from torch.utils.cpp_extension import BuildExtension, CUDAExtension

setup(
    name='attention_cuda',
    ext_modules=[
        CUDAExtension(
            'attention_cuda',
            ['attention.cpp', 'attention_kernel.cu'],
            extra_compile_args={'cxx': ['-O3'],
                              'nvcc': ['-O3', '--use_fast_math']}
        )
    ],
    cmdclass={'build_ext': BuildExtension}
)
```

### ì‚¬ìš©

```python
import torch
import attention_cuda

Q = torch.randn(32, 8, 512, 64, device='cuda')
K = torch.randn(32, 8, 512, 64, device='cuda')
V = torch.randn(32, 8, 512, 64, device='cuda')

# Custom kernel
output = attention_cuda.forward(Q, K, V)

# PyTorch (ë¹„êµ)
output_torch = torch.nn.functional.scaled_dot_product_attention(Q, K, V)

# Speed comparison
import time

start = time.time()
for _ in range(100):
    _ = attention_cuda.forward(Q, K, V)
torch.cuda.synchronize()
custom_time = time.time() - start

start = time.time()
for _ in range(100):
    _ = torch.nn.functional.scaled_dot_product_attention(Q, K, V)
torch.cuda.synchronize()
torch_time = time.time() - start

print(f"Custom: {custom_time:.3f}s")
print(f"PyTorch: {torch_time:.3f}s")
print(f"Speedup: {torch_time/custom_time:.2f}x")
```

---

## ìµœì í™” ì²´í¬ë¦¬ìŠ¤íŠ¸

### 1. Memory Access

- [ ] Coalesced access (ì—°ì† threads â†’ ì—°ì† ë©”ëª¨ë¦¬)
- [ ] Shared memory í™œìš© (Global ì ‘ê·¼ ìµœì†Œí™”)
- [ ] Bank conflict ì—†ìŒ (Shared memory)
- [ ] Register spilling ì—†ìŒ

### 2. Compute

- [ ] Warp í™œìš©ë¥  > 80%
- [ ] Occupancy > 50%
- [ ] Divergence ìµœì†Œí™” (if-else ì ê²Œ)
- [ ] Math intrinsics (__float2half, __expf)

### 3. Parallelism

- [ ] Blocks ì¶©ë¶„ (SMs ì±„ìš°ê¸°)
- [ ] Threads per block: 128-512
- [ ] Work per thread: ì ë‹¹ (ë„ˆë¬´ ë§ê±°ë‚˜ ì ì§€ ì•Šê²Œ)

---

## Profiling

### NVIDIA Nsight

```bash
# Profiling
nsys profile --stats=true python script.py

# Compute profiling
ncu --target-processes all python script.py
```

### ì£¼ìš” ë©”íŠ¸ë¦­

```
Achieved Occupancy: 75%  âœ… (> 50% ëª©í‘œ)
Memory Throughput: 1200 GB/s  âœ… (ì´ë¡ ì¹˜ì˜ 80%)
Compute Throughput: 12000 GFLOPS  âš ï¸ (ì´ë¡ ì¹˜ì˜ 60%)

Recommendations:
- Increase occupancy: More blocks or fewer registers
- Reduce bank conflicts: Padding shared memory
```

---

## ë²¤ì¹˜ë§ˆí¬

### Matrix Multiply (M=N=K=4096, FP16)

| êµ¬í˜„ | GFLOPS | ì´ë¡ ì¹˜ ëŒ€ë¹„ |
|------|--------|-----------|
| Naive CUDA | 50 | 0.3% |
| Tiled (Shared) | 1,000 | 5% |
| Register Tiling | 8,000 | 41% |
| Tensor Core (WMMA) | 19,500 | 100% |
| cuBLAS | 19,500 | 100% |

### Attention (seq=2048, batch=32, heads=32)

| êµ¬í˜„ | Time (ms) | Memory (GB) |
|------|----------|------------|
| PyTorch | 45 | 16 |
| Flash Attention v1 | 18 | 2 |
| Flash Attention v2 | 12 | 2 |

---

## ì‹¤ì „ íŒ

### 1. ë¨¼ì € Naive, ê·¸ ë‹¤ìŒ ìµœì í™”

```cuda
// Step 1: ì‘ë™í•˜ëŠ” ë²„ì „
__global__ void kernel_v1(...) {
    // Simple implementation
}

// Step 2: Shared memory
__global__ void kernel_v2(...) {
    __shared__ float smem[...];
    // ...
}

// Step 3: Register tiling
__global__ void kernel_v3(...) {
    float registers[...];
    // ...
}
```

### 2. Profile-Guided

```python
# ë³‘ëª© ì°¾ê¸°
with torch.profiler.profile(with_stack=True) as prof:
    model(input)

print(prof.key_averages().table())

# ê°€ì¥ ëŠë¦° ì—°ì‚°ë¶€í„° ìµœì í™”!
```

### 3. Numerical Precision

```cuda
// Fast math (ì •í™•ë„ ì•½ê°„ ì†ì‹¤)
__global__ void kernel() {
    float x = __expf(y);  // ë¹ ë¦„
    // vs
    float x = expf(y);    // ì •í™•
}

// Compile with: -use_fast_math
```

---

## ìš”ì•½

**CUDA Kernel ìµœì í™”**ëŠ”:

1. **Memory ê³„ì¸µ ì´í•´** (Global â†’ Shared â†’ Register)
2. **Coalesced access** (ì—°ì† ë©”ëª¨ë¦¬)
3. **Tiling** (ì¬ì‚¬ìš© ê·¹ëŒ€í™”)
4. **Warp primitives** (Shuffle, Tensor Core)
5. **Profile & Iterate**

**ì„±ëŠ¥ í–¥ìƒ:**
- Naive â†’ Optimized: **10-100ë°°**
- PyTorch â†’ Custom: **2-10ë°°**

**ì‚¬ìš©ì²˜:**
- í•µì‹¬ ì—°ì‚° (GEMM, Attention)
- ë¼ì´ë¸ŒëŸ¬ë¦¬ì— ì—†ëŠ” ì—°ì‚°
- ê·¹í•œ ìµœì í™” í•„ìš” ì‹œ

**ì¶”ì²œ:**
- ë¨¼ì € cuBLAS, cuDNN ì‚¬ìš©
- ì •ë§ í•„ìš”í•  ë•Œë§Œ custom kernel
- Triton (Python DSL) ê³ ë ¤

---

## ì‹œë¦¬ì¦ˆ ì™„ê²°! ğŸ‰

**LLM Inference ìµœì í™” ì™„ì „ ì •ë³µ (1-14í¸):**

**ë©”ëª¨ë¦¬ ìµœì í™”:**
1. Paged Attention
2. KV Caching
7. Model Quantization
10. Model Compression

**ì†ë„ ìµœì í™”:**
4. Flash Attention
5. Speculative Decoding
6. Continuous Batching
11. CUDA Kernels

**ë¶„ì‚° í•™ìŠµ:**
8. Tensor Parallelism
9. Pipeline Parallelism

**Fine-tuning:**
3. LoRA
7. QLoRA

**ì¡°í•©í•˜ë©´:**
- ë©”ëª¨ë¦¬: **100ë°° ì ˆê°** (Paged + Quantization + Compression)
- ì†ë„: **100ë°° í–¥ìƒ** (Flash + Speculative + Continuous + CUDA)
- í•™ìŠµ: **24GBë¡œ 70B í•™ìŠµ** (QLoRA)
- ì¶”ë¡ : **ë‹¨ì¼ GPUë¡œ 70B** (Quantization)

ì´ì œ ì—¬ëŸ¬ë¶„ì€ **LLM ìµœì í™” ì „ë¬¸ê°€**ì…ë‹ˆë‹¤! ğŸš€

---

*ì§ˆë¬¸ì´ë‚˜ í”¼ë“œë°±ì€ [GitHub](https://github.com/yhna941)ì—ì„œ í™˜ì˜í•©ë‹ˆë‹¤!*

*ì‹œë¦¬ì¦ˆê°€ ë„ì›€ì´ ë˜ì…¨ë‹¤ë©´ â­ Star ë¶€íƒë“œë¦½ë‹ˆë‹¤!*
