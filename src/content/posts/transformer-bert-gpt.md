---
title: "Transformer #3: BERT & GPT - Pre-trainingì˜ í˜ëª…"
description: "Encoder-only BERTì™€ Decoder-only GPTì˜ êµ¬ì¡°, Pre-training ì „ëµ, ê·¸ë¦¬ê³  Fine-tuningì„ ì™„ì „íˆ ì´í•´í•©ë‹ˆë‹¤."
pubDate: 2026-02-06
author: "Yh Na"
tags: ["transformer", "bert", "gpt", "pre-training", "nlp", "pytorch"]
draft: false
---

# Transformer #3: BERT & GPT

**"Pre-training + Fine-tuning = New Paradigm"**

2018ë…„, NLPì˜ íŒ¨ëŸ¬ë‹¤ì„ì´ ì™„ì „íˆ ë°”ë€Œì—ˆìŠµë‹ˆë‹¤.

ì´ë²ˆ ê¸€:
- BERT (Encoder-only)
- GPT (Decoder-only)
- Pre-training ì „ëµ
- Fine-tuning

---

## Pre-trainingì˜ ë“±ì¥

### ì´ì „: Task-specific í•™ìŠµ

```
Machine Translation: ë²ˆì—­ ë°ì´í„°ë¡œ í•™ìŠµ
Sentiment Analysis: ê°ì • ë°ì´í„°ë¡œ í•™ìŠµ
QA: QA ë°ì´í„°ë¡œ í•™ìŠµ

ë¬¸ì œ:
- ê° íƒœìŠ¤í¬ë§ˆë‹¤ ë°ì´í„° í•„ìš”
- ì²˜ìŒë¶€í„° í•™ìŠµ (ëŠë¦¼)
- ì‘ì€ ë°ì´í„°ì…‹ (overfitting)
```

### ìƒˆë¡œìš´ íŒ¨ëŸ¬ë‹¤ì„: Transfer Learning

```
1. Pre-training (ëŒ€ëŸ‰ì˜ unlabeled data)
   â†’ ì–¸ì–´ì˜ ì¼ë°˜ì  ì§€ì‹ í•™ìŠµ

2. Fine-tuning (ì†ŒëŸ‰ì˜ labeled data)
   â†’ íŠ¹ì • íƒœìŠ¤í¬ì— ì ì‘

ê²°ê³¼:
- ì ì€ ë°ì´í„°ë¡œ ë†’ì€ ì„±ëŠ¥
- ë¹ ë¥¸ í•™ìŠµ
- ë²”ìš© í‘œí˜„ í•™ìŠµ
```

**ë¹„ìœ : ëŒ€í•™ êµìœ¡**

```
Pre-training = ì¼ë°˜ êµìœ¡ (ìˆ˜í•™, ê³¼í•™, ì–¸ì–´...)
Fine-tuning = ì „ê³µ êµìœ¡ (ì˜í•™, ê³µí•™, ë²•í•™...)

ì¼ë°˜ êµìœ¡ ì—†ì´ ì „ê³µë§Œ? ë¹„íš¨ìœ¨!
```

---

## BERT (2018)

**Bidirectional Encoder Representations from Transformers**

### ì•„í‚¤í…ì²˜

**Encoder-only Transformer:**

```
Input
  â†“
Embedding + Positional Encoding
  â†“
Encoder Ã— 12 (Base) or 24 (Large)
  â†“
Output (Contextualized Embeddings)
```

**í¬ê¸°:**

```
BERT-Base:
- Layers: 12
- Hidden: 768
- Heads: 12
- Parameters: 110M

BERT-Large:
- Layers: 24
- Hidden: 1024
- Heads: 16
- Parameters: 340M
```

### Pre-training Tasks

#### 1. Masked Language Model (MLM)

**ì•„ì´ë””ì–´:**

> "ë¬¸ì¥ì˜ ì¼ë¶€ë¥¼ ê°€ë¦¬ê³  ë§ì¶”ê¸°"

```
Input: "I love [MASK] learning"
Label: "deep"

Input: "The [MASK] is [MASK] the street"
Label: "animal", "crossing"
```

**ë°©ë²•:**

1. 15% í† í° ì„ íƒ
   - 80%: [MASK]ë¡œ ëŒ€ì²´
   - 10%: ëœë¤ í† í°ìœ¼ë¡œ ëŒ€ì²´
   - 10%: ê·¸ëŒ€ë¡œ (unchanged)

2. ëª¨ë¸ì´ ì›ë˜ í† í° ì˜ˆì¸¡

**ì™œ ëœë¤/unchanged?**

```
Fine-tuning ì‹œì—ëŠ” [MASK]ê°€ ì—†ìŒ!
â†’ [MASK]ì—ë§Œ ì˜ì¡´í•˜ì§€ ì•Šë„ë¡
```

**êµ¬í˜„:**

```python
import torch
import torch.nn as nn
import random

class MLMDataset:
    def __init__(self, texts, tokenizer, mask_prob=0.15):
        self.texts = texts
        self.tokenizer = tokenizer
        self.mask_prob = mask_prob
        self.mask_token_id = tokenizer.mask_token_id
        self.vocab_size = tokenizer.vocab_size
    
    def __getitem__(self, idx):
        text = self.texts[idx]
        tokens = self.tokenizer.encode(text)
        
        # Create labels (copy of tokens)
        labels = tokens.copy()
        
        # Mask tokens
        for i in range(len(tokens)):
            if random.random() < self.mask_prob:
                rand = random.random()
                
                if rand < 0.8:
                    # 80%: Replace with [MASK]
                    tokens[i] = self.mask_token_id
                elif rand < 0.9:
                    # 10%: Replace with random token
                    tokens[i] = random.randint(0, self.vocab_size - 1)
                # 10%: Keep original (do nothing)
            else:
                # Not masked: ignore in loss
                labels[i] = -100  # PyTorch ignore_index
        
        return torch.tensor(tokens), torch.tensor(labels)
```

#### 2. Next Sentence Prediction (NSP)

**ì•„ì´ë””ì–´:**

> "ë‘ ë¬¸ì¥ì´ ì´ì–´ì§€ëŠ”ê°€?"

```
Input: [CLS] Sentence A [SEP] Sentence B [SEP]

Example 1 (IsNext):
A: "I love deep learning."
B: "It's very interesting."
Label: 1

Example 2 (NotNext):
A: "I love deep learning."
B: "The sky is blue."
Label: 0
```

**ë°ì´í„° ìƒì„±:**

```python
def create_nsp_data(documents):
    examples = []
    
    for doc in documents:
        sentences = doc.split('.')
        
        for i in range(len(sentences) - 1):
            # 50%: IsNext
            if random.random() < 0.5:
                sent_a = sentences[i]
                sent_b = sentences[i + 1]
                label = 1
            # 50%: NotNext
            else:
                sent_a = sentences[i]
                sent_b = random.choice(sentences)
                label = 0
            
            examples.append((sent_a, sent_b, label))
    
    return examples
```

### BERT ëª¨ë¸

```python
class BERT(nn.Module):
    def __init__(self, vocab_size, d_model=768, n_heads=12, n_layers=12, d_ff=3072, max_len=512):
        super().__init__()
        
        # Embeddings
        self.token_embedding = nn.Embedding(vocab_size, d_model)
        self.position_embedding = nn.Embedding(max_len, d_model)
        self.segment_embedding = nn.Embedding(2, d_model)  # For NSP
        
        # Encoder
        self.encoder = Encoder(
            vocab_size=vocab_size,
            d_model=d_model,
            n_heads=n_heads,
            d_ff=d_ff,
            n_layers=n_layers
        )
        
        # MLM head
        self.mlm_head = nn.Linear(d_model, vocab_size)
        
        # NSP head
        self.nsp_head = nn.Linear(d_model, 2)
    
    def forward(self, input_ids, segment_ids, attention_mask=None):
        """
        input_ids: (batch, seq_len)
        segment_ids: (batch, seq_len) - 0 for sent A, 1 for sent B
        attention_mask: (batch, seq_len)
        """
        batch_size, seq_len = input_ids.size()
        
        # Position IDs
        position_ids = torch.arange(seq_len).unsqueeze(0).expand(batch_size, -1)
        
        # Embeddings
        token_emb = self.token_embedding(input_ids)
        position_emb = self.position_embedding(position_ids)
        segment_emb = self.segment_embedding(segment_ids)
        
        embeddings = token_emb + position_emb + segment_emb
        
        # Encode
        encoder_output = self.encoder(embeddings, attention_mask)
        
        # MLM predictions (all tokens)
        mlm_logits = self.mlm_head(encoder_output)
        
        # NSP prediction ([CLS] token)
        cls_output = encoder_output[:, 0]  # First token
        nsp_logits = self.nsp_head(cls_output)
        
        return mlm_logits, nsp_logits

# ì‚¬ìš©
model = BERT(vocab_size=30000)

input_ids = torch.randint(0, 30000, (32, 128))
segment_ids = torch.cat([
    torch.zeros(32, 64),
    torch.ones(32, 64)
], dim=1).long()

mlm_logits, nsp_logits = model(input_ids, segment_ids)
print(mlm_logits.shape)  # (32, 128, 30000)
print(nsp_logits.shape)  # (32, 2)
```

### Fine-tuning

**ë‹¤ì–‘í•œ íƒœìŠ¤í¬:**

```python
class BERTForClassification(nn.Module):
    """Sentiment analysis, topic classification, etc."""
    def __init__(self, bert_model, num_classes):
        super().__init__()
        self.bert = bert_model
        self.classifier = nn.Linear(768, num_classes)
    
    def forward(self, input_ids, segment_ids, attention_mask):
        # Get [CLS] representation
        encoder_output = self.bert.encoder(input_ids, attention_mask)
        cls_output = encoder_output[:, 0]
        
        # Classify
        logits = self.classifier(cls_output)
        return logits

class BERTForQA(nn.Module):
    """Question Answering (SQuAD)"""
    def __init__(self, bert_model):
        super().__init__()
        self.bert = bert_model
        self.qa_outputs = nn.Linear(768, 2)  # Start & End
    
    def forward(self, input_ids, segment_ids, attention_mask):
        # Encode
        encoder_output = self.bert.encoder(input_ids, attention_mask)
        
        # Predict start & end positions
        logits = self.qa_outputs(encoder_output)
        start_logits, end_logits = logits.split(1, dim=-1)
        
        return start_logits.squeeze(-1), end_logits.squeeze(-1)

class BERTForNER(nn.Module):
    """Named Entity Recognition"""
    def __init__(self, bert_model, num_labels):
        super().__init__()
        self.bert = bert_model
        self.classifier = nn.Linear(768, num_labels)
    
    def forward(self, input_ids, segment_ids, attention_mask):
        # Encode (get all token representations)
        encoder_output = self.bert.encoder(input_ids, attention_mask)
        
        # Classify each token
        logits = self.classifier(encoder_output)
        return logits
```

---

## GPT (2018)

**Generative Pre-trained Transformer**

### ì•„í‚¤í…ì²˜

**Decoder-only Transformer:**

```
Input
  â†“
Embedding + Positional Encoding
  â†“
Masked Decoder Ã— 12
  â†“
Language Model Head
  â†“
Next Token Prediction
```

**ì™œ Decoder-only?**

```
ì–¸ì–´ ëª¨ë¸ = Auto-regressive ìƒì„±
â†’ Masked Self-Attention í•„ìš”
â†’ Decoderê°€ ì í•©!
```

### Pre-training: Language Modeling

**ì•„ì´ë””ì–´:**

> "ë‹¤ìŒ ë‹¨ì–´ ì˜ˆì¸¡"

```
Input:  "I love deep"
Output: "learning"

Input:  "The quick brown"
Output: "fox"
```

**ëª©ì  í•¨ìˆ˜:**

$$
\mathcal{L} = -\sum_{i=1}^n \log P(w_i | w_1, \ldots, w_{i-1})
$$

**êµ¬í˜„:**

```python
class GPT(nn.Module):
    def __init__(self, vocab_size, d_model=768, n_heads=12, n_layers=12, d_ff=3072, max_len=1024):
        super().__init__()
        
        # Embeddings
        self.token_embedding = nn.Embedding(vocab_size, d_model)
        self.position_embedding = nn.Embedding(max_len, d_model)
        
        # Decoder layers (masked self-attention)
        self.layers = nn.ModuleList([
            DecoderLayer(d_model, n_heads, d_ff)
            for _ in range(n_layers)
        ])
        
        # Language model head
        self.lm_head = nn.Linear(d_model, vocab_size, bias=False)
        
        # Tie weights (input embedding = output projection)
        self.lm_head.weight = self.token_embedding.weight
    
    def forward(self, input_ids):
        """
        input_ids: (batch, seq_len)
        """
        batch_size, seq_len = input_ids.size()
        
        # Embeddings
        position_ids = torch.arange(seq_len).unsqueeze(0).expand(batch_size, -1)
        token_emb = self.token_embedding(input_ids)
        position_emb = self.position_embedding(position_ids)
        
        x = token_emb + position_emb
        
        # Causal mask (can't see future)
        causal_mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()
        
        # Decoder layers
        for layer in self.layers:
            x = layer(x, mask=causal_mask)
        
        # Language model prediction
        logits = self.lm_head(x)
        
        return logits

# Training
model = GPT(vocab_size=50000)
optimizer = torch.optim.Adam(model.parameters(), lr=2.5e-4)

for batch in dataloader:
    input_ids = batch['input_ids']  # (batch, seq_len)
    
    # Forward
    logits = model(input_ids[:, :-1])  # All but last token
    
    # Loss (predict next token)
    targets = input_ids[:, 1:]  # All but first token
    loss = F.cross_entropy(
        logits.reshape(-1, logits.size(-1)),
        targets.reshape(-1)
    )
    
    # Backward
    loss.backward()
    optimizer.step()
    optimizer.zero_grad()
```

### Fine-tuning

**GPTì˜ íŠ¹ë³„í•œ ì :**

> "Task-specific head ì—†ì´ë„ ë™ì‘!"

```python
# Classification
def classify(model, text):
    prompt = f"{text} [SEP] Label:"
    tokens = tokenizer.encode(prompt)
    
    # Generate
    output = model.generate(tokens, max_len=5)
    
    # Extract label
    label = tokenizer.decode(output[-1])
    return label

# QA
def qa(model, question, context):
    prompt = f"Context: {context} [SEP] Question: {question} [SEP] Answer:"
    tokens = tokenizer.encode(prompt)
    
    # Generate
    answer = model.generate(tokens, max_len=50)
    return tokenizer.decode(answer)
```

---

## BERT vs GPT

### êµ¬ì¡° ë¹„êµ

```
BERT:
- Encoder-only
- Bidirectional
- [MASK] ê¸°ë°˜ í•™ìŠµ

GPT:
- Decoder-only
- Unidirectional (left-to-right)
- Next token ì˜ˆì¸¡
```

### ì¥ë‹¨ì 

**BERT:**

```
ì¥ì :
âœ… Bidirectional â†’ ë” í’ë¶€í•œ í‘œí˜„
âœ… Classification íƒœìŠ¤í¬ ê°•í•¨
âœ… ë¬¸ì¥ ê°„ ê´€ê³„ ì´í•´

ë‹¨ì :
âŒ ìƒì„± ë¶ˆê°€ëŠ¥
âŒ [MASK] tokenì´ fine-tuning ì‹œ ì—†ìŒ
```

**GPT:**

```
ì¥ì :
âœ… í…ìŠ¤íŠ¸ ìƒì„± ê°€ëŠ¥
âœ… Zero-shot learning
âœ… Pre-trainingê³¼ fine-tuning ì¼ê´€ì„±

ë‹¨ì :
âŒ Unidirectional â†’ ì œí•œì  í‘œí˜„
âŒ Classification ì•½í•¨
```

### ì–¸ì œ ì‚¬ìš©?

```python
if task == "classification":
    return "BERT"
elif task == "generation":
    return "GPT"
elif task == "qa":
    return "BERT"  # ë¬¸ë§¥ ì´í•´ ì¤‘ìš”
elif task == "summarization":
    return "GPT"  # ìƒì„± í•„ìš”
```

---

## GPT-2, GPT-3ì˜ ì§„í™”

### GPT-2 (2019)

**í¬ê¸° ì¦ê°€:**

```
GPT-2:
- Parameters: 1.5B
- Data: 40GB (WebText)

ê²°ê³¼:
- Zero-shot ì„±ëŠ¥ í–¥ìƒ
- Few-shot learning ê°€ëŠ¥
```

**Byte Pair Encoding (BPE):**

```python
# Subword tokenization
"playing" â†’ ["play", "ing"]
"unbelievable" â†’ ["un", "believ", "able"]

ì¥ì :
- OOV (Out-of-Vocabulary) í•´ê²°
- ë” ì‘ì€ vocabulary
```

### GPT-3 (2020)

**ê±°ëŒ€í™”:**

```
GPT-3:
- Parameters: 175B
- Data: 300B tokens
- Context: 2048 tokens â†’ 4096

ê²°ê³¼:
- Few-shot learning ê°•ë ¥
- Prompt engineering
```

**In-context Learning:**

```
# Few-shot example
Input:
Q: What is the capital of France?
A: Paris

Q: What is the capital of Germany?
A: Berlin

Q: What is the capital of Italy?
A:

Output: Rome
```

---

## ì‹¤ì „ ì‚¬ìš© (Hugging Face)

### BERT

```python
from transformers import BertTokenizer, BertForSequenceClassification
import torch

# Load pre-trained model
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertForSequenceClassification.from_pretrained(
    'bert-base-uncased',
    num_labels=2
)

# Fine-tuning
text = "I love this movie!"
inputs = tokenizer(text, return_tensors='pt')
labels = torch.tensor([1])  # Positive

outputs = model(**inputs, labels=labels)
loss = outputs.loss
loss.backward()

# Inference
with torch.no_grad():
    outputs = model(**inputs)
    prediction = torch.argmax(outputs.logits, dim=-1)
    print(f"Sentiment: {'Positive' if prediction == 1 else 'Negative'}")
```

### GPT-2

```python
from transformers import GPT2Tokenizer, GPT2LMHeadModel

# Load pre-trained model
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
model = GPT2LMHeadModel.from_pretrained('gpt2')

# Generate
prompt = "Once upon a time"
inputs = tokenizer(prompt, return_tensors='pt')

outputs = model.generate(
    inputs['input_ids'],
    max_length=100,
    num_return_sequences=1,
    temperature=0.7,
    top_p=0.9,
    do_sample=True
)

generated_text = tokenizer.decode(outputs[0])
print(generated_text)
```

---

## ìš”ì•½

**BERT:**
- Encoder-only
- Masked Language Model + NSP
- Classification ê°•í•¨
- Bidirectional í‘œí˜„

**GPT:**
- Decoder-only
- Language Modeling
- ìƒì„± ê°€ëŠ¥
- Unidirectional

**í•µì‹¬ ì¸ì‚¬ì´íŠ¸:**

> "Pre-trainingìœ¼ë¡œ ì¼ë°˜ì  ì–¸ì–´ ì§€ì‹ì„ í•™ìŠµí•˜ê³ , Fine-tuningìœ¼ë¡œ íŠ¹ì • íƒœìŠ¤í¬ì— ì ì‘"

**ë‹¤ìŒ ê¸€:**
- **Vision Transformer**: ì´ë¯¸ì§€ì— Transformer
- **Multimodal Models**: CLIP, Flamingo
- **Efficient Transformers**: Linformer, Performer

ê¸°ëŒ€í•´ì£¼ì„¸ìš”! ğŸš€

---

*ì§ˆë¬¸ì´ë‚˜ í”¼ë“œë°±ì€ [GitHub](https://github.com/yhna941)ì—ì„œ í™˜ì˜í•©ë‹ˆë‹¤!*
