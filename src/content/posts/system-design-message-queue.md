---
title: "System Design #4: Message Queue - Kafkaì™€ ì´ë²¤íŠ¸ ê¸°ë°˜ ì•„í‚¤í…ì²˜"
description: "ëŒ€ê·œëª¨ ë¹„ë™ê¸° ì²˜ë¦¬ì™€ ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ í†µì‹ ì˜ í•µì‹¬ì¸ Message Queueë¥¼ ì™„ì „íˆ ì´í•´í•©ë‹ˆë‹¤."
pubDate: 2026-02-06
author: "Yh Na"
tags: ["system-design", "message-queue", "kafka", "rabbitmq", "event-driven"]
draft: false
---

# System Design #4: Message Queue

**"ë¹„ë™ê¸°ê°€ ë‹µì´ë‹¤"**

ë™ê¸° ì²˜ë¦¬ì˜ ë¬¸ì œ:
```
User â†’ [API] â†’ Email â†’ SMS â†’ Notification â†’ DB
                â†“ 5s ëŒ€ê¸°...
         User still waiting...
```

ë¹„ë™ê¸° ì²˜ë¦¬:
```
User â†’ [API] â†’ Queue â†’ Response (10ms)
                â†“
         [Workers] â†’ Email, SMS, Notification (ë°±ê·¸ë¼ìš´ë“œ)
```

---

## Message Queueë€?

### ì •ì˜

> **ì„œë¹„ìŠ¤ ê°„ ë¹„ë™ê¸° í†µì‹ ì„ ìœ„í•œ ì¤‘ê°„ ì €ì¥ì†Œ**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Producer â”‚â”€â”€â”€â”€â”€â–¶â”‚ Queue â”‚â”€â”€â”€â”€â”€â–¶â”‚ Consumer â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ì–¸ì œ ì‚¬ìš©?

**1. ë¹„ë™ê¸° ì²˜ë¦¬:**

```python
# Without Queue (ë™ê¸°)
def create_user(data):
    user = db.save_user(data)
    send_email(user.email)  # 2ì´ˆ
    send_sms(user.phone)    # 1ì´ˆ
    update_analytics()      # 1ì´ˆ
    return user  # 4ì´ˆ í›„ ì‘ë‹µ!

# With Queue (ë¹„ë™ê¸°)
def create_user(data):
    user = db.save_user(data)
    
    # Enqueue background tasks
    queue.publish('send_email', {'email': user.email})
    queue.publish('send_sms', {'phone': user.phone})
    queue.publish('update_analytics', {'user_id': user.id})
    
    return user  # ì¦‰ì‹œ ì‘ë‹µ!
```

**2. Load Leveling (ë¶€í•˜ í‰ì¤€í™”):**

```
Traffic spike:
10,000 requests/sec
        â†“
    [Queue] (ë²„í¼)
        â†“
 100 requests/sec (Workersê°€ ì²˜ë¦¬ ê°€ëŠ¥í•œ ì†ë„)
```

**3. Decoupling (ì„œë¹„ìŠ¤ ë¶„ë¦¬):**

```
Before (ê°•ê²°í•©):
Order Service â†’ Payment Service (ì§ì ‘ í˜¸ì¶œ)
â†’ Payment ì£½ìœ¼ë©´ Orderë„ ì£½ìŒ!

After (ëŠìŠ¨í•œ ê²°í•©):
Order Service â†’ Queue â†’ Payment Service
â†’ Payment ì£½ì–´ë„ OrderëŠ” ë™ì‘
â†’ Queueì— ìŒ“ì´ê³  ë‚˜ì¤‘ì— ì²˜ë¦¬
```

---

## ì£¼ìš” Message Queue

### ë¹„êµ

| | RabbitMQ | Kafka | Redis Pub/Sub |
|---|---|---|---|
| íƒ€ì… | Message Broker | Event Streaming | Cache + Pub/Sub |
| ì²˜ë¦¬ëŸ‰ | 20K msg/s | 1M msg/s | 1M msg/s |
| ì˜ì†ì„± | ì„ íƒì  | ë””ìŠ¤í¬ ì €ì¥ | ë©”ëª¨ë¦¬ë§Œ |
| ìˆœì„œ ë³´ì¥ | Queue ë‚´ | Partition ë‚´ | ì—†ìŒ |
| ì‚¬ìš©ì²˜ | Task queue | Event log | Real-time notifications |

---

## RabbitMQ

### ê¸°ë³¸ ê°œë…

```
Producer â†’ Exchange â†’ Queue â†’ Consumer
```

**Exchange Types:**

1. **Direct**: Routing key ì •í™•íˆ ë§¤ì¹­
2. **Fanout**: ëª¨ë“  queueë¡œ broadcast
3. **Topic**: Pattern matching
4. **Headers**: Header ê¸°ë°˜

### êµ¬í˜„

```python
import pika
import json

class RabbitMQProducer:
    def __init__(self, host='localhost'):
        self.connection = pika.BlockingConnection(
            pika.ConnectionParameters(host=host)
        )
        self.channel = self.connection.channel()
    
    def publish(self, queue_name, message):
        """Publish message to queue"""
        # Declare queue (idempotent)
        self.channel.queue_declare(queue=queue_name, durable=True)
        
        # Publish
        self.channel.basic_publish(
            exchange='',
            routing_key=queue_name,
            body=json.dumps(message),
            properties=pika.BasicProperties(
                delivery_mode=2,  # Persistent
            )
        )
        print(f"Published: {message}")
    
    def close(self):
        self.connection.close()

class RabbitMQConsumer:
    def __init__(self, host='localhost'):
        self.connection = pika.BlockingConnection(
            pika.ConnectionParameters(host=host)
        )
        self.channel = self.connection.channel()
    
    def consume(self, queue_name, callback):
        """Consume messages from queue"""
        self.channel.queue_declare(queue=queue_name, durable=True)
        
        def wrapped_callback(ch, method, properties, body):
            message = json.loads(body)
            
            try:
                # Process message
                callback(message)
                
                # Acknowledge
                ch.basic_ack(delivery_tag=method.delivery_tag)
            except Exception as e:
                # Reject and requeue
                print(f"Error: {e}")
                ch.basic_nack(delivery_tag=method.delivery_tag, requeue=True)
        
        self.channel.basic_qos(prefetch_count=1)  # Fair dispatch
        self.channel.basic_consume(
            queue=queue_name,
            on_message_callback=wrapped_callback
        )
        
        print(f"Consuming from {queue_name}...")
        self.channel.start_consuming()

# Producer
producer = RabbitMQProducer()
producer.publish('email_queue', {
    'to': 'user@example.com',
    'subject': 'Welcome!',
    'body': 'Thanks for signing up'
})

# Consumer
def send_email(message):
    print(f"Sending email to {message['to']}")
    # Email sending logic...
    time.sleep(2)  # Simulate work

consumer = RabbitMQConsumer()
consumer.consume('email_queue', send_email)
```

### Work Queue Pattern

```python
# Multiple workers for parallel processing
import threading

def worker(worker_id):
    consumer = RabbitMQConsumer()
    
    def process(message):
        print(f"Worker {worker_id} processing: {message}")
        time.sleep(1)
    
    consumer.consume('tasks', process)

# Start 3 workers
for i in range(3):
    thread = threading.Thread(target=worker, args=(i,))
    thread.start()
```

---

## Kafka

### í•µì‹¬ ê°œë…

**êµ¬ì¡°:**

```
Topic: "user_events"
  â”œâ”€ Partition 0: [msg1, msg2, msg3, ...]
  â”œâ”€ Partition 1: [msg4, msg5, msg6, ...]
  â””â”€ Partition 2: [msg7, msg8, msg9, ...]

Consumer Group: "analytics"
  â”œâ”€ Consumer 1 â†’ Partition 0
  â”œâ”€ Consumer 2 â†’ Partition 1
  â””â”€ Consumer 3 â†’ Partition 2
```

**íŠ¹ì§•:**

1. **Partitioning**: ë³‘ë ¬ ì²˜ë¦¬
2. **Replication**: ë‚´êµ¬ì„±
3. **Retention**: ë©”ì‹œì§€ ë³´ê´€ (default 7ì¼)
4. **Offset**: ê° consumerê°€ ë…ë¦½ì ìœ¼ë¡œ ì½ê¸° ìœ„ì¹˜ ì¶”ì 

### êµ¬í˜„

```python
from kafka import KafkaProducer, KafkaConsumer
import json

class KafkaMessageProducer:
    def __init__(self, bootstrap_servers=['localhost:9092']):
        self.producer = KafkaProducer(
            bootstrap_servers=bootstrap_servers,
            value_serializer=lambda v: json.dumps(v).encode('utf-8')
        )
    
    def send(self, topic, message, key=None):
        """Send message to topic"""
        future = self.producer.send(
            topic,
            value=message,
            key=key.encode('utf-8') if key else None
        )
        
        # Wait for confirmation (optional)
        metadata = future.get(timeout=10)
        print(f"Sent to {metadata.topic} partition {metadata.partition}")
    
    def close(self):
        self.producer.flush()
        self.producer.close()

class KafkaMessageConsumer:
    def __init__(
        self,
        topics,
        group_id,
        bootstrap_servers=['localhost:9092']
    ):
        self.consumer = KafkaConsumer(
            *topics,
            group_id=group_id,
            bootstrap_servers=bootstrap_servers,
            value_deserializer=lambda m: json.loads(m.decode('utf-8')),
            auto_offset_reset='earliest',  # or 'latest'
            enable_auto_commit=False  # Manual commit for reliability
        )
    
    def consume(self, callback):
        """Consume messages"""
        for message in self.consumer:
            try:
                callback(message.value)
                
                # Commit offset
                self.consumer.commit()
            except Exception as e:
                print(f"Error: {e}")
                # Don't commit - will retry

# Producer
producer = KafkaMessageProducer()

producer.send('user_events', {
    'event': 'user_signup',
    'user_id': '12345',
    'timestamp': time.time()
}, key='12345')  # Key for partitioning

# Consumer
consumer = KafkaMessageConsumer(
    topics=['user_events'],
    group_id='analytics'
)

def process_event(event):
    print(f"Processing: {event}")
    # Analytics logic...

consumer.consume(process_event)
```

### Exactly-Once Semantics

```python
class ExactlyOnceProcessor:
    def __init__(self, kafka_consumer, kafka_producer, db):
        self.consumer = kafka_consumer
        self.producer = kafka_producer
        self.db = db
    
    def process_message(self, message):
        """Idempotent processing"""
        message_id = message['id']
        
        # 1. Check if already processed
        if self.db.is_processed(message_id):
            print(f"Already processed: {message_id}")
            return
        
        # 2. Process message
        result = self.do_processing(message)
        
        # 3. Store result + mark as processed (atomic transaction)
        with self.db.transaction():
            self.db.save_result(result)
            self.db.mark_processed(message_id)
        
        # 4. Produce output
        self.producer.send('output_topic', result)
        
        # 5. Commit offset
        self.consumer.commit()
```

---

## Event-Driven Architecture

### ì´ë²¤íŠ¸ ê¸°ë°˜ ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ User Serviceâ”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ Publishes: "user.created"
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Event Bus   â”‚ (Kafka)
â””â”€â”€â”¬â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”˜
   â”‚  â”‚   â”‚
   â†“  â†“   â†“
  â”Œâ”€â”€â”â”Œâ”€â”€â”â”Œâ”€â”€â”€â”€â”€â”€â”
  â”‚Eâ”‚â”‚Sâ”‚â”‚Nâ”‚
  â”‚mâ”‚â”‚Mâ”‚â”‚oâ”‚
  â”‚aâ”‚â”‚Sâ”‚â”‚tâ”‚
  â”‚iâ”‚â”‚ â”‚â”‚iâ”‚
  â”‚lâ”‚â”‚Sâ”‚â”‚fâ”‚
  â””â”€â”€â”˜â””â”€â”€â”˜â””â”€â”€â”€â”€â”€â”€â”˜
```

### Event Schema

```python
# Event definition
class UserCreatedEvent:
    def __init__(self, user_id, email, name, timestamp):
        self.event_type = 'user.created'
        self.user_id = user_id
        self.email = email
        self.name = name
        self.timestamp = timestamp
    
    def to_dict(self):
        return {
            'event_type': self.event_type,
            'user_id': self.user_id,
            'email': self.email,
            'name': self.name,
            'timestamp': self.timestamp
        }

# Publisher (User Service)
class UserService:
    def __init__(self, kafka_producer):
        self.producer = kafka_producer
    
    def create_user(self, data):
        # 1. Create user in DB
        user = db.insert_user(data)
        
        # 2. Publish event
        event = UserCreatedEvent(
            user_id=user.id,
            email=user.email,
            name=user.name,
            timestamp=time.time()
        )
        
        self.producer.send('user_events', event.to_dict())
        
        return user

# Subscriber (Email Service)
class EmailService:
    def __init__(self, kafka_consumer):
        self.consumer = kafka_consumer
    
    def start(self):
        def handle_event(event):
            if event['event_type'] == 'user.created':
                self.send_welcome_email(
                    event['email'],
                    event['name']
                )
        
        self.consumer.consume(handle_event)
    
    def send_welcome_email(self, email, name):
        print(f"Sending welcome email to {email}")
        # Email logic...
```

---

## Patterns

### 1. Retry with Exponential Backoff

```python
import time
import random

class RetryableConsumer:
    def __init__(self, consumer, max_retries=5):
        self.consumer = consumer
        self.max_retries = max_retries
    
    def consume_with_retry(self, callback):
        for message in self.consumer:
            self.process_with_retry(message, callback)
    
    def process_with_retry(self, message, callback):
        for attempt in range(self.max_retries):
            try:
                callback(message.value)
                self.consumer.commit()
                return
            except Exception as e:
                if attempt == self.max_retries - 1:
                    # Max retries reached - send to DLQ
                    self.send_to_dlq(message)
                    return
                
                # Exponential backoff
                sleep_time = (2 ** attempt) + random.uniform(0, 1)
                print(f"Retry {attempt + 1} after {sleep_time}s")
                time.sleep(sleep_time)
    
    def send_to_dlq(self, message):
        """Send to Dead Letter Queue"""
        dlq_producer.send('dead_letter_queue', message.value)
        print(f"Sent to DLQ: {message}")
```

### 2. Circuit Breaker

```python
class CircuitBreaker:
    def __init__(self, failure_threshold=5, timeout=60):
        self.failure_threshold = failure_threshold
        self.timeout = timeout
        self.failures = 0
        self.last_failure_time = None
        self.state = 'CLOSED'  # CLOSED, OPEN, HALF_OPEN
    
    def call(self, func, *args, **kwargs):
        if self.state == 'OPEN':
            if time.time() - self.last_failure_time > self.timeout:
                self.state = 'HALF_OPEN'
            else:
                raise Exception("Circuit breaker is OPEN")
        
        try:
            result = func(*args, **kwargs)
            self.on_success()
            return result
        except Exception as e:
            self.on_failure()
            raise e
    
    def on_success(self):
        self.failures = 0
        self.state = 'CLOSED'
    
    def on_failure(self):
        self.failures += 1
        self.last_failure_time = time.time()
        
        if self.failures >= self.failure_threshold:
            self.state = 'OPEN'
            print("Circuit breaker opened!")

# ì‚¬ìš©
breaker = CircuitBreaker()

def process_message(message):
    breaker.call(external_api_call, message)
```

### 3. Saga Pattern (Distributed Transaction)

```python
class OrderSaga:
    def __init__(self, event_bus):
        self.bus = event_bus
    
    def create_order(self, order_data):
        """
        Saga steps:
        1. Reserve inventory
        2. Process payment
        3. Update order status
        
        If any step fails â†’ compensate previous steps
        """
        saga_id = generate_id()
        
        # Step 1: Reserve inventory
        self.bus.publish('inventory.reserve', {
            'saga_id': saga_id,
            'product_id': order_data['product_id'],
            'quantity': order_data['quantity']
        })
        
        # Saga coordinator handles rest...
    
    def handle_inventory_reserved(self, event):
        """Inventory reserved - proceed to payment"""
        saga_id = event['saga_id']
        
        self.bus.publish('payment.process', {
            'saga_id': saga_id,
            'amount': event['amount']
        })
    
    def handle_payment_failed(self, event):
        """Payment failed - compensate (release inventory)"""
        saga_id = event['saga_id']
        
        self.bus.publish('inventory.release', {
            'saga_id': saga_id
        })
    
    def handle_payment_succeeded(self, event):
        """All steps succeeded - complete order"""
        saga_id = event['saga_id']
        
        self.bus.publish('order.complete', {
            'saga_id': saga_id
        })
```

---

## ì‹¤ì „ ì˜ˆì œ: E-commerce Order Processing

```python
# Order Service (Orchestrator)
class OrderService:
    def __init__(self, kafka_producer):
        self.producer = kafka_producer
    
    def place_order(self, order_data):
        order_id = db.create_order(order_data)
        
        # Publish event
        self.producer.send('orders', {
            'event': 'order.placed',
            'order_id': order_id,
            'user_id': order_data['user_id'],
            'items': order_data['items'],
            'total': order_data['total']
        })
        
        return order_id

# Inventory Service
class InventoryService:
    def __init__(self, kafka_consumer, kafka_producer):
        self.consumer = kafka_consumer
        self.producer = kafka_producer
    
    def start(self):
        def handle(event):
            if event['event'] == 'order.placed':
                self.reserve_inventory(event)
        
        self.consumer.consume(handle)
    
    def reserve_inventory(self, order):
        try:
            for item in order['items']:
                db.decrement_stock(item['product_id'], item['quantity'])
            
            # Success
            self.producer.send('orders', {
                'event': 'inventory.reserved',
                'order_id': order['order_id']
            })
        except InsufficientStock:
            # Failure
            self.producer.send('orders', {
                'event': 'inventory.failed',
                'order_id': order['order_id'],
                'reason': 'Out of stock'
            })

# Payment Service
class PaymentService:
    def __init__(self, kafka_consumer, kafka_producer):
        self.consumer = kafka_consumer
        self.producer = kafka_producer
    
    def start(self):
        def handle(event):
            if event['event'] == 'inventory.reserved':
                self.process_payment(event)
        
        self.consumer.consume(handle)
    
    def process_payment(self, order):
        try:
            payment_gateway.charge(
                order['user_id'],
                order['total']
            )
            
            self.producer.send('orders', {
                'event': 'payment.succeeded',
                'order_id': order['order_id']
            })
        except PaymentFailed as e:
            self.producer.send('orders', {
                'event': 'payment.failed',
                'order_id': order['order_id'],
                'reason': str(e)
            })

# Notification Service
class NotificationService:
    def __init__(self, kafka_consumer):
        self.consumer = kafka_consumer
    
    def start(self):
        def handle(event):
            if event['event'] == 'payment.succeeded':
                self.send_confirmation(event)
            elif event['event'] in ['inventory.failed', 'payment.failed']:
                self.send_failure_notification(event)
        
        self.consumer.consume(handle)
    
    def send_confirmation(self, order):
        send_email(
            order['user_id'],
            f"Order {order['order_id']} confirmed!"
        )
```

---

## ìš”ì•½

**Message Queue ì¥ì :**

1. **ë¹„ë™ê¸°**: ë¹ ë¥¸ ì‘ë‹µ
2. **Decoupling**: ì„œë¹„ìŠ¤ ë…ë¦½ì„±
3. **Load Leveling**: ë¶€í•˜ ë¶„ì‚°
4. **Reliability**: ë©”ì‹œì§€ ë³´ì¥

**RabbitMQ vs Kafka:**

```
RabbitMQ:
- Task queue
- ë³µì¡í•œ routing
- ë‚®ì€ latency

Kafka:
- Event streaming
- ë†’ì€ ì²˜ë¦¬ëŸ‰
- ì¥ê¸° ë³´ê´€
- Replay ê°€ëŠ¥
```

**í•µì‹¬ íŒ¨í„´:**
- Retry with backoff
- Circuit breaker
- Saga (distributed transaction)
- Dead letter queue

**ë‹¤ìŒ ê¸€:**
- **Microservices**: ì„œë¹„ìŠ¤ ë¶„ë¦¬
- **API Gateway**: ë¼ìš°íŒ…, ì¸ì¦
- **Service Mesh**: Istio, Linkerd

ê¸°ëŒ€í•´ì£¼ì„¸ìš”! ğŸš€

---

*ì§ˆë¬¸ì´ë‚˜ í”¼ë“œë°±ì€ [GitHub](https://github.com/yhna941)ì—ì„œ í™˜ì˜í•©ë‹ˆë‹¤!*
