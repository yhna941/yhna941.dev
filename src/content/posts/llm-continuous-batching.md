---
title: "LLM Inference ìµœì í™” #6: Continuous Batching - ì²˜ë¦¬ëŸ‰ ê·¹ëŒ€í™”ì˜ ë¹„ë°€"
description: "vLLMê³¼ TGIì˜ í•µì‹¬ ê¸°ìˆ ì¸ Continuous Batchingìœ¼ë¡œ GPU í™œìš©ë¥ ì„ ê·¹ëŒ€í™”í•˜ëŠ” ë°©ë²•ì„ ì•Œì•„ë´…ë‹ˆë‹¤."
pubDate: 2026-02-06
author: "Yh Na"
tags: ["llm", "optimization", "batching", "throughput", "vllm"]
draft: false
---

# LLM Inference ìµœì í™” #6: Continuous Batching

ì „í†µì ì¸ batchingì€ **ë¹„íš¨ìœ¨ì **ì…ë‹ˆë‹¤. ì§§ì€ ìš”ì²­ì´ ëë‚˜ë„ ê°€ì¥ ê¸´ ìš”ì²­ì„ ê¸°ë‹¤ë ¤ì•¼ í•˜ì£ .

**Continuous Batching**ì€ ì´ ë¬¸ì œë¥¼ í•´ê²°í•©ë‹ˆë‹¤:
- ëë‚œ ìš”ì²­ì€ **ì¦‰ì‹œ ì œê±°**
- ìƒˆ ìš”ì²­ì„ **ì¦‰ì‹œ ì¶”ê°€**
- **GPU í•­ìƒ í’€ê°€ë™**

ê²°ê³¼:
- ì²˜ë¦¬ëŸ‰: **2-10ë°° í–¥ìƒ**
- ë ˆì´í„´ì‹œ: **ê°ì†Œ**
- GPU í™œìš©ë¥ : **90%+**

vLLMê³¼ TGIì˜ í•µì‹¬ ê¸°ìˆ ì…ë‹ˆë‹¤.

---

## ë¬¸ì œ: Static Batchingì˜ ë‚­ë¹„

### ì „í†µì ì¸ ë°©ì‹

```python
batch = [
    "Hi",                              # 5 tokens â†’ ë¹ ë¦„
    "Explain quantum physics in detail",  # 500 tokens â†’ ëŠë¦¼
    "What's 2+2?",                    # 8 tokens â†’ ë¹ ë¦„
]

# ëª¨ë“  ìš”ì²­ì´ ëë‚  ë•Œê¹Œì§€ ëŒ€ê¸°
while not all_finished(batch):
    next_tokens = model.forward(batch)
    update_all(batch, next_tokens)
```

### ë¬¸ì œì 

**ì‹œê°„ ë‚­ë¹„:**
```
Time:  0s -------- 10s --------- 20s
Req 1: [â–ˆâ–ˆâ–ˆâ–ˆ]                          â† 5ì´ˆì— ëë‚¬ì§€ë§Œ 20ì´ˆê¹Œì§€ ëŒ€ê¸°
Req 2: [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ]  â† ëŠë¦¼
Req 3: [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ]                         â† 6ì´ˆì— ëë‚¬ì§€ë§Œ 20ì´ˆê¹Œì§€ ëŒ€ê¸°

GPU:   [â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘]  â† ë‚­ë¹„!
```

**ë°°ì¹˜ í¬ê¸° ê°ì†Œ:**
- ì‹œì‘: 3
- 5ì´ˆ í›„: 2 (Req 1 ë)
- 6ì´ˆ í›„: 1 (Req 3 ë)
- **GPU í™œìš©ë¥  ê¸‰ë½!**

### ì‹¤ì œ ì˜í–¥

**LLaMA-7B, ë°°ì¹˜ 32:**
- ìš”ì²­ ê¸¸ì´ ë¶„í¬: [10, 50, 100, 500] tokens
- í‰ê·  ì™„ë£Œ ì‹œê°„: 100 tokens
- **í•˜ì§€ë§Œ ë§ˆì§€ë§‰ ìš”ì²­(500)ì„ ê¸°ë‹¤ë¦¼**
- ì‹¤ì œ ì²˜ë¦¬ëŸ‰: ì´ë¡ ì¹˜ì˜ **20%**

---

## í•´ê²°ì±…: Continuous Batching

### í•µì‹¬ ì•„ì´ë””ì–´

> **ìš”ì²­ì´ ëë‚˜ëŠ” ì¦‰ì‹œ ì œê±°í•˜ê³ , ìƒˆ ìš”ì²­ì„ ë°”ë¡œ ì¶”ê°€í•œë‹¤**

```python
running_batch = []
queue = []

while True:
    # 1. ëë‚œ ìš”ì²­ ì œê±°
    remove_finished(running_batch)
    
    # 2. íì—ì„œ ìƒˆ ìš”ì²­ ì¶”ê°€
    while len(running_batch) < max_batch_size and queue:
        running_batch.append(queue.pop())
    
    # 3. í•œ ìŠ¤í… ì‹¤í–‰
    if running_batch:
        next_tokens = model.forward(running_batch)
        update_batch(running_batch, next_tokens)
```

### ì‹œê°í™”

```
Time:  0s -------- 5s --------- 10s -------- 15s
Req 1: [â–ˆâ–ˆâ–ˆâ–ˆ]
Req 2:                                [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ]
Req 3: [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ]
Req 4:         [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ]
Req 5:                  [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ]
Req 6:                             [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ]

Batch: [1,2,3] [2,3,4] [2,4,5] [2,5] [2,5,6] [5,6]
GPU:   [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] [â–ˆâ–ˆâ–ˆâ–ˆ] [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] [â–ˆâ–ˆâ–ˆâ–ˆ]  â† í•­ìƒ ì‚¬ìš©!
```

---

## êµ¬í˜„

### 1. ìš”ì²­ ê´€ë¦¬

```python
from dataclasses import dataclass
from enum import Enum
from typing import List, Optional

class RequestStatus(Enum):
    WAITING = "waiting"
    RUNNING = "running"
    FINISHED = "finished"

@dataclass
class GenerationRequest:
    id: str
    prompt: List[int]  # token IDs
    max_tokens: int
    temperature: float
    
    # ìƒíƒœ
    status: RequestStatus = RequestStatus.WAITING
    generated: List[int] = None
    num_generated: int = 0
    
    def __post_init__(self):
        if self.generated is None:
            self.generated = []
    
    def is_finished(self) -> bool:
        return (
            self.num_generated >= self.max_tokens or
            self.generated and self.generated[-1] == EOS_TOKEN
        )


class RequestPool:
    def __init__(self, max_batch_size: int):
        self.max_batch_size = max_batch_size
        self.waiting: List[GenerationRequest] = []
        self.running: List[GenerationRequest] = []
        self.finished: List[GenerationRequest] = []
    
    def add_request(self, req: GenerationRequest):
        """ìƒˆ ìš”ì²­ ì¶”ê°€"""
        req.status = RequestStatus.WAITING
        self.waiting.append(req)
    
    def schedule(self):
        """ì‹¤í–‰í•  ë°°ì¹˜ êµ¬ì„±"""
        # ëë‚œ ìš”ì²­ ì œê±°
        finished = [r for r in self.running if r.is_finished()]
        for req in finished:
            req.status = RequestStatus.FINISHED
            self.running.remove(req)
            self.finished.append(req)
        
        # ìƒˆ ìš”ì²­ ì¶”ê°€ (ë°°ì¹˜ í¬ê¸°ê¹Œì§€)
        available_slots = self.max_batch_size - len(self.running)
        new_requests = self.waiting[:available_slots]
        
        for req in new_requests:
            req.status = RequestStatus.RUNNING
            self.waiting.remove(req)
            self.running.append(req)
    
    def get_running_batch(self) -> List[GenerationRequest]:
        return self.running
```

### 2. ë°°ì¹˜ ì‹¤í–‰ ì—”ì§„

```python
import torch

class ContinuousBatchingEngine:
    def __init__(self, model, tokenizer, max_batch_size=32):
        self.model = model
        self.tokenizer = tokenizer
        self.pool = RequestPool(max_batch_size)
        self.kv_caches = {}  # request_id -> KVCache
    
    def add_request(self, prompt: str, max_tokens: int = 100, temperature: float = 1.0):
        """ìƒˆ ìš”ì²­ ì¶”ê°€"""
        tokens = self.tokenizer.encode(prompt)
        req = GenerationRequest(
            id=f"req_{len(self.pool.waiting)}",
            prompt=tokens,
            max_tokens=max_tokens,
            temperature=temperature
        )
        self.pool.add_request(req)
        return req.id
    
    def step(self):
        """í•œ ìŠ¤í… ì‹¤í–‰"""
        # 1. ìŠ¤ì¼€ì¤„ë§ (ëë‚œ ê²ƒ ì œê±°, ìƒˆ ê²ƒ ì¶”ê°€)
        self.pool.schedule()
        
        running = self.pool.get_running_batch()
        if not running:
            return
        
        # 2. ì…ë ¥ ì¤€ë¹„
        input_ids = []
        attention_masks = []
        
        for req in running:
            if req.num_generated == 0:
                # Prefill: ì „ì²´ í”„ë¡¬í”„íŠ¸
                tokens = req.prompt
            else:
                # Decode: ë§ˆì§€ë§‰ í† í°ë§Œ
                tokens = [req.generated[-1]]
            
            input_ids.append(tokens)
        
        # 3. Padding (ê¸¸ì´ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ)
        max_len = max(len(ids) for ids in input_ids)
        padded = []
        masks = []
        
        for ids in input_ids:
            pad_len = max_len - len(ids)
            padded.append([PAD_TOKEN] * pad_len + ids)
            masks.append([0] * pad_len + [1] * len(ids))
        
        input_tensor = torch.tensor(padded, device='cuda')
        mask_tensor = torch.tensor(masks, device='cuda')
        
        # 4. Forward
        with torch.no_grad():
            outputs = self.model(
                input_ids=input_tensor,
                attention_mask=mask_tensor,
                use_cache=True,
                past_key_values=self.get_kv_caches(running)
            )
        
        logits = outputs.logits[:, -1, :]  # [batch, vocab_size]
        
        # 5. ìƒ˜í”Œë§ & ì—…ë°ì´íŠ¸
        for i, req in enumerate(running):
            # Temperature scaling
            probs = torch.softmax(logits[i] / req.temperature, dim=-1)
            next_token = torch.multinomial(probs, num_samples=1).item()
            
            # ì—…ë°ì´íŠ¸
            req.generated.append(next_token)
            req.num_generated += 1
            
            # KV Cache ì—…ë°ì´íŠ¸
            self.update_kv_cache(req.id, outputs.past_key_values[i])
    
    def run(self):
        """ê³„ì† ì‹¤í–‰"""
        while self.pool.waiting or self.pool.running:
            self.step()
    
    def get_result(self, request_id: str) -> Optional[str]:
        """ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°"""
        for req in self.pool.finished:
            if req.id == request_id:
                return self.tokenizer.decode(req.generated)
        return None


# ì‚¬ìš©
engine = ContinuousBatchingEngine(model, tokenizer, max_batch_size=32)

# ìš”ì²­ ì¶”ê°€
req1 = engine.add_request("Hello", max_tokens=50)
req2 = engine.add_request("Explain AI", max_tokens=200)
req3 = engine.add_request("What's 2+2?", max_tokens=20)

# ì‹¤í–‰
engine.run()

# ê²°ê³¼
print(engine.get_result(req1))
print(engine.get_result(req2))
print(engine.get_result(req3))
```

---

## ê³ ê¸‰ ìµœì í™”

### 1. Iteration-level Scheduling

ë§¤ ìŠ¤í…ë§ˆë‹¤ ìŠ¤ì¼€ì¤„ë§ (ë” ê³µê²©ì ):

```python
def iteration_level_schedule(self):
    """ë§¤ í† í°ë§ˆë‹¤ ë°°ì¹˜ ì¬êµ¬ì„±"""
    # Preemption: ê¸´ ìš”ì²­ì„ ì¼ì‹œ ì¤‘ë‹¨í•˜ê³  ì§§ì€ ìš”ì²­ ìš°ì„ 
    self.running.sort(key=lambda r: r.num_generated)
    
    # ì˜¤ë˜ëœ ìš”ì²­ ì¤‘ë‹¨
    if len(self.waiting) > 0 and len(self.running) == self.max_batch_size:
        # ê°€ì¥ ê¸´ ìš”ì²­ ì¤‘ë‹¨
        oldest = max(self.running, key=lambda r: r.num_generated)
        if oldest.num_generated > 50:  # ì„ê³„ê°’
            self.running.remove(oldest)
            self.waiting.insert(0, oldest)  # í ì•ì— ì¶”ê°€
```

### 2. Priority Scheduling

ìš°ì„ ìˆœìœ„ ê¸°ë°˜:

```python
@dataclass
class GenerationRequest:
    priority: int = 0  # ë†’ì„ìˆ˜ë¡ ìš°ì„ 

class PriorityRequestPool(RequestPool):
    def schedule(self):
        # ìš°ì„ ìˆœìœ„ë¡œ ì •ë ¬
        self.waiting.sort(key=lambda r: r.priority, reverse=True)
        super().schedule()

# ì‚¬ìš©
high_priority = GenerationRequest(..., priority=10)
low_priority = GenerationRequest(..., priority=1)
```

### 3. Mixed Prefill/Decode

Prefillê³¼ Decodeë¥¼ ê°™ì€ ë°°ì¹˜ì—:

```python
def mixed_batch_forward(self, requests):
    """Prefill + Decode ë™ì‹œ ì²˜ë¦¬"""
    prefill_reqs = [r for r in requests if r.num_generated == 0]
    decode_reqs = [r for r in requests if r.num_generated > 0]
    
    # Prefill (ê¸´ ì…ë ¥)
    if prefill_reqs:
        prefill_inputs = prepare_prefill(prefill_reqs)
        prefill_outputs = self.model.forward(prefill_inputs)
    
    # Decode (í† í° 1ê°œ)
    if decode_reqs:
        decode_inputs = prepare_decode(decode_reqs)
        decode_outputs = self.model.forward(decode_inputs)
    
    # ê²°í•©
    return merge_outputs(prefill_outputs, decode_outputs)
```

---

## vLLMì˜ Continuous Batching

vLLMì€ Paged Attention + Continuous Batchingì„ ê²°í•©í•©ë‹ˆë‹¤.

### í•µì‹¬ êµ¬ì¡°

```python
class LLMEngine:
    def __init__(self):
        self.scheduler = Scheduler()
        self.model_executor = ModelExecutor()
        self.cache_engine = CacheEngine()  # Paged KV Cache
    
    def step(self):
        # 1. ìŠ¤ì¼€ì¤„: ì–´ë–¤ ìš”ì²­ì„ ì‹¤í–‰í• ì§€
        scheduler_output = self.scheduler.schedule()
        
        # 2. ë©”ëª¨ë¦¬ í• ë‹¹: Paged blocks
        self.cache_engine.allocate(scheduler_output.running)
        
        # 3. ì‹¤í–‰
        outputs = self.model_executor.execute(
            scheduler_output.running,
            self.cache_engine.get_kv_cache()
        )
        
        # 4. ìƒ˜í”Œë§
        for seq, output in zip(scheduler_output.running, outputs):
            next_token = sample(output)
            seq.append_token(next_token)
        
        # 5. ë©”ëª¨ë¦¬ í•´ì œ
        self.cache_engine.free(scheduler_output.finished)
```

### Scheduler

```python
class Scheduler:
    def __init__(self, max_num_seqs=256):
        self.max_num_seqs = max_num_seqs
        self.waiting = []
        self.running = []
        self.swapped = []  # CPUë¡œ ì˜®ê¸´ ê²ƒ
    
    def schedule(self):
        # ëë‚œ ìš”ì²­ ì œê±°
        finished = [s for s in self.running if s.is_finished()]
        for seq in finished:
            self.running.remove(seq)
        
        # ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ swap
        if self.cache_engine.is_full():
            # ê°€ì¥ ê¸´ ìš”ì²­ì„ CPUë¡œ
            victim = max(self.running, key=lambda s: len(s))
            self.running.remove(victim)
            self.swapped.append(victim)
            self.cache_engine.swap_out(victim)
        
        # Swap in (ì—¬ìœ  ìˆìœ¼ë©´)
        if not self.cache_engine.is_full() and self.swapped:
            seq = self.swapped.pop(0)
            self.running.append(seq)
            self.cache_engine.swap_in(seq)
        
        # ìƒˆ ìš”ì²­ ì¶”ê°€
        while (len(self.running) < self.max_num_seqs and
               self.waiting and
               not self.cache_engine.is_full()):
            seq = self.waiting.pop(0)
            self.running.append(seq)
        
        return SchedulerOutput(
            running=self.running,
            finished=finished
        )
```

---

## ë²¤ì¹˜ë§ˆí¬

### Static vs Continuous

**LLaMA-7B, 1000 requests, ë‹¤ì–‘í•œ ê¸¸ì´:**

| ë°©ì‹ | ì²˜ë¦¬ëŸ‰ (req/s) | P99 ì§€ì—° (s) | GPU í™œìš©ë¥  |
|------|----------------|--------------|-----------|
| Static Batching | 12 | 8.5 | 45% |
| Continuous Batching | 48 | 2.3 | 87% |
| vLLM (Paged + Continuous) | 64 | 1.8 | 92% |

**4ë°° ì²˜ë¦¬ëŸ‰ í–¥ìƒ!**

### ë°°ì¹˜ í¬ê¸°ë³„

**Continuous Batching:**

| ìµœëŒ€ ë°°ì¹˜ í¬ê¸° | ì²˜ë¦¬ëŸ‰ | ì§€ì—° |
|---------------|--------|------|
| 8 | 25 req/s | 1.2s |
| 16 | 42 req/s | 1.5s |
| 32 | 58 req/s | 1.8s |
| 64 | 64 req/s | 2.3s |
| 128 | 63 req/s | 3.5s |

**ìµœì : 32-64**

---

## ì‹¤ì „ ì˜ˆì œ: FastAPI ì„œë¹™

```python
from fastapi import FastAPI, BackgroundTasks
from pydantic import BaseModel
import asyncio
from uuid import uuid4

app = FastAPI()

# ê¸€ë¡œë²Œ ì—”ì§„
engine = ContinuousBatchingEngine(model, tokenizer, max_batch_size=32)

# ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰
@app.on_event("startup")
async def startup():
    asyncio.create_task(run_engine())

async def run_engine():
    """ë°±ê·¸ë¼ìš´ë“œì—ì„œ ê³„ì† ì‹¤í–‰"""
    while True:
        engine.step()
        await asyncio.sleep(0)  # ë‹¤ë¥¸ íƒœìŠ¤í¬ì— ì–‘ë³´

# API
class GenerateRequest(BaseModel):
    prompt: str
    max_tokens: int = 100
    temperature: float = 1.0

@app.post("/generate")
async def generate(req: GenerateRequest):
    request_id = engine.add_request(
        req.prompt,
        max_tokens=req.max_tokens,
        temperature=req.temperature
    )
    
    # ê²°ê³¼ ëŒ€ê¸°
    while True:
        result = engine.get_result(request_id)
        if result:
            return {"output": result}
        await asyncio.sleep(0.01)

# ìŠ¤íŠ¸ë¦¬ë° ë²„ì „
from fastapi.responses import StreamingResponse

@app.post("/generate/stream")
async def generate_stream(req: GenerateRequest):
    request_id = engine.add_request(req.prompt, req.max_tokens, req.temperature)
    
    async def stream():
        last_len = 0
        while True:
            result = engine.get_result(request_id)
            if result:
                # ìƒˆë¡œ ìƒì„±ëœ ë¶€ë¶„ë§Œ yield
                new_text = result[last_len:]
                if new_text:
                    yield f"data: {new_text}\n\n"
                    last_len = len(result)
                
                # ëë‚¬ìœ¼ë©´ ì¢…ë£Œ
                req_obj = engine.pool.finished[-1]
                if req_obj.id == request_id and req_obj.is_finished():
                    break
            
            await asyncio.sleep(0.01)
    
    return StreamingResponse(stream(), media_type="text/event-stream")

# ì‚¬ìš©
# curl -X POST "http://localhost:8000/generate" \
#   -H "Content-Type: application/json" \
#   -d '{"prompt": "Once upon a time", "max_tokens": 100}'
```

---

## TGI (Text Generation Inference)

HuggingFaceì˜ TGIë„ Continuous Batchingì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

### ì„¤ì¹˜ & ì‹¤í–‰

```bash
# Dockerë¡œ ì‹¤í–‰
docker run --gpus all -p 8080:80 \
  -v $PWD/models:/data \
  ghcr.io/huggingface/text-generation-inference:latest \
  --model-id meta-llama/Llama-2-7b-hf \
  --max-batch-size 64 \
  --max-input-length 1024 \
  --max-total-tokens 2048
```

### API ì‚¬ìš©

```python
import requests

url = "http://localhost:8080/generate"
payload = {
    "inputs": "Once upon a time",
    "parameters": {
        "max_new_tokens": 100,
        "temperature": 0.7
    }
}

response = requests.post(url, json=payload)
print(response.json()["generated_text"])
```

### ìŠ¤íŠ¸ë¦¬ë°

```python
import requests

url = "http://localhost:8080/generate_stream"
payload = {
    "inputs": "Explain quantum physics",
    "parameters": {"max_new_tokens": 500}
}

with requests.post(url, json=payload, stream=True) as response:
    for line in response.iter_lines():
        if line:
            # SSE format
            data = json.loads(line.decode().replace("data: ", ""))
            print(data["token"]["text"], end="", flush=True)
```

---

## ìµœì  ì„¤ì • ê°€ì´ë“œ

### ë°°ì¹˜ í¬ê¸°

```python
# GPU ë©”ëª¨ë¦¬ì— ë”°ë¼
A100 80GB:  max_batch_size = 128
A100 40GB:  max_batch_size = 64
RTX 4090:   max_batch_size = 32
RTX 3090:   max_batch_size = 16
```

### í ê´€ë¦¬

```python
# í í¬ê¸° ì œí•œ (ë©”ëª¨ë¦¬ ê´€ë¦¬)
max_queue_size = max_batch_size * 4

# íƒ€ì„ì•„ì›ƒ (ë„ˆë¬´ ì˜¤ë˜ ëŒ€ê¸°í•˜ë©´ ê±°ë¶€)
max_wait_time = 5.0  # seconds
```

### Preemption

```python
# ê¸´ ìš”ì²­ ì¤‘ë‹¨ ì„ê³„ê°’
preemption_threshold = 100  # tokens

# ìš°ì„ ìˆœìœ„ ì°¨ì´
priority_boost = 2  # 2ë°° ë” ìì£¼ ìŠ¤ì¼€ì¤„
```

---

## í•œê³„ì™€ íŠ¸ë ˆì´ë“œì˜¤í”„

### 1. ë³µì¡ë„ ì¦ê°€

êµ¬í˜„ì´ ë³µì¡í•©ë‹ˆë‹¤:
- KV Cache ê´€ë¦¬
- ë©”ëª¨ë¦¬ í• ë‹¹/í•´ì œ
- ë™ì  ë°°ì¹˜ ì²˜ë¦¬

**ëŒ€ì±…:** vLLM, TGI ê°™ì€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©

### 2. Prefill ë³‘ëª©

ìƒˆ ìš”ì²­ì˜ prefillì€ ëŠë¦½ë‹ˆë‹¤:

```
Prefill (100 tokens): 50ms
Decode (1 token): 5ms

100 í† í° ìƒì„±: 50 + 100*5 = 550ms
```

**ëŒ€ì±…:** Prefillì„ ë³„ë„ ë°°ì¹˜ë¡œ ì²˜ë¦¬

### 3. ë©”ëª¨ë¦¬ ë‹¨í¸í™”

Paged Attention ì—†ì´ëŠ” ë©”ëª¨ë¦¬ ë‹¨í¸í™” ë°œìƒ

**ëŒ€ì±…:** Paged Attention ê²°í•© (vLLM)

---

## ìš”ì•½

**Continuous Batching**ì€:

1. **ë™ì  ë°°ì¹˜**: ëë‚˜ëŠ” ì¦‰ì‹œ ì œê±°, ìƒˆë¡œ ì¶”ê°€
2. **GPU ìµœëŒ€ í™œìš©**: 90%+ í™œìš©ë¥ 
3. **2-10ë°° ì²˜ë¦¬ëŸ‰** í–¥ìƒ
4. **ë ˆì´í„´ì‹œ ê°ì†Œ**

**í•µì‹¬ ê¸°ë²•:**
- Iteration-level scheduling
- Priority-based scheduling
- Mixed prefill/decode
- Paged Attention ê²°í•©

**ì‚¬ìš© ë¼ì´ë¸ŒëŸ¬ë¦¬:**
- vLLM (ì¶”ì²œ!)
- Text Generation Inference (HuggingFace)
- TensorRT-LLM (NVIDIA)

**ê²°ë¡ :** ëª¨ë“  í”„ë¡œë•ì…˜ LLM ì„œë¹™ì— í•„ìˆ˜!

---

## ë‹¤ìŒ ê¸€

**10í¸: Model Quantization**
- INT8/INT4 ì–‘ìí™”
- ë©”ëª¨ë¦¬ 4ë°° ì ˆê°
- ì†ë„ 2-3ë°° í–¥ìƒ
- QLoRA, GPTQ, AWQ

ì‹œë¦¬ì¦ˆ ì™„ê²°í¸! ê¸°ëŒ€í•´ì£¼ì„¸ìš”! ğŸš€

---

*ì§ˆë¬¸ì´ë‚˜ í”¼ë“œë°±ì€ [GitHub](https://github.com/yhna941)ì—ì„œ í™˜ì˜í•©ë‹ˆë‹¤!*
