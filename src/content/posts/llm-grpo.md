---
title: "LLM Post-training #3: GRPO - Group Relative Policy Optimization"
description: "DeepSeekì˜ GRPOë¡œ ìƒ˜í”Œ íš¨ìœ¨ì„ ê·¹ëŒ€í™”í•˜ê³  ì•ˆì •ì ìœ¼ë¡œ RL í•™ìŠµí•˜ëŠ” ë°©ë²•ì„ ì•Œì•„ë´…ë‹ˆë‹¤."
pubDate: 2026-02-06
author: "Yh Na"
tags: ["llm", "grpo", "alignment", "post-training", "reinforcement-learning"]
draft: false
---

# LLM Post-training #3: GRPO

PPOì˜ ë¬¸ì œ:
- **ìƒ˜í”Œ íš¨ìœ¨ ë‚®ìŒ** (ë§ì€ ìƒì„± í•„ìš”)
- Advantage ì¶”ì • ë¶ˆì•ˆì •
- Value function í•™ìŠµ ì–´ë ¤ì›€

DPOì˜ ë¬¸ì œ:
- Preference data í•„ìš”
- Reference model ê³ ì • (ì—…ë°ì´íŠ¸ ì•ˆ ë¨)

**GRPO (Group Relative Policy Optimization)**ëŠ” ìµœê³ ì˜ ì¡°í•©:
- **Group ë‚´ ìƒëŒ€ ë¹„êµ** (íš¨ìœ¨ì )
- **On-policy** (ì•ˆì •ì )
- **Value function ë¶ˆí•„ìš”** (ê°„ë‹¨)

ê²°ê³¼:
- DeepSeek-V2: GRPOë¡œ GPT-4 ëŠ¥ê°€
- ìƒ˜í”Œ íš¨ìœ¨ 10ë°°
- êµ¬í˜„ ê°„ë‹¨

---

## í•µì‹¬ ì•„ì´ë””ì–´

### PPOì˜ Advantage

```python
# PPOëŠ” Q(s,a) - V(s) í•„ìš”
advantage = Q_value - baseline_value

# ë¬¸ì œ: V(s) í•™ìŠµ ì–´ë ¤ì›€
```

### GRPOì˜ Group Baseline

```python
# ê°™ì€ promptì—ì„œ Nê°œ ìƒì„±
outputs = [y1, y2, ..., yN] for prompt x

# Group í‰ê· ì„ baselineìœ¼ë¡œ
baseline = mean([r(y1), r(y2), ..., r(yN)])

# Advantage
advantage_i = r(yi) - baseline

# ìƒëŒ€ì  ë¹„êµ! (ì ˆëŒ€ê°’ ì•„ë‹˜)
```

**ì¥ì :**
- Value function ë¶ˆí•„ìš”
- ê°™ì€ prompt â†’ ê³µì •í•œ ë¹„êµ
- ë¶„ì‚° ë‚®ìŒ

---

## ìˆ˜ì‹

### 1. Standard PPO

```
L^PPO = E[min(r_t(Î¸)Â·Ã‚_t, clip(r_t(Î¸))Â·Ã‚_t)]

where:
  r_t(Î¸) = Ï€_Î¸(a|s) / Ï€_old(a|s)
  Ã‚_t = Q(s,a) - V(s)  â† Value function í•„ìš”!
```

### 2. GRPO

```
L^GRPO = E[min(r_t(Î¸)Â·Ã‚^group_t, clip(r_t(Î¸))Â·Ã‚^group_t)]

where:
  Ã‚^group_i = r(y_i) - (1/N)Â·Î£_j r(y_j)
  
  (ê°™ì€ prompt xì˜ Nê°œ ìƒ˜í”Œ í‰ê· )
```

**+ KL penalty:**

```
L^total = L^GRPO - Î²Â·KL(Ï€_Î¸ || Ï€_ref)
```

---

## ì•Œê³ ë¦¬ì¦˜

### Pseudocode

```python
def grpo_step(
    policy_model,
    ref_model,
    prompts,
    num_samples_per_prompt=4,
    beta=0.1
):
    """
    GRPO í•™ìŠµ ìŠ¤í…
    """
    all_advantages = []
    all_log_ratios = []
    
    for prompt in prompts:
        # 1. Generate N samples
        samples = []
        for _ in range(num_samples_per_prompt):
            sample = policy_model.generate(prompt)
            reward = reward_model(prompt, sample)
            samples.append((sample, reward))
        
        # 2. Group baseline
        rewards = [r for _, r in samples]
        baseline = sum(rewards) / len(rewards)
        
        # 3. Advantages
        for sample, reward in samples:
            advantage = reward - baseline
            all_advantages.append(advantage)
            
            # Log probability ratio
            log_prob = policy_model.log_prob(prompt, sample)
            ref_log_prob = ref_model.log_prob(prompt, sample)
            log_ratio = log_prob - ref_log_prob
            all_log_ratios.append(log_ratio)
    
    # 4. Compute loss
    advantages = torch.tensor(all_advantages)
    log_ratios = torch.tensor(all_log_ratios)
    
    # Normalize advantages
    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
    
    # PPO-style clipped loss
    ratio = torch.exp(log_ratios)
    loss1 = ratio * advantages
    loss2 = torch.clamp(ratio, 1-epsilon, 1+epsilon) * advantages
    policy_loss = -torch.min(loss1, loss2).mean()
    
    # KL penalty
    kl_loss = beta * log_ratios.mean()
    
    total_loss = policy_loss + kl_loss
    
    return total_loss
```

---

## ì‹¤ì „ êµ¬í˜„

### ì™„ì „í•œ GRPO Trainer

```python
import torch
import torch.nn.functional as F
from transformers import AutoModelForCausalLM, AutoTokenizer
from torch.utils.data import DataLoader

class GRPOTrainer:
    def __init__(
        self,
        policy_model,
        ref_model,
        reward_model,
        tokenizer,
        num_samples_per_prompt=4,
        beta=0.1,
        epsilon=0.2,
        learning_rate=1e-6
    ):
        self.policy = policy_model
        self.ref = ref_model
        self.reward = reward_model
        self.tokenizer = tokenizer
        
        self.num_samples = num_samples_per_prompt
        self.beta = beta
        self.epsilon = epsilon
        
        self.optimizer = torch.optim.AdamW(
            self.policy.parameters(),
            lr=learning_rate
        )
        
        # Freeze reference model
        for param in self.ref.parameters():
            param.requires_grad = False
    
    def generate_group(self, prompt):
        """Generate N samples for a prompt"""
        samples = []
        
        prompt_tokens = self.tokenizer(prompt, return_tensors="pt").to(self.policy.device)
        
        for _ in range(self.num_samples):
            # Generate
            with torch.no_grad():
                output = self.policy.generate(
                    **prompt_tokens,
                    max_new_tokens=256,
                    do_sample=True,
                    temperature=0.7,
                    top_p=0.9,
                    pad_token_id=self.tokenizer.pad_token_id
                )
            
            # Decode
            response = self.tokenizer.decode(
                output[0][len(prompt_tokens.input_ids[0]):],
                skip_special_tokens=True
            )
            
            # Reward
            with torch.no_grad():
                full_text = prompt + response
                reward = self.reward(full_text)
            
            samples.append({
                'response': response,
                'output_ids': output[0],
                'reward': reward.item()
            })
        
        return samples
    
    def compute_log_probs(self, prompt_tokens, output_ids):
        """Compute log probabilities"""
        # Forward pass
        outputs = self.policy(
            input_ids=output_ids,
            attention_mask=torch.ones_like(output_ids)
        )
        logits = outputs.logits
        
        # Log probs
        log_probs = F.log_softmax(logits, dim=-1)
        
        # Gather log probs for generated tokens
        prompt_len = len(prompt_tokens.input_ids[0])
        generated_ids = output_ids[:, prompt_len:]
        
        # Shift for next-token prediction
        log_probs = log_probs[:, prompt_len-1:-1, :]
        token_log_probs = log_probs.gather(
            -1,
            generated_ids.unsqueeze(-1)
        ).squeeze(-1)
        
        return token_log_probs.sum()
    
    def train_step(self, prompts):
        """Single training step"""
        total_loss = 0
        total_advantages = []
        
        for prompt in prompts:
            # 1. Generate group
            samples = self.generate_group(prompt)
            
            # 2. Group baseline
            rewards = [s['reward'] for s in samples]
            baseline = sum(rewards) / len(rewards)
            
            # 3. Process each sample
            for sample in samples:
                advantage = sample['reward'] - baseline
                
                # Tokenize
                prompt_tokens = self.tokenizer(prompt, return_tensors="pt").to(self.policy.device)
                
                # Log prob (policy)
                policy_log_prob = self.compute_log_probs(
                    prompt_tokens,
                    sample['output_ids'].unsqueeze(0)
                )
                
                # Log prob (reference)
                with torch.no_grad():
                    ref_outputs = self.ref(
                        input_ids=sample['output_ids'].unsqueeze(0)
                    )
                    ref_logits = ref_outputs.logits
                    ref_log_probs = F.log_softmax(ref_logits, dim=-1)
                    
                    prompt_len = len(prompt_tokens.input_ids[0])
                    generated_ids = sample['output_ids'][prompt_len:]
                    ref_log_probs_shifted = ref_log_probs[0, prompt_len-1:-1, :]
                    
                    ref_log_prob = ref_log_probs_shifted.gather(
                        -1,
                        generated_ids.unsqueeze(-1)
                    ).squeeze(-1).sum()
                
                # Log ratio
                log_ratio = policy_log_prob - ref_log_prob
                
                # PPO loss
                ratio = torch.exp(log_ratio)
                loss1 = ratio * advantage
                loss2 = torch.clamp(ratio, 1-self.epsilon, 1+self.epsilon) * advantage
                policy_loss = -torch.min(loss1, loss2)
                
                # KL penalty
                kl_loss = self.beta * log_ratio
                
                # Total
                loss = policy_loss + kl_loss
                total_loss += loss
                total_advantages.append(advantage)
        
        # Backward
        total_loss = total_loss / len(prompts)
        total_loss.backward()
        
        # Gradient clipping
        torch.nn.utils.clip_grad_norm_(self.policy.parameters(), 1.0)
        
        self.optimizer.step()
        self.optimizer.zero_grad()
        
        return {
            'loss': total_loss.item(),
            'mean_advantage': sum(total_advantages) / len(total_advantages),
            'mean_reward': sum([s['reward'] for samples in [self.generate_group(p) for p in prompts] for s in samples]) / (len(prompts) * self.num_samples)
        }
    
    def train(self, prompts, num_epochs=3):
        """Full training loop"""
        for epoch in range(num_epochs):
            for i, batch_prompts in enumerate(DataLoader(prompts, batch_size=8)):
                stats = self.train_step(batch_prompts)
                
                if i % 10 == 0:
                    print(f"Epoch {epoch}, Step {i}: Loss={stats['loss']:.4f}, "
                          f"Reward={stats['mean_reward']:.4f}, Advantage={stats['mean_advantage']:.4f}")


# ì‚¬ìš©
policy_model = AutoModelForCausalLM.from_pretrained("llama2-7b-sft").cuda()
ref_model = AutoModelForCausalLM.from_pretrained("llama2-7b-sft").cuda()
reward_model = RewardModel.from_pretrained("llama2-7b-reward").cuda()
tokenizer = AutoTokenizer.from_pretrained("llama2-7b-sft")

trainer = GRPOTrainer(
    policy_model=policy_model,
    ref_model=ref_model,
    reward_model=reward_model,
    tokenizer=tokenizer,
    num_samples_per_prompt=4,
    beta=0.1
)

prompts = [
    "Explain quantum mechanics to a 5-year-old",
    "Write a poem about AI",
    # ...
]

trainer.train(prompts, num_epochs=3)
```

---

## GRPO vs PPO vs DPO

### ìƒ˜í”Œ íš¨ìœ¨

**PPO:**
```
1 prompt â†’ 1 sample â†’ 1 update
100K prompts for convergence
```

**GRPO:**
```
1 prompt â†’ 4 samples â†’ 4 updates
25K prompts for convergence  (4ë°° íš¨ìœ¨!)
```

### ì•ˆì •ì„±

**PPO:**
```
Critic network í•„ìš”
- Advantage = Q - V
- V í•™ìŠµ ë¶ˆì•ˆì •
```

**GRPO:**
```
Group baseline
- Advantage = r - mean(group_r)
- í•™ìŠµ ë¶ˆí•„ìš”, ì•ˆì •ì 
```

### ë©”ëª¨ë¦¬

| ë°©ë²• | ëª¨ë¸ ìˆ˜ | ë©”ëª¨ë¦¬ |
|------|---------|--------|
| PPO | Policy + Value + Ref + Reward | 4 models |
| **GRPO** | **Policy + Ref + Reward** | **3 models** |
| DPO | Policy + Ref | 2 models |

---

## í•˜ì´í¼íŒŒë¼ë¯¸í„°

### num_samples_per_prompt

```python
N = 2:  ë¹ ë¥´ì§€ë§Œ baseline ë¶ˆì•ˆì •
N = 4:  ê¶Œì¥ (ê· í˜•)
N = 8:  ëŠë¦¬ì§€ë§Œ ì•ˆì •ì 
N = 16: ë§¤ìš° ëŠë¦¼, ì•½ê°„ ë” ë‚˜ìŒ
```

**ì„ íƒ:**
```python
# ì‘ì€ ëª¨ë¸ (7B)
num_samples = 4

# í° ëª¨ë¸ (70B)
num_samples = 2  (ë©”ëª¨ë¦¬ ì œì•½)

# ë¦¬ì†ŒìŠ¤ í’ë¶€
num_samples = 8
```

### Beta (KL penalty)

```python
beta = 0.01: í¬ê²Œ ë³€í™” (ìœ„í—˜)
beta = 0.05: ì ë‹¹
beta = 0.1:  ì•ˆì „ (ê¶Œì¥)
```

### Epsilon (Clipping)

```python
epsilon = 0.1:  ë³´ìˆ˜ì 
epsilon = 0.2:  í‘œì¤€ (ê¶Œì¥)
epsilon = 0.3:  ê³µê²©ì 
```

---

## ìµœì í™” ê¸°ë²•

### 1. Batch Processing

```python
# Naive: ìˆœì°¨ ìƒì„±
for prompt in prompts:
    samples = generate_group(prompt)  # ëŠë¦¼

# Optimized: ë³‘ë ¬ ìƒì„±
all_prompts = [p for p in prompts for _ in range(N)]
all_samples = model.generate(all_prompts, batch_size=32)  # ë¹ ë¦„!
```

### 2. Advantage Normalization

```python
# ì „ì²´ batchì—ì„œ normalize
advantages = torch.tensor(all_advantages)
advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)

# ì•ˆì •ì„± í–¥ìƒ!
```

### 3. Reward Clipping

```python
# Outlier reward ì œê±°
rewards = torch.tensor(rewards)
rewards = torch.clamp(rewards, -10, 10)

# ë˜ëŠ” percentile
low, high = torch.quantile(rewards, torch.tensor([0.05, 0.95]))
rewards = torch.clamp(rewards, low, high)
```

---

## DeepSeek-V2 ì‚¬ë¡€

### êµ¬ì„±

```python
# DeepSeek-V2 GRPO ì„¤ì •
config = {
    "model": "DeepSeek-V2-236B",
    "num_samples_per_prompt": 4,
    "beta": 0.05,
    "epsilon": 0.2,
    "learning_rate": 1e-6,
    "batch_size": 256,
    "total_steps": 10000
}
```

### ê²°ê³¼

| ë²¤ì¹˜ë§ˆí¬ | GPT-4 | DeepSeek-V2 (GRPO) |
|---------|-------|-------------------|
| MMLU | 86.4% | 88.1% |
| HumanEval | 67.0% | 81.8% |
| GSM8K | 92.0% | 94.2% |

**GRPOë¡œ GPT-4 ëŠ¥ê°€!**

---

## GRPOì˜ ì¥ì 

### 1. ìƒ˜í”Œ íš¨ìœ¨

```python
# ê°™ì€ ì„±ëŠ¥ ë‹¬ì„±ì— í•„ìš”í•œ ìƒ˜í”Œ ìˆ˜
PPO:  100K prompts
GRPO: 25K prompts  (4ë°° ì ìŒ)
```

### 2. êµ¬í˜„ ê°„ë‹¨

```python
# PPO: Value function í•„ìš”
class Critic(nn.Module):
    def forward(self, state):
        return value  # í•™ìŠµ ì–´ë ¤ì›€

# GRPO: í‰ê· ë§Œ
baseline = mean(group_rewards)  # ê°„ë‹¨!
```

### 3. ì•ˆì •ì„±

```
PPO:  Value function ë°œì‚° ê°€ëŠ¥
GRPO: Group baseline í•­ìƒ ì•ˆì •
```

---

## ì‹¤ì „ íŒ

### 1. Reward Shaping

```python
# ì—¬ëŸ¬ reward ì¡°í•©
def total_reward(response):
    reward = 0
    
    # Helpfulness
    reward += 1.0 * helpfulness_model(response)
    
    # Harmlessness
    reward += 0.5 * harmlessness_model(response)
    
    # Length penalty
    reward -= 0.01 * len(response)
    
    return reward
```

### 2. Curriculum Learning

```python
# ì ì§„ì  ë‚œì´ë„ ì¦ê°€
epoch_1: ì‰¬ìš´ prompts (ëª…í™•í•œ ë‹µ)
epoch_2: ì¤‘ê°„ prompts
epoch_3: ì–´ë ¤ìš´ prompts (ì£¼ê´€ì )
```

### 3. Monitoring

```python
# í•™ìŠµ ì¤‘ ì¶”ì 
metrics = {
    "reward_mean": ...,
    "reward_std": ...,  # ë„ˆë¬´ í¬ë©´ ë¬¸ì œ
    "advantage_mean": ...,  # 0 ê·¼ì²˜ ìœ ì§€
    "kl_divergence": ...,  # beta ì¡°ì ˆ
    "policy_loss": ...,
    "grad_norm": ...  # Exploding ë°©ì§€
}
```

---

## ë²¤ì¹˜ë§ˆí¬

### ìƒ˜í”Œ íš¨ìœ¨ ë¹„êµ

**Task:** ìœ ìš©ì„± í–¥ìƒ (0.6 â†’ 0.8)

| ë°©ë²• | Prompts | Time | Cost |
|------|---------|------|------|
| PPO | 100K | 50h | $5K |
| DPO | N/A | - | - |
| **GRPO** | **25K** | **15h** | **$1.5K** |

### ìµœì¢… ì„±ëŠ¥

**Llama-2-7B-chat baseline:**

| Metric | PPO | DPO | GRPO |
|--------|-----|-----|------|
| Helpfulness | 7.2 | 7.8 | **8.1** |
| Harmlessness | 8.5 | 9.0 | **9.1** |
| MT-Bench | 6.3 | 7.1 | **7.4** |

**GRPOê°€ ìµœê³ !**

---

## ê³ ê¸‰ ë³€í˜•

### 1. Adaptive Group Size

```python
# Reward varianceì— ë”°ë¼ ì¡°ì ˆ
if reward_std > threshold:
    num_samples += 1  # ë” ë§ì€ ìƒ˜í”Œ
else:
    num_samples -= 1  # íš¨ìœ¨
```

### 2. Multi-turn GRPO

```python
# ëŒ€í™” ì „ì²´ë¥¼ groupìœ¼ë¡œ
for turn in conversation:
    samples = generate_group(history + turn)
    # Group baseline per turn
```

### 3. Hierarchical GRPO

```python
# Coarse-grained + Fine-grained
level_1: Generate 4 high-level plans
level_2: For each plan, generate 4 implementations

# 16 samples total
```

---

## ìš”ì•½

**GRPO**ëŠ”:

1. **Group baseline**: Value function ë¶ˆí•„ìš”
2. **ìƒ˜í”Œ íš¨ìœ¨**: PPOë³´ë‹¤ 4ë°°
3. **ì•ˆì •ì„±**: Group í‰ê· ìœ¼ë¡œ ë¶„ì‚° ê°ì†Œ
4. **ê°„ë‹¨**: êµ¬í˜„ ì‰¬ì›€

**í•µì‹¬:**
```python
advantage = reward - mean(group_rewards)
```

**íŒŒë¼ë¯¸í„°:**
- `num_samples`: 4 (ê¶Œì¥)
- `beta`: 0.1 (KL penalty)
- `epsilon`: 0.2 (clipping)

**ì„±ê³µ ì‚¬ë¡€:**
- DeepSeek-V2: GPT-4 ëŠ¥ê°€
- ìƒ˜í”Œ íš¨ìœ¨ 4ë°°
- ë¹„ìš© ì ˆê°

**ë‹¤ìŒ ê¸€:**
- **RLAIF**: AI feedback í™œìš©
- **Constitutional AI**: ê·œì¹™ ê¸°ë°˜ ì •ë ¬

ê¸°ëŒ€í•´ì£¼ì„¸ìš”! ğŸš€

---

*ì§ˆë¬¸ì´ë‚˜ í”¼ë“œë°±ì€ [GitHub](https://github.com/yhna941)ì—ì„œ í™˜ì˜í•©ë‹ˆë‹¤!*
