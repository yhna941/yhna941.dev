---
title: "LLM Inference ìµœì í™” #3: LoRA Fine-tuning ì™„ì „ ì •ë³µ"
description: "ì ì€ íŒŒë¼ë¯¸í„°ë¡œ ê±°ëŒ€ ì–¸ì–´ ëª¨ë¸ì„ íš¨ìœ¨ì ìœ¼ë¡œ í•™ìŠµí•˜ëŠ” LoRAì˜ ì›ë¦¬ì™€ ì‹¤ì „ êµ¬í˜„ì„ ì•Œì•„ë´…ë‹ˆë‹¤."
pubDate: 2026-02-06
author: "Yh Na"
tags: ["llm", "fine-tuning", "lora", "peft", "qlora"]
draft: false
---

# LLM Inference ìµœì í™” #3: LoRA Fine-tuning

7B íŒŒë¼ë¯¸í„° ëª¨ë¸ì„ fine-tuningí•˜ë ¤ë©´ ë³´í†µ **14GB ì´ìƒ**ì˜ GPU ë©”ëª¨ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤. Adam optimizerê¹Œì§€ ì“°ë©´ **80GB**ë„ ëª¨ìëë‹ˆë‹¤.

**LoRA**ëŠ” ì´ ë¬¸ì œë¥¼ í•´ê²°í•©ë‹ˆë‹¤. **0.1%ì˜ íŒŒë¼ë¯¸í„°**ë§Œ í•™ìŠµí•´ë„ full fine-tuningê³¼ ë¹„ìŠ·í•œ ì„±ëŠ¥ì„ ëƒ…ë‹ˆë‹¤.

ì–´ë–»ê²Œ ê°€ëŠ¥í• ê¹Œìš”?

---

## ë¬¸ì œ: Fine-tuningì€ ë¹„ì‹¸ë‹¤

### Full Fine-tuning

ëª¨ë“  íŒŒë¼ë¯¸í„°ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.

```
LLaMA-7B: 7,000,000,000 parameters
```

**ë©”ëª¨ë¦¬ ìš”êµ¬ì‚¬í•­:**
- ëª¨ë¸: 14 GB (fp16)
- Gradients: 14 GB
- Optimizer states (Adam): 56 GB (4x)
- Activations: 10+ GB

**ì´í•©: 94 GB+**

A100 80GBë¡œë„ ë¶€ì¡±í•©ë‹ˆë‹¤!

### ê¸°ì¡´ í•´ê²°ì±…ë“¤

**1. Adapter Layers**
- ì‘ì€ ë ˆì´ì–´ ì¶”ê°€
- ì•½ê°„ì˜ ì„±ëŠ¥ ì†ì‹¤
- ì—¬ì „íˆ ë§ì€ ë©”ëª¨ë¦¬ í•„ìš”

**2. Prompt Tuning**
- Soft promptë§Œ í•™ìŠµ
- ì„±ëŠ¥ì´ ë§ì´ ë–¨ì–´ì§
- Task-specific

---

## í•´ê²°ì±…: LoRA (Low-Rank Adaptation)

### í•µì‹¬ ì•„ì´ë””ì–´

> **ëŒ€ë¶€ë¶„ì˜ ë³€í™”ëŠ” ë‚®ì€ rankì—ì„œ ì¼ì–´ë‚œë‹¤**

ì‹ ê²½ë§ì˜ ê°€ì¤‘ì¹˜ ë³€í™” Î”WëŠ” **low-rank matrix**ë¡œ ê·¼ì‚¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```
W' = W + Î”W

Î”W â‰ˆ BA

where:
  B: [d, r]  (r << d)
  A: [r, k]  (r << k)
  rank(Î”W) = r  (ë§¤ìš° ì‘ìŒ)
```

### ìˆ˜ì‹

**ê¸°ì¡´ ì„ í˜• ë ˆì´ì–´:**
```
h = Wx
```

**LoRA:**
```
h = Wx + BAx
  = Wx + (BA)x
```

**ì—¬ê¸°ì„œ:**
- W: ì›ë³¸ ê°€ì¤‘ì¹˜ (frozen, í•™ìŠµ ì•ˆ í•¨)
- B, A: LoRA ê°€ì¤‘ì¹˜ (í•™ìŠµí•¨)
- r: rank (ë³´í†µ 8, 16, 32)

### íŒŒë¼ë¯¸í„° ê³„ì‚°

**ì›ë³¸ ë ˆì´ì–´:**
```
W: [4096, 4096]
parameters = 4096 Ã— 4096 = 16,777,216
```

**LoRA (r=16):**
```
B: [4096, 16]
A: [16, 4096]
parameters = 4096Ã—16 + 16Ã—4096 = 131,072
```

**ë¹„ìœ¨: 0.78%** ğŸ‰

---

## LoRA êµ¬í˜„

### 1. ê¸°ë³¸ LoRA Layer

```python
import torch
import torch.nn as nn
import math

class LoRALayer(nn.Module):
    def __init__(self, in_features, out_features, rank=16, alpha=16):
        super().__init__()
        self.rank = rank
        self.alpha = alpha
        
        # LoRA matrices
        self.lora_A = nn.Parameter(torch.zeros(rank, in_features))
        self.lora_B = nn.Parameter(torch.zeros(out_features, rank))
        
        # Initialize A with Kaiming, B with zeros
        nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))
        nn.init.zeros_(self.lora_B)
        
        # Scaling factor
        self.scaling = alpha / rank
    
    def forward(self, x):
        """
        Args:
            x: [batch, ..., in_features]
        Returns:
            delta: [batch, ..., out_features]
        """
        # x @ A^T @ B^T
        result = x @ self.lora_A.T  # [batch, ..., rank]
        result = result @ self.lora_B.T  # [batch, ..., out_features]
        result = result * self.scaling
        return result


class LinearWithLoRA(nn.Module):
    def __init__(self, linear_layer, rank=16, alpha=16):
        super().__init__()
        
        # ì›ë³¸ ë ˆì´ì–´ (frozen)
        self.linear = linear_layer
        for param in self.linear.parameters():
            param.requires_grad = False
        
        # LoRA ë ˆì´ì–´
        self.lora = LoRALayer(
            linear_layer.in_features,
            linear_layer.out_features,
            rank=rank,
            alpha=alpha
        )
    
    def forward(self, x):
        # h = Wx + BAx
        return self.linear(x) + self.lora(x)
```

### 2. ëª¨ë¸ì— LoRA ì ìš©

```python
def apply_lora_to_model(model, rank=16, alpha=16, target_modules=None):
    """
    ëª¨ë¸ì˜ íŠ¹ì • ë ˆì´ì–´ë¥¼ LoRAë¡œ êµì²´
    
    Args:
        model: Transformer ëª¨ë¸
        rank: LoRA rank
        alpha: LoRA alpha (scaling)
        target_modules: LoRAë¥¼ ì ìš©í•  ëª¨ë“ˆ ì´ë¦„ë“¤
                       (ì˜ˆ: ['q_proj', 'v_proj'])
    """
    if target_modules is None:
        target_modules = ['q_proj', 'k_proj', 'v_proj', 'o_proj']
    
    for name, module in model.named_modules():
        # Attentionì˜ linear layerë“¤ë§Œ êµì²´
        if any(target in name for target in target_modules):
            if isinstance(module, nn.Linear):
                # ë¶€ëª¨ ëª¨ë“ˆ ì°¾ê¸°
                parent_name = '.'.join(name.split('.')[:-1])
                parent = model.get_submodule(parent_name)
                
                # LoRAë¡œ êµì²´
                lora_layer = LinearWithLoRA(module, rank=rank, alpha=alpha)
                setattr(parent, name.split('.')[-1], lora_layer)
    
    return model
```

### 3. í•™ìŠµ ì˜ˆì œ

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments

# ëª¨ë¸ ë¡œë“œ
model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b-hf")
tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-hf")

# LoRA ì ìš©
model = apply_lora_to_model(model, rank=16, alpha=16)

# í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°ë§Œ ì¶œë ¥
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
total_params = sum(p.numel() for p in model.parameters())
print(f"Trainable: {trainable_params:,} ({trainable_params/total_params*100:.2f}%)")
# Trainable: 4,194,304 (0.06%)

# í•™ìŠµ ì„¤ì •
training_args = TrainingArguments(
    output_dir="./lora_output",
    num_train_epochs=3,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    learning_rate=3e-4,
    fp16=True,
    logging_steps=10,
    save_steps=100,
)

# í•™ìŠµ
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
)

trainer.train()

# LoRA ê°€ì¤‘ì¹˜ë§Œ ì €ì¥
torch.save({
    'lora_A': model.lora_A.state_dict(),
    'lora_B': model.lora_B.state_dict(),
}, 'lora_weights.pt')
```

---

## PEFT ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©

HuggingFaceì˜ PEFT ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì“°ë©´ ë” ì‰½ìŠµë‹ˆë‹¤.

### ì„¤ì¹˜

```bash
pip install peft
```

### ê¸°ë³¸ ì‚¬ìš©

```python
from peft import LoraConfig, get_peft_model
from transformers import AutoModelForCausalLM

# ëª¨ë¸ ë¡œë“œ
model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-2-7b-hf",
    torch_dtype=torch.float16,
    device_map="auto"
)

# LoRA ì„¤ì •
lora_config = LoraConfig(
    r=16,                        # rank
    lora_alpha=16,               # alpha scaling
    target_modules=[             # ì–´ë–¤ ë ˆì´ì–´ì— ì ìš©í• ì§€
        "q_proj",
        "k_proj",
        "v_proj",
        "o_proj",
        "gate_proj",
        "up_proj",
        "down_proj",
    ],
    lora_dropout=0.05,           # dropout
    bias="none",                 # bias í•™ìŠµ ì•ˆ í•¨
    task_type="CAUSAL_LM"        # task ì¢…ë¥˜
)

# LoRA ì ìš©
model = get_peft_model(model, lora_config)

# íŒŒë¼ë¯¸í„° í™•ì¸
model.print_trainable_parameters()
# trainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06%
```

### í•™ìŠµ

```python
from transformers import Trainer, TrainingArguments

training_args = TrainingArguments(
    output_dir="./lora_llama2",
    num_train_epochs=3,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    learning_rate=3e-4,
    fp16=True,
    logging_steps=10,
    save_strategy="epoch",
    optim="adamw_torch",
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
)

trainer.train()
```

### ì €ì¥ & ë¡œë“œ

```python
# LoRA ì–´ëŒ‘í„°ë§Œ ì €ì¥ (ëª‡ MB)
model.save_pretrained("./lora_adapter")

# ë¡œë“œ
from peft import PeftModel

base_model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b-hf")
model = PeftModel.from_pretrained(base_model, "./lora_adapter")

# ì¶”ë¡ 
model.eval()
with torch.no_grad():
    outputs = model.generate(**inputs, max_new_tokens=100)
```

---

## ê³ ê¸‰ ê¸°ë²•

### 1. QLoRA (Quantized LoRA)

**ì•„ì´ë””ì–´:** ë² ì´ìŠ¤ ëª¨ë¸ì„ 4-bitë¡œ ì–‘ìí™”í•´ì„œ ë©”ëª¨ë¦¬ ë” ì ˆì•½

```python
from transformers import BitsAndBytesConfig

# 4-bit ì–‘ìí™” ì„¤ì •
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)

# ëª¨ë¸ ë¡œë“œ (4-bit)
model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-2-7b-hf",
    quantization_config=bnb_config,
    device_map="auto"
)

# LoRA ì ìš©
model = get_peft_model(model, lora_config)
```

**ë©”ëª¨ë¦¬:**
- Full fine-tuning: 94 GB
- LoRA (fp16): 18 GB
- QLoRA (4-bit): **9 GB** ğŸ‰

**RTX 4090 24GBë¡œë„ í•™ìŠµ ê°€ëŠ¥!**

### 2. LoRA+

**ë¬¸ì œ:** Aì™€ Bì˜ í•™ìŠµë¥ ì´ ê°™ìœ¼ë©´ ë¹„íš¨ìœ¨ì 

**í•´ê²°:** BëŠ” ë” ë¹ ë¥´ê²Œ, AëŠ” ì²œì²œíˆ

```python
# LoRA+ optimizer
param_groups = [
    {
        'params': [p for n, p in model.named_parameters() if 'lora_B' in n],
        'lr': 3e-4 * 16,  # BëŠ” 16ë°° ë¹ ë¥´ê²Œ
    },
    {
        'params': [p for n, p in model.named_parameters() if 'lora_A' in n],
        'lr': 3e-4,       # AëŠ” ê¸°ë³¸ ì†ë„
    }
]

optimizer = torch.optim.AdamW(param_groups)
```

### 3. DoRA (Weight-Decomposed LoRA)

**ì•„ì´ë””ì–´:** ê°€ì¤‘ì¹˜ë¥¼ í¬ê¸°(magnitude)ì™€ ë°©í–¥(direction)ìœ¼ë¡œ ë¶„í•´

```python
W_new = m * (W + BA) / ||W + BA||

where:
  m: learnable magnitude
  W + BA: direction
```

**êµ¬í˜„:**
```python
class DoRALayer(nn.Module):
    def __init__(self, in_features, out_features, rank=16):
        super().__init__()
        self.lora_A = nn.Parameter(torch.zeros(rank, in_features))
        self.lora_B = nn.Parameter(torch.zeros(out_features, rank))
        self.magnitude = nn.Parameter(torch.ones(out_features))
        
    def forward(self, W, x):
        # Direction
        direction = W + self.lora_B @ self.lora_A
        direction = direction / direction.norm(dim=1, keepdim=True)
        
        # Magnitude
        W_new = self.magnitude.unsqueeze(1) * direction
        
        return x @ W_new.T
```

### 4. AdaLoRA (Adaptive LoRA)

**ì•„ì´ë””ì–´:** ê° ë ˆì´ì–´ë§ˆë‹¤ ë‹¤ë¥¸ rank ì‚¬ìš©

```python
# ì¤‘ìš”í•œ ë ˆì´ì–´ëŠ” ë†’ì€ rank
lora_config = LoraConfig(
    r=16,  # ê¸°ë³¸
    init_r=32,  # ì´ˆê¸° rank (pruning ë¨)
    target_r=8,  # ëª©í‘œ í‰ê·  rank
    # ...
)
```

---

## ì‹¤ì „ ì˜ˆì œ: ì±—ë´‡ Fine-tuning

### ë°ì´í„°ì…‹ ì¤€ë¹„

```python
from datasets import load_dataset

# ëŒ€í™” ë°ì´í„°ì…‹
dataset = load_dataset("databricks/databricks-dolly-15k")

def format_instruction(sample):
    """ëŒ€í™” í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    return f"""### Instruction:
{sample['instruction']}

### Context:
{sample['context']}

### Response:
{sample['response']}"""

def tokenize(sample):
    text = format_instruction(sample)
    return tokenizer(
        text,
        truncation=True,
        max_length=512,
        padding="max_length"
    )

# í† í¬ë‚˜ì´ì§•
train_dataset = dataset['train'].map(tokenize, remove_columns=dataset['train'].column_names)
```

### LoRA í•™ìŠµ

```python
# LoRA ì„¤ì •
lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=["q_proj", "v_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)

# ëª¨ë¸ ì¤€ë¹„
model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-2-7b-hf",
    load_in_8bit=True,
    device_map="auto"
)
model = get_peft_model(model, lora_config)

# í•™ìŠµ
training_args = TrainingArguments(
    output_dir="./llama2_dolly_lora",
    num_train_epochs=3,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    learning_rate=2e-4,
    fp16=True,
    logging_steps=10,
    save_total_limit=3,
    save_strategy="epoch",
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
)

trainer.train()

# ì €ì¥
model.save_pretrained("./llama2_dolly_lora")
```

### ì¶”ë¡ 

```python
from peft import PeftModel

# ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ
base_model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-2-7b-hf",
    load_in_8bit=True,
    device_map="auto"
)

# LoRA ì–´ëŒ‘í„° ë¡œë“œ
model = PeftModel.from_pretrained(base_model, "./llama2_dolly_lora")

# ì¶”ë¡  ëª¨ë“œ
model.eval()

# í…ŒìŠ¤íŠ¸
prompt = """### Instruction:
Explain what is machine learning.

### Response:
"""

inputs = tokenizer(prompt, return_tensors="pt").to("cuda")

with torch.no_grad():
    outputs = model.generate(
        **inputs,
        max_new_tokens=200,
        temperature=0.7,
        top_p=0.95,
        do_sample=True
    )

print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```

---

## ì„±ëŠ¥ ë¹„êµ

### ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰

**LLaMA-7B Fine-tuning:**

| ë°©ì‹ | ë©”ëª¨ë¦¬ (GB) | í•™ìŠµ ê°€ëŠ¥ |
|------|------------|----------|
| Full fine-tuning | 94 | A100 80GB ë¶€ì¡± |
| LoRA (fp16) | 18 | A100 40GB |
| QLoRA (4-bit) | 9 | RTX 4090 24GB |
| QLoRA (3-bit) | 6 | RTX 3090 24GB |

### ì •í™•ë„

**GLUE Benchmark (RoBERTa-base):**

| ë°©ì‹ | í‰ê·  ì ìˆ˜ |
|------|----------|
| Full fine-tuning | 87.6 |
| LoRA (r=8) | 87.2 (-0.4) |
| LoRA (r=16) | 87.5 (-0.1) |
| LoRA (r=32) | 87.6 (ë™ì¼) |

**ê²°ë¡ :** rank 16-32ë©´ full fine-tuningê³¼ ê±°ì˜ ë™ì¼!

### í•™ìŠµ ì†ë„

| ë°©ì‹ | ì‹œê°„ (epochë‹¹) |
|------|---------------|
| Full fine-tuning | 45ë¶„ |
| LoRA | 38ë¶„ (15% ë¹ ë¦„) |
| QLoRA | 52ë¶„ (15% ëŠë¦¼) |

---

## Best Practices

### 1. Rank ì„ íƒ

**ì¼ë°˜ ê°€ì´ë“œ:**
- **ì‘ì€ ëª¨ë¸** (< 1B): r=4-8
- **ì¤‘ê°„ ëª¨ë¸** (1-10B): r=8-16
- **í° ëª¨ë¸** (> 10B): r=16-64

**ì‹¤í—˜ ì¶”ì²œ:**
```python
ranks = [4, 8, 16, 32]
for r in ranks:
    # ì‘ì€ ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸
    test_lora(r, num_samples=1000)
```

### 2. Target Modules

**ì¶”ì²œ ìš°ì„ ìˆœìœ„:**
1. **Q, V**: í•„ìˆ˜ (ê°€ì¥ íš¨ê³¼ì )
2. **K, O**: ì¶”ê°€ ì„±ëŠ¥
3. **MLP layers**: ë” ë§ì€ ìš©ëŸ‰ í•„ìš”í•  ë•Œ

```python
# Minimal (ë¹ ë¦„, ì ì€ ë©”ëª¨ë¦¬)
target_modules = ["q_proj", "v_proj"]

# Recommended (ë°¸ëŸ°ìŠ¤)
target_modules = ["q_proj", "k_proj", "v_proj", "o_proj"]

# Full (ìµœê³  ì„±ëŠ¥, ëŠë¦¼)
target_modules = [
    "q_proj", "k_proj", "v_proj", "o_proj",
    "gate_proj", "up_proj", "down_proj"
]
```

### 3. Learning Rate

**LoRAëŠ” ë” ë†’ì€ learning rate í•„ìš”:**
- Full fine-tuning: 1e-5 ~ 5e-5
- LoRA: **3e-4 ~ 1e-3**

### 4. Alpha ì„¤ì •

**Alpha = 2 Ã— rank** (ì¼ë°˜ì )
- rank=8 â†’ alpha=16
- rank=16 â†’ alpha=32

---

## ì—¬ëŸ¬ LoRA ì–´ëŒ‘í„° ê´€ë¦¬

### 1. ì—¬ëŸ¬ íƒœìŠ¤í¬

```python
# ì˜ì–´ â†’ í•œêµ­ì–´
lora_en_ko = PeftModel.from_pretrained(base_model, "./lora_en_ko")

# ì˜ì–´ â†’ ì¼ë³¸ì–´
lora_en_ja = PeftModel.from_pretrained(base_model, "./lora_en_ja")

# ì½”ë“œ ìƒì„±
lora_code = PeftModel.from_pretrained(base_model, "./lora_code")
```

### 2. ë™ì  ì „í™˜

```python
from peft import PeftModel

class MultiLoRAModel:
    def __init__(self, base_model):
        self.base_model = base_model
        self.adapters = {}
        self.current_adapter = None
    
    def load_adapter(self, name, path):
        """ì–´ëŒ‘í„° ë¡œë“œ"""
        self.adapters[name] = PeftModel.from_pretrained(
            self.base_model, path
        )
    
    def switch_adapter(self, name):
        """ì–´ëŒ‘í„° ì „í™˜"""
        self.current_adapter = self.adapters[name]
    
    def generate(self, prompt):
        return self.current_adapter.generate(prompt)

# ì‚¬ìš©
model = MultiLoRAModel(base_model)
model.load_adapter("translate", "./lora_translate")
model.load_adapter("code", "./lora_code")

# ë²ˆì—­
model.switch_adapter("translate")
output = model.generate("Hello world")

# ì½”ë“œ ìƒì„±
model.switch_adapter("code")
output = model.generate("Write a function to sort a list")
```

### 3. ì–´ëŒ‘í„° ë³‘í•©

ì—¬ëŸ¬ LoRAë¥¼ í•˜ë‚˜ë¡œ:

```python
from peft import PeftModel

# LoRA 1
model1 = PeftModel.from_pretrained(base_model, "./lora1")

# LoRA 2 ì¶”ê°€
model1.load_adapter("./lora2", adapter_name="lora2")

# ê°€ì¤‘ì¹˜ í‰ê· 
model1.set_adapter(["default", "lora2"])  # ë‘˜ ë‹¤ í™œì„±í™”
```

---

## ìš”ì•½

**LoRA**ëŠ”:

1. **0.1-1%ì˜ íŒŒë¼ë¯¸í„°**ë§Œ í•™ìŠµ
2. **ì €ë­í¬ ë¶„í•´**: Î”W â‰ˆ BA
3. **ë©”ëª¨ë¦¬**: 10ë°° ì ˆì•½
4. **ì„±ëŠ¥**: Full fine-tuningê³¼ ë™ì¼
5. **ì†ë„**: ë¹„ìŠ·í•˜ê±°ë‚˜ ë” ë¹ ë¦„

**QLoRA**:
- 4-bit ì–‘ìí™” + LoRA
- 24GB GPUë¡œ 70B ëª¨ë¸ í•™ìŠµ!

**ì‚¬ìš©ì²˜**:
- Instruction tuning
- Domain adaptation
- Task-specific fine-tuning
- Multi-task learning

**í•µì‹¬**: ì ì€ ë¹„ìš©ìœ¼ë¡œ ê°•ë ¥í•œ ëª¨ë¸ ì»¤ìŠ¤í„°ë§ˆì´ì§•!

---

## ì‹œë¦¬ì¦ˆ ì™„ë£Œ! ğŸ‰

LLM ìµœì í™” ì‹œë¦¬ì¦ˆ:
1. **Paged Attention**: ë©”ëª¨ë¦¬ íš¨ìœ¨ 10ë°°
2. **KV Caching**: ì†ë„ 50-100ë°°
3. **LoRA**: í•™ìŠµ ë¹„ìš© 10ë°° ì ˆê°

ì´ì œ ì—¬ëŸ¬ë¶„ë„ íš¨ìœ¨ì ì¸ LLM inference & fine-tuningì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!

---

*ì§ˆë¬¸ì´ë‚˜ í”¼ë“œë°±ì€ [GitHub](https://github.com/yhna941)ì—ì„œ í™˜ì˜í•©ë‹ˆë‹¤!*
