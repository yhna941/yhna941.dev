---
title: "RAG #1: ê¸°ì´ˆ - Retrieval-Augmented Generation ì™„ì „ ì •ë³µ"
description: "LLMì˜ í•œê³„ë¥¼ ê·¹ë³µí•˜ëŠ” RAGì˜ ì›ë¦¬ì™€ êµ¬í˜„ì„ ì²˜ìŒë¶€í„° ëê¹Œì§€ ì•Œì•„ë´…ë‹ˆë‹¤."
pubDate: 2026-02-06
author: "Yh Na"
tags: ["rag", "retrieval", "llm", "vector-search", "embeddings"]
draft: false
---

# RAG #1: ê¸°ì´ˆ

**"LLMì—ê²Œ ì§€ì‹ì„ ì£¼ì…í•˜ë¼"**

ë¬¸ì œ:
```
User: "2024ë…„ 3ë¶„ê¸° ë§¤ì¶œì´ ì–¼ë§ˆì•¼?"
LLM: "ì£„ì†¡í•©ë‹ˆë‹¤. 2024ë…„ ë°ì´í„°ëŠ” í•™ìŠµë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."

í•´ê²° (RAG):
User: "2024ë…„ 3ë¶„ê¸° ë§¤ì¶œì´ ì–¼ë§ˆì•¼?"
â†’ [ê²€ìƒ‰] â†’ ê´€ë ¨ ë¬¸ì„œ ì°¾ê¸°
â†’ [ìƒì„±] â†’ "2024ë…„ 3ë¶„ê¸° ë§¤ì¶œì€ 120ì–µì›ì…ë‹ˆë‹¤."
```

---

## RAGë€?

### ì •ì˜

> **Retrieval-Augmented Generation**  
> = ê²€ìƒ‰(Retrieval) + ìƒì„±(Generation)

**ê¸°ë³¸ ì•„ì´ë””ì–´:**
```
1. ì§ˆë¬¸ ë°›ìŒ
2. ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ (Retrieval)
3. ë¬¸ì„œ + ì§ˆë¬¸ì„ LLMì— ì…ë ¥
4. ë‹µë³€ ìƒì„± (Generation)
```

### ì™œ í•„ìš”?

**LLMì˜ í•œê³„:**

1. **í•™ìŠµ ì‹œì  ì´í›„ ë°ì´í„° ëª¨ë¦„**
   ```
   GPT-4 (2023ë…„ 4ì›” í•™ìŠµ)
   â†’ 2024ë…„ ë‰´ìŠ¤? ëª¨ë¦„
   â†’ ì˜¤ëŠ˜ ë‚ ì”¨? ëª¨ë¦„
   ```

2. **íšŒì‚¬ ë‚´ë¶€ ë¬¸ì„œ ëª¨ë¦„**
   ```
   "ìš°ë¦¬ íšŒì‚¬ íœ´ê°€ ì •ì±…ì€?"
   â†’ LLM: ëª¨ë¦„ (í•™ìŠµ ì•ˆ ë¨)
   ```

3. **Hallucination (í™˜ê°)**
   ```
   "2024ë…„ ëŒ€í†µë ¹ì€?"
   â†’ LLM: "ê¹€ì² ìˆ˜ì…ë‹ˆë‹¤" (ì§€ì–´ëƒ„!)
   ```

**RAGë¡œ í•´ê²°:**
- ìµœì‹  ë°ì´í„° ì ‘ê·¼
- íšŒì‚¬ ë¬¸ì„œ í™œìš©
- ê·¼ê±° ê¸°ë°˜ ë‹µë³€ (í™˜ê° ê°ì†Œ)

---

## RAG ì•„í‚¤í…ì²˜

### ì „ì²´ íë¦„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          Indexing (ì˜¤í”„ë¼ì¸)         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                     â”‚
â”‚  Documents â†’ Chunking â†’ Embedding  â”‚
â”‚      â†“                      â†“       â”‚
â”‚  PDF, TXT, ...         Vector DB    â”‚
â”‚                       (Pinecone,    â”‚
â”‚                        Weaviate)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Retrieval (ì˜¨ë¼ì¸)           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                     â”‚
â”‚  User Query                         â”‚
â”‚      â†“                              â”‚
â”‚  Query Embedding                    â”‚
â”‚      â†“                              â”‚
â”‚  Vector Search (Top-K)              â”‚
â”‚      â†“                              â”‚
â”‚  Retrieved Documents                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Generation (ì˜¨ë¼ì¸)          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                     â”‚
â”‚  Query + Retrieved Docs             â”‚
â”‚      â†“                              â”‚
â”‚  Prompt Engineering                 â”‚
â”‚      â†“                              â”‚
â”‚  LLM (GPT-4, Claude)                â”‚
â”‚      â†“                              â”‚
â”‚  Final Answer                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 1. Document Indexing

### ë¬¸ì„œ ë¡œë”©

```python
from langchain.document_loaders import (
    PyPDFLoader,
    TextLoader,
    UnstructuredMarkdownLoader,
    WebBaseLoader
)

# PDF
loader = PyPDFLoader("company_policy.pdf")
documents = loader.load()

# ì›¹í˜ì´ì§€
loader = WebBaseLoader("https://example.com/docs")
documents = loader.load()

# ì—¬ëŸ¬ íŒŒì¼
from langchain.document_loaders import DirectoryLoader

loader = DirectoryLoader(
    "./docs",
    glob="**/*.md",
    loader_cls=UnstructuredMarkdownLoader
)
documents = loader.load()

print(f"Loaded {len(documents)} documents")
```

### ë¬¸ì„œ ë¶„í•  (Chunking)

**ì™œ ë¶„í• ?**
```
ì „ì²´ ë¬¸ì„œ (10,000 í† í°)
â†’ LLM context ì œí•œ (4,096 í† í°)
â†’ ì‘ì€ ì²­í¬ë¡œ ë‚˜ëˆ” (512 í† í°ì”©)
```

**ì „ëµ:**

```python
from langchain.text_splitter import (
    RecursiveCharacterTextSplitter,
    TokenTextSplitter
)

# 1. ë¬¸ì ê¸°ë°˜ (ê°€ì¥ ì¼ë°˜ì )
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,        # ì²­í¬ í¬ê¸°
    chunk_overlap=200,      # ì˜¤ë²„ë© (ë¬¸ë§¥ ìœ ì§€)
    separators=["\n\n", "\n", " ", ""]  # ë¶„í•  ìš°ì„ ìˆœìœ„
)

chunks = text_splitter.split_documents(documents)

# 2. í† í° ê¸°ë°˜ (ì •í™•í•œ í† í° ì œì–´)
token_splitter = TokenTextSplitter(
    chunk_size=512,
    chunk_overlap=50
)

chunks = token_splitter.split_documents(documents)

# ê²°ê³¼
for i, chunk in enumerate(chunks[:3]):
    print(f"Chunk {i}:")
    print(f"  Length: {len(chunk.page_content)}")
    print(f"  Content: {chunk.page_content[:100]}...")
    print(f"  Metadata: {chunk.metadata}")
```

**ì²­í¬ ì˜¤ë²„ë© ì¤‘ìš”!**

```
ì²­í¬ 1: "...ê¸°ì—… íœ´ê°€ ì •ì±…ì€ ì—°ì°¨ 15ì¼ì…ë‹ˆë‹¤."
              â†‘ ì˜¤ë²„ë© ì˜ì—­ â†“
ì²­í¬ 2: "ì—°ì°¨ 15ì¼ì…ë‹ˆë‹¤. ë³‘ê°€ëŠ” ë³„ë„ë¡œ..."

â†’ ë¬¸ë§¥ì´ ëŠê¸°ì§€ ì•ŠìŒ!
```

### ì„ë² ë”© (Embedding)

**ê°œë…:**
```
í…ìŠ¤íŠ¸ â†’ ë²¡í„° (ìˆ«ì ë°°ì—´)

"ê°•ì•„ì§€ê°€ ê·€ì—½ë‹¤" â†’ [0.2, 0.8, -0.3, ..., 0.5]  (1536ì°¨ì›)
"ê°œê°€ ì‚¬ë‘ìŠ¤ëŸ½ë‹¤" â†’ [0.3, 0.7, -0.2, ..., 0.6]  (ìœ ì‚¬!)

"ìë™ì°¨ê°€ ë¹ ë¥´ë‹¤" â†’ [-0.5, 0.1, 0.9, ..., -0.2] (ë‹¤ë¦„)
```

**êµ¬í˜„:**

```python
from langchain.embeddings import OpenAIEmbeddings, HuggingFaceEmbeddings

# 1. OpenAI Embeddings (text-embedding-3-small)
embeddings = OpenAIEmbeddings(
    model="text-embedding-3-small",
    openai_api_key="sk-..."
)

# 2. ì˜¤í”ˆì†ŒìŠ¤ (ë¬´ë£Œ!)
embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2"
)

# ì„ë² ë”© ìƒì„±
text = "ê°•ì•„ì§€ê°€ ê·€ì—½ë‹¤"
vector = embeddings.embed_query(text)

print(f"Dimension: {len(vector)}")  # 384 or 1536
print(f"Vector: {vector[:5]}...")   # [0.123, -0.456, ...]
```

**ì„ë² ë”© ëª¨ë¸ ë¹„êµ:**

| ëª¨ë¸ | ì°¨ì› | ì†ë„ | í’ˆì§ˆ | ë¹„ìš© |
|------|------|------|------|------|
| OpenAI text-embedding-3-small | 1536 | ë¹ ë¦„ | ìš°ìˆ˜ | ìœ ë£Œ |
| OpenAI text-embedding-3-large | 3072 | ëŠë¦¼ | ìµœê³  | ë¹„ì‹¸ |
| all-MiniLM-L6-v2 | 384 | ë§¤ìš° ë¹ ë¦„ | ê´œì°®ìŒ | ë¬´ë£Œ |
| bge-large-en | 1024 | ë³´í†µ | ìš°ìˆ˜ | ë¬´ë£Œ |

### Vector Database ì €ì¥

```python
from langchain.vectorstores import FAISS, Chroma, Pinecone

# 1. FAISS (ë¡œì»¬, ë¹ ë¦„)
from langchain.vectorstores import FAISS

vectorstore = FAISS.from_documents(
    documents=chunks,
    embedding=embeddings
)

# ì €ì¥
vectorstore.save_local("faiss_index")

# ë¡œë“œ
vectorstore = FAISS.load_local(
    "faiss_index",
    embeddings,
    allow_dangerous_deserialization=True
)

# 2. Chroma (ë¡œì»¬, ê°„í¸)
from langchain.vectorstores import Chroma

vectorstore = Chroma.from_documents(
    documents=chunks,
    embedding=embeddings,
    persist_directory="./chroma_db"
)

# 3. Pinecone (í´ë¼ìš°ë“œ, í”„ë¡œë•ì…˜)
import pinecone
from langchain.vectorstores import Pinecone

pinecone.init(
    api_key="your-api-key",
    environment="us-west1-gcp"
)

index_name = "company-docs"

vectorstore = Pinecone.from_documents(
    documents=chunks,
    embedding=embeddings,
    index_name=index_name
)
```

---

## 2. Retrieval

### ìœ ì‚¬ë„ ê²€ìƒ‰

```python
# ì§ˆë¬¸
query = "íœ´ê°€ëŠ” ë©°ì¹ ì´ì•¼?"

# ê²€ìƒ‰ (Top-3)
results = vectorstore.similarity_search(
    query,
    k=3  # ìƒìœ„ 3ê°œ
)

for i, doc in enumerate(results):
    print(f"\n=== Result {i+1} ===")
    print(f"Content: {doc.page_content}")
    print(f"Metadata: {doc.metadata}")
```

### ìŠ¤ì½”ì–´ í¬í•¨ ê²€ìƒ‰

```python
# ìœ ì‚¬ë„ ìŠ¤ì½”ì–´ í¬í•¨
results = vectorstore.similarity_search_with_score(
    query,
    k=3
)

for doc, score in results:
    print(f"Score: {score:.4f}")
    print(f"Content: {doc.page_content[:100]}...")
```

### MMR (Maximum Marginal Relevance)

**ë¬¸ì œ:** ìœ ì‚¬ë„ ê²€ìƒ‰ì€ ì¤‘ë³µëœ ê²°ê³¼ ë°˜í™˜

```
ì§ˆë¬¸: "íœ´ê°€ ì •ì±…"
ê²°ê³¼:
1. "íœ´ê°€ëŠ” 15ì¼ì…ë‹ˆë‹¤."
2. "íœ´ê°€ëŠ” ì—°ì°¨ 15ì¼ì…ë‹ˆë‹¤." â† ì¤‘ë³µ!
3. "íœ´ê°€ëŠ” 15ì¼ ì œê³µë©ë‹ˆë‹¤." â† ë˜ ì¤‘ë³µ!
```

**MMR:** ìœ ì‚¬í•˜ë©´ì„œë„ ë‹¤ì–‘í•œ ê²°ê³¼

```python
# MMR ê²€ìƒ‰
results = vectorstore.max_marginal_relevance_search(
    query,
    k=3,
    fetch_k=10,      # ë¨¼ì € 10ê°œ í›„ë³´ ê°€ì ¸ì˜¤ê¸°
    lambda_mult=0.5  # 0=ë‹¤ì–‘ì„±, 1=ìœ ì‚¬ì„±
)

# ê²°ê³¼ê°€ ë” ë‹¤ì–‘í•´ì§!
```

---

## 3. Generation

### Prompt êµ¬ì„±

```python
from langchain.prompts import PromptTemplate

template = """ë‹¹ì‹ ì€ íšŒì‚¬ ì •ì±…ì„ ì•ˆë‚´í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

ì•„ë˜ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.
ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ "ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µí•˜ì„¸ìš”.

ë¬¸ì„œ:
{context}

ì§ˆë¬¸: {question}

ë‹µë³€:"""

prompt = PromptTemplate(
    template=template,
    input_variables=["context", "question"]
)
```

### ì „ì²´ RAG ì²´ì¸

```python
from langchain.chains import RetrievalQA
from langchain.llms import OpenAI

# LLM
llm = OpenAI(
    model="gpt-3.5-turbo",
    temperature=0  # ì¼ê´€ëœ ë‹µë³€
)

# RAG ì²´ì¸
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",  # ëª¨ë“  ë¬¸ì„œë¥¼ í•˜ë‚˜ë¡œ í•©ì¹¨
    retriever=vectorstore.as_retriever(
        search_kwargs={"k": 3}
    ),
    return_source_documents=True  # ì¶œì²˜ ë°˜í™˜
)

# ì§ˆë¬¸
query = "ì—°ì°¨ëŠ” ë©°ì¹ ì´ì•¼?"
result = qa_chain({"query": query})

print(f"ë‹µë³€: {result['result']}")
print(f"\nì¶œì²˜:")
for doc in result['source_documents']:
    print(f"- {doc.metadata['source']}: {doc.page_content[:100]}...")
```

### Chain Types

**1. Stuff (ê¸°ë³¸)**

```python
# ëª¨ë“  ë¬¸ì„œë¥¼ í•˜ë‚˜ë¡œ í•©ì³ì„œ ì „ë‹¬
context = "\n\n".join([doc.page_content for doc in retrieved_docs])
prompt = f"Context: {context}\n\nQuestion: {query}"
```

- ì¥ì : ê°„ë‹¨, ë¹ ë¦„
- ë‹¨ì : ë¬¸ì„œ ë§ìœ¼ë©´ context ì´ˆê³¼

**2. Map-Reduce**

```python
# ê° ë¬¸ì„œë§ˆë‹¤ ë‹µë³€ â†’ ê²°í•©
for doc in retrieved_docs:
    partial_answer = llm(f"Context: {doc}\nQuestion: {query}")

final_answer = llm(f"Combine these: {partial_answers}")
```

- ì¥ì : ë§ì€ ë¬¸ì„œ ì²˜ë¦¬ ê°€ëŠ¥
- ë‹¨ì : LLM í˜¸ì¶œ ë§ìŒ (ë¹„ìš©â†‘)

**3. Refine**

```python
# ìˆœì°¨ì ìœ¼ë¡œ ë‹µë³€ ê°œì„ 
answer = llm(f"Context: {doc1}\nQuestion: {query}")
answer = llm(f"Previous: {answer}\nNew context: {doc2}\nRefine:")
...
```

- ì¥ì : ì ì§„ì  ê°œì„ 
- ë‹¨ì : ìˆœì°¨ ì²˜ë¦¬ (ëŠë¦¼)

---

## 4. ì™„ì „í•œ RAG êµ¬í˜„

```python
from langchain.document_loaders import DirectoryLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain.llms import OpenAI

class RAGSystem:
    def __init__(self, docs_path, persist_dir="./faiss_index"):
        self.docs_path = docs_path
        self.persist_dir = persist_dir
        self.vectorstore = None
        self.qa_chain = None
    
    def index_documents(self):
        """ë¬¸ì„œ ì¸ë±ì‹±"""
        print("ğŸ“š Loading documents...")
        
        # 1. ë¬¸ì„œ ë¡œë“œ
        loader = DirectoryLoader(
            self.docs_path,
            glob="**/*.md",
            show_progress=True
        )
        documents = loader.load()
        print(f"Loaded {len(documents)} documents")
        
        # 2. ì²­í¬ ë¶„í• 
        print("âœ‚ï¸ Splitting into chunks...")
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200
        )
        chunks = text_splitter.split_documents(documents)
        print(f"Created {len(chunks)} chunks")
        
        # 3. ì„ë² ë”© & ì €ì¥
        print("ğŸ”¢ Creating embeddings...")
        embeddings = OpenAIEmbeddings()
        
        self.vectorstore = FAISS.from_documents(
            chunks,
            embeddings
        )
        
        # 4. ë””ìŠ¤í¬ì— ì €ì¥
        self.vectorstore.save_local(self.persist_dir)
        print(f"âœ… Saved to {self.persist_dir}")
    
    def load_index(self):
        """ì €ì¥ëœ ì¸ë±ìŠ¤ ë¡œë“œ"""
        embeddings = OpenAIEmbeddings()
        self.vectorstore = FAISS.load_local(
            self.persist_dir,
            embeddings,
            allow_dangerous_deserialization=True
        )
        print(f"âœ… Loaded index from {self.persist_dir}")
    
    def setup_qa_chain(self):
        """QA ì²´ì¸ ì„¤ì •"""
        llm = OpenAI(
            model="gpt-3.5-turbo",
            temperature=0
        )
        
        self.qa_chain = RetrievalQA.from_chain_type(
            llm=llm,
            chain_type="stuff",
            retriever=self.vectorstore.as_retriever(
                search_type="mmr",  # MMR ì‚¬ìš©
                search_kwargs={"k": 3}
            ),
            return_source_documents=True
        )
        print("âœ… QA chain ready")
    
    def query(self, question: str):
        """ì§ˆë¬¸ ë‹µë³€"""
        if not self.qa_chain:
            self.setup_qa_chain()
        
        print(f"\nâ“ Question: {question}")
        
        result = self.qa_chain({"query": question})
        
        print(f"\nğŸ’¡ Answer: {result['result']}")
        print(f"\nğŸ“„ Sources:")
        for i, doc in enumerate(result['source_documents'], 1):
            print(f"\n{i}. {doc.metadata.get('source', 'Unknown')}")
            print(f"   {doc.page_content[:200]}...")
        
        return result

# ì‚¬ìš©
rag = RAGSystem("./company_docs")

# ì²« ì‹¤í–‰: ì¸ë±ì‹±
# rag.index_documents()

# ì´í›„: ë¡œë“œë§Œ
rag.load_index()

# ì§ˆë¬¸
rag.query("íœ´ê°€ëŠ” ë©°ì¹ ì´ì•¼?")
rag.query("ì¬íƒê·¼ë¬´ ì •ì±…ì€?")
```

---

## 5. ê³ ê¸‰ ê¸°ë²•

### í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (Keyword + Semantic)

```python
from langchain.retrievers import BM25Retriever, EnsembleRetriever

# Keyword ê²€ìƒ‰ (BM25)
bm25_retriever = BM25Retriever.from_documents(chunks)
bm25_retriever.k = 3

# Semantic ê²€ìƒ‰ (Vector)
vector_retriever = vectorstore.as_retriever(search_kwargs={"k": 3})

# ê²°í•© (Ensemble)
ensemble_retriever = EnsembleRetriever(
    retrievers=[bm25_retriever, vector_retriever],
    weights=[0.5, 0.5]  # ë™ë“±í•œ ê°€ì¤‘ì¹˜
)

# ê²€ìƒ‰
results = ensemble_retriever.get_relevant_documents(query)
```

### Re-ranking

```python
from sentence_transformers import CrossEncoder

# 1ì°¨ ê²€ìƒ‰ (Top-10)
candidates = vectorstore.similarity_search(query, k=10)

# 2ì°¨ re-ranking
reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-12-v2')

pairs = [[query, doc.page_content] for doc in candidates]
scores = reranker.predict(pairs)

# ìŠ¤ì½”ì–´ ìˆœìœ¼ë¡œ ì •ë ¬
ranked_docs = sorted(
    zip(candidates, scores),
    key=lambda x: x[1],
    reverse=True
)

# Top-3
top_docs = [doc for doc, score in ranked_docs[:3]]
```

---

## í‰ê°€ (Evaluation)

### Retrieval í‰ê°€

```python
from ragas.metrics import context_precision, context_recall

# Ground truth
ground_truth = [
    {
        'question': 'íœ´ê°€ëŠ” ë©°ì¹ ?',
        'ground_truth': 'ì—°ì°¨ 15ì¼',
        'retrieved_contexts': [doc.page_content for doc in retrieved_docs]
    }
]

# í‰ê°€
results = evaluate(
    ground_truth,
    metrics=[context_precision, context_recall]
)

print(f"Precision: {results['context_precision']}")
print(f"Recall: {results['context_recall']}")
```

### Generation í‰ê°€

```python
# Faithfulness (ì¶©ì‹¤ì„±)
# ë‹µë³€ì´ ë¬¸ì„œì— ê·¼ê±°í–ˆëŠ”ê°€?

# Answer Relevance
# ë‹µë³€ì´ ì§ˆë¬¸ì— ì ì ˆí•œê°€?

from ragas.metrics import faithfulness, answer_relevancy

results = evaluate(
    test_dataset,
    metrics=[faithfulness, answer_relevancy]
)
```

---

## ìš”ì•½

**RAG íŒŒì´í”„ë¼ì¸:**

1. **Indexing**: ë¬¸ì„œ â†’ ì²­í¬ â†’ ì„ë² ë”© â†’ Vector DB
2. **Retrieval**: ì§ˆë¬¸ â†’ ìœ ì‚¬ë„ ê²€ìƒ‰ â†’ Top-K ë¬¸ì„œ
3. **Generation**: ë¬¸ì„œ + ì§ˆë¬¸ â†’ LLM â†’ ë‹µë³€

**í•µì‹¬ ì»´í¬ë„ŒíŠ¸:**
- Document Loader
- Text Splitter
- Embedding Model
- Vector Database
- LLM

**ì¥ì :**
- ìµœì‹  ì •ë³´ í™œìš©
- í™˜ê° ê°ì†Œ
- ì¶œì²˜ ì œê³µ

**ë‹¤ìŒ ê¸€:**
- **RAG #2**: Production RAG (ì„±ëŠ¥ ìµœì í™”, ìºì‹±, ëª¨ë‹ˆí„°ë§)
- **RAG #3**: Advanced RAG (Query Rewriting, HyDE, Self-RAG)

ê¸°ëŒ€í•´ì£¼ì„¸ìš”! ğŸš€

---

*ì§ˆë¬¸ì´ë‚˜ í”¼ë“œë°±ì€ [GitHub](https://github.com/yhna941)ì—ì„œ í™˜ì˜í•©ë‹ˆë‹¤!*
