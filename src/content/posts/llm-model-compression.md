---
title: "LLM Inference ìµœì í™” #10: Model Compression - ì‘ì§€ë§Œ ê°•ë ¥í•˜ê²Œ"
description: "Pruning, Distillation, Low-rank decompositionìœ¼ë¡œ ëª¨ë¸ í¬ê¸°ë¥¼ ì¤„ì´ë©´ì„œ ì„±ëŠ¥ì„ ìœ ì§€í•˜ëŠ” ë°©ë²•ì„ ì•Œì•„ë´…ë‹ˆë‹¤."
pubDate: 2026-02-06
author: "Yh Na"
tags: ["llm", "optimization", "compression", "pruning", "distillation"]
draft: false
---

# LLM Inference ìµœì í™” #10: Model Compression

LLaMA-7BëŠ” 13GBì…ë‹ˆë‹¤. í•˜ì§€ë§Œ **ì‹¤ì œë¡œ í•„ìš”í•œ íŒŒë¼ë¯¸í„°ëŠ” ì–¼ë§ˆë‚˜ ë ê¹Œìš”?**

ì—°êµ¬ì— ë”°ë¥´ë©´:
- **20-30%ëŠ” pruning ê°€ëŠ¥** (ì •í™•ë„ ê±°ì˜ ìœ ì§€)
- **Student ëª¨ë¸ (1/4 í¬ê¸°)**ì´ teacherì˜ 95% ì„±ëŠ¥
- **Low-rankë¡œ 50% íŒŒë¼ë¯¸í„°** ì ˆê°

**Model Compression**ì€ ì´ë¥¼ ì‹¤í˜„í•©ë‹ˆë‹¤.

---

## ì••ì¶• ë°©ë²• ê°œìš”

| ë°©ë²• | ì••ì¶•ë¥  | ì •í™•ë„ | ì¶”ë¡  ì†ë„ | ë‚œì´ë„ |
|------|--------|--------|----------|--------|
| Quantization | 75% | 98-99% | 2-3x | ì‰¬ì›€ |
| **Pruning** | 50-70% | 95-98% | 1.5-2x | ì¤‘ê°„ |
| **Distillation** | 75% | 90-95% | 4x | ì–´ë ¤ì›€ |
| Low-rank | 40-60% | 98% | 1.2x | ì¤‘ê°„ |

**ì´ë²ˆ ê¸€:** Pruning + Distillation + Low-rank

---

## Pruning (ê°€ì§€ì¹˜ê¸°)

### í•µì‹¬ ì•„ì´ë””ì–´

> **ì¤‘ìš”í•˜ì§€ ì•Šì€ ê°€ì¤‘ì¹˜ë¥¼ 0ìœ¼ë¡œ ë§Œë“ ë‹¤**

```python
# ì˜ˆ: ì‘ì€ ê°€ì¤‘ì¹˜ ì œê±°
threshold = 0.01
mask = torch.abs(weight) > threshold
pruned_weight = weight * mask

# 50%ê°€ 0ì´ ë¨!
```

### Pruning ì¢…ë¥˜

**1. Unstructured Pruning**
```python
# ê°œë³„ ê°€ì¤‘ì¹˜ ë‹¨ìœ„
weight: [4096, 4096]

# 50% pruning
pruned: [4096, 4096] with 50% zeros

# ì €ì¥: Sparse matrix (COO format)
indices: [[row1, col1], [row2, col2], ...]
values: [val1, val2, ...]
```

**ì¥ì :** ë†’ì€ ì••ì¶•ë¥   
**ë‹¨ì :** í•˜ë“œì›¨ì–´ ê°€ì† ì–´ë ¤ì›€

**2. Structured Pruning**
```python
# í–‰/ì—´/ì±„ë„ ë‹¨ìœ„
weight: [4096, 4096]

# ì±„ë„ 50% pruning
pruned: [4096, 2048]  # Dense!
```

**ì¥ì :** í•˜ë“œì›¨ì–´ ì¹œí™”ì   
**ë‹¨ì :** ì••ì¶•ë¥  ë‚®ìŒ

**3. N:M Sparsity**
```python
# Nê°œ ì¤‘ Mê°œë§Œ ìœ ì§€
# 2:4 = 4ê°œ ì¤‘ 2ê°œë§Œ ìœ ì§€

weight = [0.1, 0.5, 0.2, 0.8]
# â†’ [0, 0.5, 0, 0.8]  (Top-2 ìœ ì§€)

# NVIDIA A100 í•˜ë“œì›¨ì–´ ì§€ì›!
```

**ì¥ì :** í•˜ë“œì›¨ì–´ ê°€ì† + ì••ì¶•  
**ë‹¨ì :** ì œì•½ì 

---

## Magnitude-based Pruning

### ì•Œê³ ë¦¬ì¦˜

```python
def magnitude_pruning(model, sparsity=0.5):
    """
    ì‘ì€ ê°€ì¤‘ì¹˜ë¶€í„° ì œê±°
    
    Args:
        sparsity: ì œê±°í•  ë¹„ìœ¨ (0.5 = 50%)
    """
    for name, param in model.named_parameters():
        if 'weight' in name:
            # ê°€ì¤‘ì¹˜ í¬ê¸° ê³„ì‚°
            importance = torch.abs(param.data)
            
            # Threshold ê³„ì‚° (í•˜ìœ„ 50%)
            threshold = torch.quantile(importance, sparsity)
            
            # Mask ìƒì„±
            mask = importance > threshold
            
            # Pruning
            param.data *= mask
            
            # Mask ì €ì¥ (gradient ê³„ì‚° ì‹œ í•„ìš”)
            param.register_buffer('mask', mask)
```

### Iterative Pruning

í•œ ë²ˆì— í¬ê²Œ pruningí•˜ë©´ ì •í™•ë„ ê¸‰ë½. ì ì§„ì ìœ¼ë¡œ:

```python
def iterative_pruning(model, dataset, target_sparsity=0.9, steps=10):
    """
    ì ì§„ì  pruning + fine-tuning
    """
    current_sparsity = 0.0
    step_size = target_sparsity / steps
    
    for step in range(steps):
        # 1. Prune
        current_sparsity += step_size
        magnitude_pruning(model, sparsity=current_sparsity)
        
        # 2. Fine-tune
        fine_tune(model, dataset, epochs=1)
        
        # 3. Evaluate
        acc = evaluate(model, val_dataset)
        print(f"Step {step}: Sparsity={current_sparsity:.1%}, Acc={acc:.2%}")
    
    return model
```

---

## Structured Pruning

### Channel Pruning

ì±„ë„ ë‹¨ìœ„ë¡œ ì œê±°:

```python
def channel_pruning(layer, num_channels_to_prune):
    """
    CNN/Transformerì—ì„œ ì±„ë„ pruning
    
    Args:
        layer: Conv or Linear layer
        num_channels_to_prune: ì œê±°í•  ì±„ë„ ìˆ˜
    """
    # Channel importance ê³„ì‚°
    weight = layer.weight.data  # [out_channels, in_channels, ...]
    channel_norms = torch.norm(weight, dim=(1, 2, 3))  # L2 norm per channel
    
    # ì¤‘ìš”ë„ ë‚®ì€ ì±„ë„ ì„ íƒ
    _, indices = torch.sort(channel_norms)
    prune_indices = indices[:num_channels_to_prune]
    
    # ìƒˆë¡œìš´ ê°€ì¤‘ì¹˜ ìƒì„± (ì±„ë„ ì œê±°)
    keep_mask = torch.ones(weight.size(0), dtype=torch.bool)
    keep_mask[prune_indices] = False
    
    new_weight = weight[keep_mask]
    
    # ë ˆì´ì–´ êµì²´
    out_channels = weight.size(0) - num_channels_to_prune
    new_layer = nn.Conv2d(
        layer.in_channels,
        out_channels,
        layer.kernel_size,
        # ... other params
    )
    new_layer.weight.data = new_weight
    
    return new_layer
```

### Head Pruning (Attention)

Attention head ì œê±°:

```python
def prune_attention_heads(attention_layer, num_heads_to_prune):
    """
    Multi-head attentionì—ì„œ head pruning
    """
    num_heads = attention_layer.num_heads
    head_dim = attention_layer.head_dim
    
    # Head importance ê³„ì‚° (Taylor approximation)
    head_importance = []
    for h in range(num_heads):
        # Head hì˜ gradient Ã— activation
        grad = attention_layer.get_head_gradient(h)
        act = attention_layer.get_head_activation(h)
        importance = (grad * act).sum()
        head_importance.append(importance)
    
    # ë‚®ì€ importance head ì œê±°
    head_importance = torch.tensor(head_importance)
    _, indices = torch.sort(head_importance)
    prune_indices = indices[:num_heads_to_prune]
    
    # QKV ê°€ì¤‘ì¹˜ ì¬êµ¬ì„±
    new_num_heads = num_heads - num_heads_to_prune
    new_qkv_weight = remove_heads_from_qkv(
        attention_layer.qkv.weight,
        prune_indices,
        num_heads,
        head_dim
    )
    
    # ìƒˆ ë ˆì´ì–´ ìƒì„±
    new_attention = MultiHeadAttention(
        embed_dim=attention_layer.embed_dim,
        num_heads=new_num_heads
    )
    new_attention.qkv.weight.data = new_qkv_weight
    
    return new_attention
```

---

## Knowledge Distillation

### í•µì‹¬ ì•„ì´ë””ì–´

> **í° ëª¨ë¸(teacher)ì˜ ì§€ì‹ì„ ì‘ì€ ëª¨ë¸(student)ì—ê²Œ ì „ë‹¬**

```
Teacher (70B): "Cat"ì— 90% í™•ë¥ 
Student (7B): Teacherë¥¼ ëª¨ë°©í•˜ë„ë¡ í•™ìŠµ
```

### Distillation Loss

```python
def distillation_loss(student_logits, teacher_logits, labels, T=2.0, alpha=0.5):
    """
    Hinton's Distillation Loss
    
    Args:
        T: Temperature (ë†’ì„ìˆ˜ë¡ soft)
        alpha: Teacher loss ê°€ì¤‘ì¹˜
    """
    # 1. Hard loss (ì •ë‹µ ë ˆì´ë¸”)
    hard_loss = F.cross_entropy(student_logits, labels)
    
    # 2. Soft loss (teacher í™•ë¥  ë¶„í¬)
    student_soft = F.log_softmax(student_logits / T, dim=-1)
    teacher_soft = F.softmax(teacher_logits / T, dim=-1)
    
    soft_loss = F.kl_div(
        student_soft,
        teacher_soft,
        reduction='batchmean'
    ) * (T ** 2)
    
    # 3. ê²°í•©
    total_loss = alpha * soft_loss + (1 - alpha) * hard_loss
    
    return total_loss


# í•™ìŠµ
for batch in dataloader:
    inputs, labels = batch
    
    # Teacher prediction (frozen)
    with torch.no_grad():
        teacher_logits = teacher_model(inputs)
    
    # Student prediction
    student_logits = student_model(inputs)
    
    # Loss
    loss = distillation_loss(
        student_logits,
        teacher_logits,
        labels,
        T=3.0,
        alpha=0.7
    )
    
    # Backprop
    loss.backward()
    optimizer.step()
```

### Feature Distillation

ì¤‘ê°„ ë ˆì´ì–´ë„ ëª¨ë°©:

```python
class FeatureDistillationLoss(nn.Module):
    def __init__(self, alpha=0.5, beta=0.5):
        super().__init__()
        self.alpha = alpha  # Logits
        self.beta = beta    # Features
    
    def forward(self, student_outputs, teacher_outputs, labels):
        # 1. Logits distillation
        logit_loss = distillation_loss(
            student_outputs['logits'],
            teacher_outputs['logits'],
            labels
        )
        
        # 2. Feature distillation (hidden states)
        feature_loss = 0
        for s_feat, t_feat in zip(
            student_outputs['hidden_states'],
            teacher_outputs['hidden_states']
        ):
            # MSE loss between features
            feature_loss += F.mse_loss(s_feat, t_feat)
        
        feature_loss /= len(student_outputs['hidden_states'])
        
        # 3. Total
        return self.alpha * logit_loss + self.beta * feature_loss
```

---

## Distillation ì „ëµ

### 1. Standard Distillation

```python
# Teacher: 70B
teacher = AutoModelForCausalLM.from_pretrained("llama-70b")
teacher.eval()

# Student: 7B
student = AutoModelForCausalLM.from_pretrained("llama-7b")

# Distill
for epoch in range(3):
    for batch in dataloader:
        with torch.no_grad():
            teacher_out = teacher(batch)
        
        student_out = student(batch)
        loss = distillation_loss(student_out, teacher_out, batch['labels'])
        
        loss.backward()
        optimizer.step()
```

### 2. On-the-fly Distillation

ì‹¤ì‹œê°„ ìƒì„± ë°ì´í„°ë¡œ:

```python
def on_the_fly_distillation(teacher, student, prompts):
    """Teacherê°€ ìƒì„±í•œ ë°ì´í„°ë¡œ í•™ìŠµ"""
    for prompt in prompts:
        # Teacher ìƒì„±
        with torch.no_grad():
            teacher_outputs = teacher.generate(
                prompt,
                max_new_tokens=100,
                return_dict_in_generate=True,
                output_scores=True
            )
        
        generated_text = teacher_outputs.sequences
        teacher_logits = teacher_outputs.scores
        
        # Student í•™ìŠµ
        student_logits = student(generated_text)
        loss = F.kl_div(
            F.log_softmax(student_logits, dim=-1),
            F.softmax(teacher_logits, dim=-1),
            reduction='batchmean'
        )
        
        loss.backward()
        optimizer.step()
```

### 3. Task-specific Distillation

íŠ¹ì • íƒœìŠ¤í¬ì— ì§‘ì¤‘:

```python
# ì˜ˆ: Summarization
def distill_for_summarization(teacher, student, dataset):
    for article, summary in dataset:
        # Teacher: ìš”ì•½ ìƒì„±
        with torch.no_grad():
            teacher_summary = teacher.generate(article)
            teacher_logits = teacher(article).logits
        
        # Student: Teacher ëª¨ë°©
        student_logits = student(article).logits
        loss = distillation_loss(
            student_logits,
            teacher_logits,
            labels=None  # No ground truth needed!
        )
        
        loss.backward()
        optimizer.step()
```

---

## Low-Rank Decomposition

### Matrix Factorization

í° í–‰ë ¬ì„ ì‘ì€ í–‰ë ¬ ê³±ìœ¼ë¡œ:

```python
# ì›ë³¸
W: [4096, 4096]  # 16M parameters

# Low-rank decomposition
W â‰ˆ U @ V
U: [4096, 256]   # 1M
V: [256, 4096]   # 1M
# Total: 2M (8ë°° ì ˆê°!)
```

### SVD-based Decomposition

```python
def low_rank_decompose(weight, rank):
    """
    SVDë¡œ low-rank ë¶„í•´
    
    Args:
        weight: [out_features, in_features]
        rank: Target rank
    """
    # SVD
    U, S, Vh = torch.linalg.svd(weight, full_matrices=False)
    
    # Top-r ìœ ì§€
    U_r = U[:, :rank]
    S_r = S[:rank]
    V_r = Vh[:rank, :]
    
    # ì¬êµ¬ì„±
    U_scaled = U_r * torch.sqrt(S_r)
    V_scaled = torch.sqrt(S_r).unsqueeze(1) * V_r
    
    return U_scaled, V_scaled


# ëª¨ë¸ì— ì ìš©
class LowRankLinear(nn.Module):
    def __init__(self, in_features, out_features, rank):
        super().__init__()
        self.U = nn.Linear(in_features, rank, bias=False)
        self.V = nn.Linear(rank, out_features, bias=True)
    
    def forward(self, x):
        return self.V(self.U(x))


def convert_to_low_rank(model, rank=256):
    """ëª¨ë¸ì˜ Linearë¥¼ Low-rankë¡œ êµì²´"""
    for name, module in model.named_modules():
        if isinstance(module, nn.Linear):
            in_feat = module.in_features
            out_feat = module.out_features
            
            # Decompose
            U, V = low_rank_decompose(module.weight.data, rank)
            
            # ìƒˆ ë ˆì´ì–´
            new_module = LowRankLinear(in_feat, out_feat, rank)
            new_module.U.weight.data = U.T
            new_module.V.weight.data = V
            if module.bias is not None:
                new_module.V.bias.data = module.bias.data
            
            # êµì²´
            parent = get_parent_module(model, name)
            setattr(parent, name.split('.')[-1], new_module)
    
    return model
```

---

## ì‹¤ì „ ì˜ˆì œ

### 1. LLM Pruning (Wanda)

```python
# Wanda: Weight + Activation pruning
def wanda_pruning(model, calibration_data, sparsity=0.5):
    """
    Activation-aware pruning
    """
    # 1. Activation ìˆ˜ì§‘
    activations = collect_activations(model, calibration_data)
    
    # 2. Layer-wise pruning
    for name, module in model.named_modules():
        if isinstance(module, nn.Linear):
            weight = module.weight.data
            act = activations[name]
            
            # Importance = |weight| Ã— activation magnitude
            importance = torch.abs(weight) * act.mean(dim=0)
            
            # Threshold
            threshold = torch.quantile(importance, sparsity)
            mask = importance > threshold
            
            # Apply mask
            module.weight.data *= mask
    
    return model


# ì‚¬ìš©
model = AutoModelForCausalLM.from_pretrained("llama-7b")
calibration_data = load_dataset("c4", split="train[:1000]")

pruned_model = wanda_pruning(model, calibration_data, sparsity=0.5)

# 50% sparse, ì •í™•ë„ ê±°ì˜ ìœ ì§€!
```

### 2. DistilBERT (ì‹¤ì œ ì‚¬ë¡€)

```python
from transformers import DistilBertModel, BertModel

# Teacher: BERT-base (110M)
teacher = BertModel.from_pretrained("bert-base-uncased")

# Student: DistilBERT (66M, 40% ì‘ìŒ)
student = DistilBertModel.from_pretrained("distilbert-base-uncased")

# Distillation
class DistilBERTTrainer:
    def __init__(self, teacher, student):
        self.teacher = teacher.eval()
        self.student = student
        self.ce_loss = nn.CrossEntropyLoss()
        self.mse_loss = nn.MSELoss()
    
    def train_step(self, batch):
        inputs = batch['input_ids']
        labels = batch['labels']
        
        # Teacher (frozen)
        with torch.no_grad():
            teacher_outputs = self.teacher(inputs, output_hidden_states=True)
        
        # Student
        student_outputs = self.student(inputs, output_hidden_states=True)
        
        # 1. Hard loss (task-specific)
        hard_loss = self.ce_loss(student_outputs.logits, labels)
        
        # 2. Soft loss (distillation)
        soft_loss = F.kl_div(
            F.log_softmax(student_outputs.logits / 2.0, dim=-1),
            F.softmax(teacher_outputs.logits / 2.0, dim=-1),
            reduction='batchmean'
        ) * 4.0
        
        # 3. Hidden state loss
        hidden_loss = 0
        for s_hidden, t_hidden in zip(
            student_outputs.hidden_states,
            teacher_outputs.hidden_states
        ):
            hidden_loss += self.mse_loss(s_hidden, t_hidden)
        
        # Total
        loss = 0.5 * hard_loss + 0.5 * soft_loss + 0.1 * hidden_loss
        
        return loss

# ê²°ê³¼: 97% BERT ì„±ëŠ¥, 40% ì‘ìŒ, 60% ë¹ ë¦„!
```

---

## ë²¤ì¹˜ë§ˆí¬

### Pruning

**LLaMA-7B, WikiText-2:**

| ë°©ë²• | Sparsity | Perplexity | âˆ† |
|------|----------|-----------|---|
| Dense | 0% | 5.68 | - |
| Magnitude | 50% | 5.95 | +4.8% |
| Wanda | 50% | 5.74 | +1.1% âœ… |
| Wanda | 70% | 6.12 | +7.7% |

### Distillation

**GPT-2 â†’ DistilGPT-2:**

| ëª¨ë¸ | Size | Speed | Accuracy |
|------|------|-------|----------|
| GPT-2 | 117M | 1x | 100% |
| DistilGPT-2 | 82M (70%) | 1.5x | 97% |

### Low-Rank

**LLaMA-7B, rank=256:**

| Layer | Original | Low-rank | Compression |
|-------|----------|----------|-------------|
| Q/K/V | 16M | 3M | 5.3x |
| MLP | 64M | 16M | 4x |
| **Total** | 7B | **4.2B** | **1.7x** |

---

## ì¡°í•©: Pruning + Quantization + Distillation

ìµœê³  ì••ì¶•:

```python
# 1. Distillation (70B â†’ 7B)
student = distill(teacher_70b, target_size=7B)
# 7B, 95% accuracy

# 2. Pruning (7B â†’ 3.5B)
student = prune(student, sparsity=0.5)
# 3.5B, 93% accuracy

# 3. Quantization (3.5B INT8)
student = quantize(student, bits=8)
# 1.75 GB, 93% accuracy

# ì›ë³¸ 70B (140GB) â†’ 1.75GB (80ë°° ì••ì¶•!)
```

---

## Best Practices

### 1. Pruning ìˆœì„œ

```python
# 1. Global magnitude pruning (quick win)
# 2. Fine-tune
# 3. Layer-wise structured pruning
# 4. Fine-tune again
```

### 2. Distillation Tips

```python
# Temperature ì„ íƒ
T = 3.0  # ì¼ë°˜ì 
T = 5.0  # í° ëª¨ë¸ ì°¨ì´
T = 2.0  # ì‘ì€ ëª¨ë¸ ì°¨ì´

# Alpha ì„ íƒ
alpha = 0.7  # Teacherì— ì§‘ì¤‘
alpha = 0.5  # ë°¸ëŸ°ìŠ¤
alpha = 0.3  # Hard labels ì¤‘ì‹œ
```

### 3. ê²€ì¦

```python
# í•­ìƒ ì—¬ëŸ¬ ë©”íŠ¸ë¦­ í™•ì¸
metrics = {
    'perplexity': evaluate_perplexity(model),
    'accuracy': evaluate_accuracy(model, tasks),
    'speed': measure_throughput(model),
    'memory': measure_memory(model)
}
```

---

## ìš”ì•½

**Model Compression**ì€:

1. **Pruning**: ê°€ì¤‘ì¹˜ ì œê±° (50-70% ì••ì¶•)
2. **Distillation**: ì‘ì€ ëª¨ë¸ì— ì§€ì‹ ì „ë‹¬ (75% ì••ì¶•)
3. **Low-rank**: í–‰ë ¬ ë¶„í•´ (40-60% ì••ì¶•)

**ì¡°í•© íš¨ê³¼:**
- Pruning + Quantization: **8ë°° ì••ì¶•**
- Distillation + Quantization: **16ë°° ì••ì¶•**
- All: **80ë°° ì´ìƒ** ê°€ëŠ¥!

**ì‚¬ìš©ì²˜:**
- Edge devices (ëª¨ë°”ì¼, IoT)
- ë¹„ìš© ì ˆê° (API í˜¸ìŠ¤íŒ…)
- ë ˆì´í„´ì‹œ ì¤‘ìš”í•œ ì„œë¹„ìŠ¤

---

## ë‹¤ìŒ ê¸€

**14í¸: CUDA Kernel ìµœì í™”**
- Custom CUDA kernel ì‘ì„±
- Memory coalescing
- Warp-level primitives
- ì§ì ‘ ì§œëŠ” ê³ ì„±ëŠ¥ ì—°ì‚°

ê¸°ëŒ€í•´ì£¼ì„¸ìš”! ğŸš€

---

*ì§ˆë¬¸ì´ë‚˜ í”¼ë“œë°±ì€ [GitHub](https://github.com/yhna941)ì—ì„œ í™˜ì˜í•©ë‹ˆë‹¤!*
